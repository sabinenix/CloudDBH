{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CloudDBH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import open3d as o3d\n",
    "\n",
    "from shapely.ops import cascaded_union, polygonize\n",
    "from shapely.geometry import Polygon, LineString\n",
    "import shapely.geometry as geometry\n",
    "\n",
    "from scipy import optimize\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d, Delaunay\n",
    "import scipy.io as sio\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import math\n",
    "from math import pi\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from descartes import PolygonPatch\n",
    "\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Calders et al. 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from Calders et al\n",
    "# nearest neighbour needed for outlier removal\n",
    "def nn(x,rad):\n",
    "    \"\"\"\n",
    "    Function from TLS_Inventory.\n",
    "    \n",
    "    x: a numpy array, rad: radius to search for neighbors within\n",
    "    \n",
    "    My understanding - can either specify number of neighbors to return (in sklearn NearestNeighbors),\n",
    "    or return all the neighbors and then filter to return only those within a specified distance radius\n",
    "    \"\"\"\n",
    "    nbrs = NearestNeighbors(algorithm='auto', metric='euclidean').fit(x) \n",
    "    distances, indices = nbrs.radius_neighbors(radius=rad) #find all neighbours within this search radius\n",
    "    \n",
    "    #output explained: https://stackoverflow.com/questions/53886289/confused-with-the-output-of-sklearn-neighbors-nearestneighbors \n",
    "    return distances, indices\n",
    "\n",
    "\n",
    "def load_pts(cloud,downsample=False,outliers=False): \n",
    "    \"\"\"\n",
    "    Function from TLS_Inventory that reads the point cloud data for single tree at a time and\n",
    "    returns pandas dataframe with the points split into x, y, z coordinates. \n",
    "    \n",
    "    cloud: point cloud from a single tree - note, point cloud data should be pre-processed and \n",
    "           split into individual trees prior to using this function.\n",
    "    downsample (Optional): Default is False, meaning all the points are retained. If \n",
    "                           True, the point clouds are sampled to keep only fraction of points.\n",
    "    outliers (Optional): Default is False, meaning outliers are retained. If True, \n",
    "    \"\"\"\n",
    "    # read the point cloud data for single tree and save as pandas df with columns for x, y, z coords\n",
    "    dftemp=o3d.io.read_point_cloud(cloud)\n",
    "    df=pd.DataFrame(data=np.asarray(dftemp.points),columns=['x', 'y', 'z']) #access the points\n",
    "    \n",
    "    # Optional downsampling\n",
    "    if downsample:\n",
    "        df=df.sample(frac=0.1) # keep 10pct of points\n",
    "    \n",
    "    # Optional outlier removal \n",
    "    if outliers: #remove outliers\n",
    "        xy=df.iloc[:,0:2].values # takes ALL rows, first and second column - x and y - and .values converts to numpy array\n",
    "        dist, indi = nn(xy,0.5) # get nearest neighbors within search radius of 0.5\n",
    "        cnt=[len(i) for i in indi] # count the kNN within the search radius\n",
    "        cnt = pd.DataFrame({'count':cnt})\n",
    "        \n",
    "        # set threshold for the number of neighbors we want to keep \n",
    "        threshold=df.shape[0]*0.0001 #1 neighbor for every 10 000 pts\n",
    "        \n",
    "        removed=sum(np.array(cnt)<threshold)\n",
    "        df=df[np.array(cnt)>=threshold]\n",
    "        print(\"Removed %i outliers using kNN threshold %.2f\" % (removed[0], threshold-1))\n",
    "    \n",
    "    return df #return pandas dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatplots(trees, ncols=4, shape='square'):\n",
    "    \"\"\"\n",
    "    Returns matplotlib fig and axs objects with desired shape and number of subplots.\n",
    "    \n",
    "    Args:\n",
    "    trees - the list of trees (or other objects) that need to be plotted.\n",
    "            the length of this list will be the number of subplots returned.\n",
    "    ncols - the desired number of columns in the subplot grid \n",
    "    shape - optional argument describing the shape of the subplots. \n",
    "            Options are \"square\", \"horiz\", or \"vert\"\n",
    "    \n",
    "    Returns: \n",
    "    fig, axs - matplotlib objects corresponding to the grid of subplots\n",
    "    \"\"\"\n",
    "    # determine number of subplots\n",
    "    nplots = len(trees)\n",
    "    \n",
    "    # determine number of rows \n",
    "    if(nplots % ncols) != 0:\n",
    "        nrows = int(nplots / ncols + 1)\n",
    "    else:\n",
    "        nrows = int(nplots / ncols)\n",
    "\n",
    "    if shape == 'horiz':\n",
    "        # initiate figure with nrows and ncols\n",
    "        fig, axs = plt.subplots(nrows, ncols, figsize=(20,nrows*4))\n",
    "        fig.tight_layout()\n",
    "    if shape == 'vert':\n",
    "        # initiate figure with nrows and ncols\n",
    "        fig, axs = plt.subplots(nrows, ncols, figsize=(20,nrows*10))\n",
    "        fig.tight_layout()\n",
    "    if shape == 'square':\n",
    "        # initiate figure with nrows and ncols\n",
    "        fig, axs = plt.subplots(nrows, ncols, figsize=(20,nrows*5))\n",
    "        fig.tight_layout()\n",
    "    \n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_inds(treeind, rowind, colind, ncols):\n",
    "    \"\"\"\n",
    "    Function to be used in combination with the formatplots function. This function \n",
    "    updates indices as necessary to loop through the subplots and plot data in each. \n",
    "    \"\"\"\n",
    "    # increase tree index to get to next tree\n",
    "    treeind += 1\n",
    "\n",
    "    if (treeind % ncols) != 0:\n",
    "        colind += 1\n",
    "    else:\n",
    "        rowind +=1 \n",
    "        colind = 0\n",
    "    \n",
    "    return treeind, colind, rowind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_treeid(tree, ptspath):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # For wytham woods tree dataset\n",
    "    if str(ptspath).__contains__('DATA_clouds_ply'):\n",
    "        if tree[-4:] != '.ply':\n",
    "            tree = str(ptspath) + '/wytham_winter_' + str(tree) + '.ply' # TODO: don't hard code this\n",
    "        else:\n",
    "            tree = tree # TODO: make this more robust\n",
    "\n",
    "        tree_num = tree.split(\"_\")[-1].split(\".\")[0]\n",
    "    \n",
    "    # For tropical tree dataset - must have the leaf files already removed\n",
    "    if str(ptspath).__contains__('Tropical_manual_ply'):\n",
    "        if tree[-4:] != '.ply':\n",
    "            tree = str(ptspath) + '/MLA01_2018_' + str(tree) + '.wood.ply' # TODO: don't hard code this\n",
    "        else:\n",
    "            tree = str(tree) # TODO: make this more robust\n",
    "\n",
    "        tree_num = tree.split(\"_\")[-1].split(\".\")[0]\n",
    "    return tree, tree_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wytham_singlemulti(ptspath):\n",
    "    \"\"\"\n",
    "    Function that splits the wytham woods dataset into single vs multistemmed trees \n",
    "    with string operations based on how the wytham dataset is named.\n",
    "    \"\"\"\n",
    "\n",
    "    singlestemmed = []\n",
    "    multistemmed = []\n",
    "    \n",
    "    # get list of trees from pts path\n",
    "    trees = glob.glob(\"%s/*ply\" % ptspath)\n",
    "    \n",
    "    for tree in trees:\n",
    "        tree_num = tree.split(\"_\")[-1].split(\".\")[0]\n",
    "        \n",
    "        # if the treeID ends in a letter, it is not single stem\n",
    "        if tree_num[-1].isalpha():\n",
    "            multistemmed.append(tree)\n",
    "        else:\n",
    "            singlestemmed.append(tree)\n",
    "\n",
    "    return singlestemmed, multistemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_convexhull(tree, ptspath, hgt=1.3, cluster_pts=None):\n",
    "    \"\"\"\n",
    "    Function to calculate and plot convex hulls for each tree in a list of trees.\n",
    "    \n",
    "    Returns: \n",
    "        dbh - value for diameter at the specified height \n",
    "        verts - number of vertices used in calculating convex hull \n",
    "        pts_dbh - the 3D point dataset for the given slice (for plotting)\n",
    "    \"\"\"\n",
    "\n",
    "    # Get treeID and ensure inputted format is correct\n",
    "    tree, tree_num = construct_treeid(tree, ptspath)\n",
    "\n",
    "    # get DBH from full cloud to preserve max number of hits on stem slice\n",
    "    pts = load_pts(tree,False,False)\n",
    "    \n",
    "    # extract part of df with z values between 1.27 and 1.33 m by default - Tansey et al. 2009, Calders et al. 2015\n",
    "    pts_dbh = pts[(pts['z'] > pts['z'].min() + hgt - 0.03) & (pts['z'] < pts['z'].min() + hgt + 0.03)]\n",
    "    \n",
    "    if cluster_pts is not None:\n",
    "        pts_dbh = cluster_pts\n",
    "    \n",
    "    xy_dbh_arr = np.asarray(pts_dbh[['x', 'y']])\n",
    "\n",
    "    # Calculate DBH (convex hull) using scipy ConvexHull\n",
    "    hull = ConvexHull(xy_dbh_arr)\n",
    "    \n",
    "    # calculate return values\n",
    "    dbh = hull.area / np.pi\n",
    "    verts = len(hull.vertices)\n",
    "    \n",
    "    \n",
    "                      \n",
    "    return dbh, verts, pts_dbh, hull\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: redo this function so it takes in better args\n",
    "def plot_convexhull(treelist, ptspath, ncols=4, fixedheight = True, hgt=1.3, hgtlist = None):\n",
    "    fig, axs = formatplots(treelist, ncols)\n",
    "    rowind = colind = treeind = 0\n",
    "    \n",
    "    for tree in treelist:\n",
    "        tree, tree_num = construct_treeid(tree, ptspath)\n",
    "        \n",
    "        if fixedheight:\n",
    "            hgt = hgt\n",
    "        else:\n",
    "            hgt = hgtlist[tree_num]\n",
    "        \n",
    "        dbh_cv, verts, pts_dbh, hull = calc_convexhull(tree, pts_path, hgt=hgt)\n",
    "        xy_dbh_arr = np.asarray(pts_dbh[['x', 'y']])\n",
    "        \n",
    "        \n",
    "        axs[rowind, colind].scatter(pts_dbh['x'], pts_dbh['y']) \n",
    "        \n",
    "        # plot line connecting all the dbh vertices\n",
    "        for simplex in hull.simplices:\n",
    "            axs[rowind, colind].plot(xy_dbh_arr[simplex, 0], xy_dbh_arr[simplex, 1], 'r--')\n",
    "        \n",
    "        axs[rowind, colind].plot(xy_dbh_arr[hull.vertices,0], xy_dbh_arr[hull.vertices,1], 'r--', label=f\"DBH: {dbh_cv:.3f}\") # DBH (convex hull)\n",
    "        axs[rowind, colind].plot(xy_dbh_arr[hull.vertices,0], xy_dbh_arr[hull.vertices,1], 'ro', label=f\"Vertices: {len(hull.vertices)}\") # vertices\n",
    "\n",
    "        axs[rowind, colind].set_title(f'DBH (Convex Hull) for {tree_num}')\n",
    "        axs[rowind, colind].legend(fontsize='xx-large', loc='upper left')\n",
    "        axs[rowind, colind].axis('equal')\n",
    "\n",
    "        treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_dbh(tree, ptspath, step=0.1, lim=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Start from bottom of tree and calculate convex hull in 0.06m slices separated\n",
    "    by defined step size until limit. \n",
    "    \"\"\"\n",
    "\n",
    "    # Get treeID\n",
    "    tree, tree_num = construct_treeid(tree, ptspath)\n",
    "\n",
    "    #get DBH from full cloud to preserve max number of hits on stem slice\n",
    "    pts = load_pts(tree,False,False)\n",
    "\n",
    "    # calculate dbh for slices up to smaller of lim or tree height\n",
    "    disc = 0.1\n",
    "    hgt = pts['z'].max() - pts['z'].min()\n",
    "    \n",
    "    # set the limit equal to the smaller of tree height and defined limit\n",
    "    lim = min(hgt, lim)\n",
    "    \n",
    "    # empty lists to hold values for each tree\n",
    "    dbh_list = []\n",
    "    vert_list = []\n",
    "    disc_list = []\n",
    "\n",
    "    # loop upward through height of tree (up to lim)\n",
    "    while disc < lim:\n",
    "        # Calculate DBH (convex hull) for current slice using scipy ConvexHull\n",
    "        pts_slice = pts[(pts['z'] > pts['z'].min() + disc - 0.03) & (pts['z'] < pts['z'].min() + disc + 0.03)]\n",
    "        xy_dbh = np.asarray(pts_slice[['x', 'y']])\n",
    "\n",
    "        if len(xy_dbh) < 3:\n",
    "            if verbose: print(f'tree {tree_num} had not enough points at disc {disc}')\n",
    "            dbh_list.append(np.nan)\n",
    "            vert_list.append(np.nan)\n",
    "            disc_list.append(round(disc, 1))\n",
    "            disc += step\n",
    "            continue\n",
    "\n",
    "        circum = ConvexHull(xy_dbh) \n",
    "        \n",
    "        dbh = circum.area / np.pi\n",
    "        verts = len(circum.vertices)\n",
    "\n",
    "        # append data to lists\n",
    "        dbh_list.append(dbh)\n",
    "        vert_list.append(verts)\n",
    "        disc_list.append(round(disc, 1))\n",
    "\n",
    "        # move up to next slice  \n",
    "        disc += step\n",
    "    \n",
    "    df_out = pd.DataFrame(list(zip(dbh_list, vert_list, disc_list)), columns = ['DBH_CV', 'DBH_Verts', 'Slice_Hgt'])\n",
    "    \n",
    "    return df_out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iter_dbh(treelist, ptspath, ncols = 2, step=0.1, slope_thresh=0.1, dbh_thresh=2):\n",
    "    \"\"\"\n",
    "    Takes the output of function iter_dbh and plots results in grid.\n",
    "    \"\"\"\n",
    "    \n",
    "    # format plotting grid\n",
    "    fig, axs = formatplots(treelist, ncols, shape='horiz')\n",
    "    rowind = colind = treeind = 0\n",
    "\n",
    "    for tree in treelist:\n",
    "        \n",
    "        # Get treeID\n",
    "        tree, tree_num = construct_treeid(tree, ptspath)\n",
    "        \n",
    "        # get dataframe of iterdbh function\n",
    "        treedf = iter_dbh(tree, ptspath, step=step, lim=10)\n",
    "        \n",
    "        # get height for dbh for the tree \n",
    "        hgt = get_dbh_hgt(tree, ptspath, step=step, slope_thresh=slope_thresh, dbh_thresh=dbh_thresh)\n",
    "        hgtplot = treedf['Slice_Hgt'] == hgt\n",
    "        \n",
    "        # get buttress height \n",
    "        buttress_hgt, treedf = get_buttress_hgt(tree, ptspath, step=step, lim=10, slope_thresh = slope_thresh)\n",
    "        buttressplot = treedf['Slice_Hgt'] == buttress_hgt\n",
    "        \n",
    "        \n",
    "        # plot data\n",
    "        axs[rowind, colind].scatter(treedf['Slice_Hgt'], treedf['DBH_CV'])\n",
    "        axs[rowind, colind].set_title(f'DBH w Height for {tree_num}, Slope Thresh: {slope_thresh}', fontsize=20)\n",
    "        axs[rowind, colind].set_xlabel('Height of DBH Slice (m)', fontsize=20)\n",
    "        axs[rowind, colind].set_ylabel('Value of DBH Slice (m)', fontsize=20)\n",
    "        axs[rowind, colind].plot(treedf['Slice_Hgt'][hgtplot], treedf['DBH_CV'][hgtplot],'r+', markersize=30)\n",
    "        axs[rowind, colind].plot(treedf['Slice_Hgt'][buttressplot], treedf['DBH_CV'][buttressplot],'b+', markersize=30)\n",
    "        \n",
    "        # plot all flagged points\n",
    "        flagdf = flag_irregular(tree, ptspath, step=step, slope_thresh=slope_thresh, lim=10, dbh_thresh = dbh_thresh)\n",
    "        flagged = flagdf[flagdf['Flagged'] == 1]\n",
    "        axs[rowind, colind].plot(flagged['Slice_Hgt'], flagged['DBH_CV'],'r+', markersize=20)\n",
    "        \n",
    "        \n",
    "        fig.tight_layout()\n",
    "\n",
    "        \n",
    "        treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buttress_hgt(tree, ptspath, step=0.1, lim=10, slope_thresh = 0.1, verbose=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    tree - \n",
    "    ptspath - \n",
    "    step - \n",
    "    lim - \n",
    "    slope_thresh - slope threshold above which points are flagged as part of a buttress\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get treeID\n",
    "    tree, tree_num = construct_treeid(tree, ptspath)\n",
    "    \n",
    "    # run iterative dbh for specified step size and lim to calc slopes\n",
    "    treedf = iter_dbh(tree, ptspath, step=step, lim=lim)\n",
    "    \n",
    "    # calculate slope between pair of dbh measurements\n",
    "    for disc in treedf['Slice_Hgt'].iloc[1:-1]:\n",
    "\n",
    "        # round disc heights to one decimal place\n",
    "        disc = round(disc, 1)\n",
    "        prev_disc = round(disc - step, 1)\n",
    "\n",
    "        # get dbh values at each disc height\n",
    "        val1 = treedf[treedf['Slice_Hgt'] == disc]['DBH_CV'].iloc[0]\n",
    "        val2 = treedf[treedf['Slice_Hgt'] == prev_disc]['DBH_CV'].iloc[0]\n",
    "\n",
    "        # calculate slope between dbh measurements\n",
    "        slope = (val1 - val2) / step\n",
    "\n",
    "        # make new column in treedf for slope\n",
    "        treedf.loc[treedf['Slice_Hgt'] == prev_disc, 'slope'] = slope\n",
    "\n",
    "        # flag slope values greater than given threshold\n",
    "        if abs(slope) > slope_thresh:\n",
    "            if slope < 0: treedf.loc[treedf['Slice_Hgt'] == prev_disc, 'Buttress'] = -1\n",
    "            if slope > 0: treedf.loc[treedf['Slice_Hgt'] == prev_disc, 'Buttress'] = 1\n",
    "\n",
    "        else:\n",
    "            treedf.loc[treedf['Slice_Hgt'] == prev_disc, 'Buttress'] = 0\n",
    "        \n",
    "    # if first three points are not flagged as buttress points:\n",
    "    if all(np.array(treedf['Buttress'].iloc[0:3]) == np.array([0, 0, 0])):\n",
    "        # set buttress height to 0.0 (i.e. bottom of tree stem)\n",
    "        buttress_index = 0\n",
    "        buttress_hgt = 0.0\n",
    "\n",
    "    # otherwise, find height of top of buttress\n",
    "    else:\n",
    "        try:\n",
    "            # find first sequence of three 0's (non-buttress points) that come after a negative slope \n",
    "            val_find = np.array([-1, 0, 0, 0])\n",
    "            df_condition = treedf['Buttress'].rolling(4).apply(lambda g: all(g.astype(int) == val_find), raw=True)\n",
    "            \n",
    "            # get the index where three 0's first occurs and height of buttress\n",
    "            buttress_index = np.where(df_condition == 1)[0][0]\n",
    "            buttress_hgt = treedf['Slice_Hgt'].iloc[buttress_index]\n",
    "        \n",
    "        except:\n",
    "            if verbose: print(f\"tree {tree_num}: no suitable place to measure dbh from. Setting buttress height to 0.1m by default.\")\n",
    "            \n",
    "            buttress_hgt = 0.1\n",
    "               \n",
    "    return buttress_hgt, treedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_irregular(tree, ptspath, step=0.1, slope_thresh=0.1, lim=10, dbh_thresh = 2):\n",
    "    \"\"\"\n",
    "    Flag any irregular parts of the tree stem to avoid using them for dbh.\n",
    "    \n",
    "    Using a rolling mean to easily account for/ avoid the influence of extremely \n",
    "    small outliers that may be caused by data occlusion. \n",
    "    \"\"\"\n",
    "    # run iterative dbh for specified step size and lim to calc slopes\n",
    "    treedf = iter_dbh(tree, ptspath, step=step, lim=lim)\n",
    "    \n",
    "    # get buttress height and subset dataframe to parts over the buttress\n",
    "    buttress_hgt, treedf = get_buttress_hgt(tree, ptspath, step=step, lim=lim, slope_thresh=slope_thresh)\n",
    "    over_buttress = treedf[treedf[\"Slice_Hgt\"] > buttress_hgt]\n",
    "    \n",
    "    # calculate a max dbh threshold - no greater 2 * minimum rolling mean dbh\n",
    "    dbh_thresh = np.nanmin(over_buttress['DBH_CV'].rolling(3).mean()) * dbh_thresh\n",
    "    \n",
    "    # create a filter using this threshold\n",
    "    dbh_filt = treedf['DBH_CV'] > dbh_thresh\n",
    "    \n",
    "    # flag values greater than the threshold\n",
    "    treedf.loc[dbh_filt, 'Flagged'] = 1\n",
    "    treedf.loc[~dbh_filt, 'Flagged'] = 0\n",
    "\n",
    "    return treedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dbh_hgt(tree, ptspath, step=0.1, slope_thresh=0.1, lim=10, dbh_thresh = 2):\n",
    "    \"\"\"\n",
    "    Take output of iter_dbh function and determine ideal height from which to calculate dbh\n",
    "    \n",
    "    Note: the rolling search doesn't seem to work if there is no buttress so this is broken up into \n",
    "    if else - TODO: but could simplify if I figure out how to fix the else part of the code to work\n",
    "    for non buttressed trees too \n",
    "    \"\"\"\n",
    "    # Get treeID\n",
    "    tree, tree_num = construct_treeid(tree, ptspath)\n",
    "\n",
    "    # get buttress height\n",
    "    buttress_hgt, treedf = get_buttress_hgt(tree, ptspath, step=step, lim=lim, slope_thresh=slope_thresh)\n",
    "    \n",
    "    # flag irregular points to be excluded from dbh height consideration\n",
    "    treedf = flag_irregular(tree, ptspath, step=step, slope_thresh=slope_thresh, lim=lim, dbh_thresh = dbh_thresh)\n",
    "\n",
    "    # subset treedf to only above buttress height\n",
    "    over_buttress = treedf[treedf[\"Slice_Hgt\"] > buttress_hgt]\n",
    "    \n",
    "    # select only non-flagged data\n",
    "    filt = over_buttress['Flagged'] == 1\n",
    "    validdf = over_buttress[~filt]\n",
    "\n",
    "    # get suitable value closest to 1.3m to use for dbh height\n",
    "    disclist = list(validdf['Slice_Hgt'])\n",
    "    dif = lambda disclist : abs(disclist - 1.3)\n",
    "    hgt = min(disclist, key=dif)\n",
    "\n",
    "    return hgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2D(trees, ptspath, ncols=4, dbh=True, iterdbh=False, hgtlist = None):\n",
    "    \"\"\"\n",
    "    Function to plot tree point clouds in x/z or y/z direction\n",
    "    \n",
    "    TODO: make iter dbh function work on just one tree - then call iter_dbh from within this function\n",
    "    \"\"\"\n",
    "    # determine number of rows to plot figures\n",
    "    fig, axs = formatplots(trees, ncols)\n",
    "    rowind = colind = treeind = 0\n",
    "\n",
    "    # loop through trees\n",
    "    for tree in trees:\n",
    "        # Get treeID\n",
    "        tree, tree_num = construct_treeid(tree, ptspath)\n",
    "\n",
    "        #get DBH from full cloud to preserve max number of hits on stem slice\n",
    "        pts = load_pts(tree,False,False)\n",
    "\n",
    "        # calculate tree height\n",
    "        tree_hgt = pts['z'].max() - pts['z'].min()\n",
    "            \n",
    "        max_pt = pts[pts['z'] == pts['z'].max()]\n",
    "\n",
    "        # plot scatter plot of points in x and z \n",
    "        axs[rowind, colind].scatter(pts['x'], pts['z'], s=3) \n",
    "        if dbh: \n",
    "            pts_dbh = pts[(pts['z'] > pts['z'].min() + 1.27) & (pts['z'] < pts['z'].min() + 1.33)]\n",
    "            axs[rowind, colind].plot(pts_dbh['x'], pts_dbh['z'], 'ro', markersize=3, label=f\"DBH Slice\")\n",
    "        \n",
    "        if iterdbh: \n",
    "            hgt = hgtlist[tree_num]\n",
    "            pts_hgt = pts[(pts['z'] > pts['z'].min() + hgt - 0.03) & (pts['z'] < pts['z'].min() + hgt + 0.03)]\n",
    "            axs[rowind, colind].plot(pts_hgt['x'], pts_hgt['z'], 'yo', markersize=3, label=f\"HGT Slice\")\n",
    "            \n",
    "\n",
    "        axs[rowind, colind].annotate(f'Height: {tree_hgt:.2f}', xy=(max_pt['x'].iloc[0], max_pt['z'].iloc[0]), va='top', ha='left', size=15)\n",
    "        axs[rowind, colind].legend(fontsize='xx-large', loc='upper left')\n",
    "        axs[rowind, colind].set_title(f'{tree_num} ({rowind}, {colind})')\n",
    "\n",
    "        # increase tree index to get to next tree\n",
    "        treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Stem Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_stems(tree, ptspath, nclusters, fixedheight = True, hgt=1.3, hgtlist = None):\n",
    "    \"\"\"\n",
    "    Looks at the tree point cloud at the desired height (either 1.3m or the optimal dbh height)\n",
    "    and splits the data into clusters representing tree stems.\n",
    "    \"\"\"\n",
    "    \n",
    "    tree, tree_num = construct_treeid(tree, trop_pts_path)\n",
    "\n",
    "    if fixedheight:\n",
    "        hgt = hgt\n",
    "    else:\n",
    "        hgt = hgtlist[tree_num]\n",
    "\n",
    "    #get DBH from full cloud at desired height \n",
    "    pts = load_pts(tree,False,False)\n",
    "    pts_dbh = pts[(pts['z'] > pts['z'].min() + hgt - 0.03) & (pts['z'] < pts['z'].min() + hgt + 0.03)]\n",
    "    \n",
    "    # create clustering object with desired number of clusters\n",
    "    kmeans = KMeans(nclusters, n_init='auto')\n",
    "\n",
    "    # Fit data to the defined number of clusters\n",
    "    kmeans.fit(pts_dbh)\n",
    "\n",
    "    # Get labels for each point \n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_circle(circle_pts):\n",
    "    \"\"\" fit the circle to the point cloud and calculate dbh, residuals, etc.\"\"\"\n",
    "    # define subfunctions used to describe how well circle fits cross section \n",
    "    def calc_R(x,y, xc, yc):\n",
    "        \"\"\" \n",
    "        calculate the distance of each 2D points from the center (xc, yc) \n",
    "        calc_R() from Calders et al. 2015 (https://doi.org/10.5281/zenodo.7307956)\n",
    "        \"\"\"\n",
    "        return np.sqrt((x-xc)**2 + (y-yc)**2)\n",
    "\n",
    "    def f(c, x, y):\n",
    "        \"\"\" \n",
    "        calculate the algebraic distance between the data points and the mean circle centered at c=(xc, yc) \n",
    "        f() from Calders et al. 2015 (https://doi.org/10.5281/zenodo.7307956)\n",
    "        \"\"\"\n",
    "        Ri = calc_R(x, y, *c)\n",
    "        return Ri - Ri.mean()\n",
    "    \n",
    "    x = circle_pts['x']\n",
    "    y = circle_pts['y']\n",
    "\n",
    "    x_m = x.mean() # as first estimte of center\n",
    "    y_m = y.mean()\n",
    "\n",
    "    center_estimate = x_m, y_m\n",
    "\n",
    "    # optimize for the distance between data points and circle centered at the center estimate\n",
    "    center, ier = optimize.leastsq(f, center_estimate, args=(x,y)) \n",
    "    xc, yc = center\n",
    "    Ri = calc_R(x, y, *center)\n",
    "    R = Ri.mean()\n",
    "    dbh_circ = R * 2.0\n",
    "    residu = np.sum((Ri - R)**2)/len(Ri) # average residual between data points and circle fit\n",
    "\n",
    "    return residu, dbh_circ, xc, yc, circle_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overlap(circle_dict):\n",
    "    \"\"\" checks how much overlap between circles and disregards if more than 10% overlap \"\"\"\n",
    "    \n",
    "    labels = list(circle_dict.keys())\n",
    "    \n",
    "    i = 0\n",
    "    j = 1\n",
    "    \n",
    "    # initialize overlap to False\n",
    "    overlap = False \n",
    "    \n",
    "    # loop through each pair of circles and check overlap\n",
    "    while j < len(labels) and i < len(labels):\n",
    "\n",
    "        # pull circle values out of the dictionary passed into function\n",
    "        r1 = circle_dict[labels[i]]['r']\n",
    "        r2 = circle_dict[labels[j]]['r']\n",
    "\n",
    "        xc1 = circle_dict[labels[i]]['xc']\n",
    "        yc1 = circle_dict[labels[i]]['yc']\n",
    "\n",
    "        xc2 = circle_dict[labels[j]]['xc']\n",
    "        yc2 = circle_dict[labels[j]]['yc']\n",
    "\n",
    "        # calculate distance between centers of circles\n",
    "        dist = math.sqrt((xc1 - xc2) * (xc1 - xc2) + (yc1 - yc2) * (yc1 - yc2))\n",
    "\n",
    "        \n",
    "        if dist <= (r1 - r2) or dist <= (r2 - r1):\n",
    "            overlap = True\n",
    "        \n",
    "        # if circles overlap more than 10%, disregard \n",
    "        if dist < (r1 + r2) * 0.9:\n",
    "            overlap = True\n",
    "\n",
    "        # if j+1 is still less than the number of labels, increase j by 1\n",
    "        if j + 1 < len(labels):\n",
    "            j+=1\n",
    "\n",
    "        elif j + 1 >= len(labels) and i + 1 <= len(labels):\n",
    "            i+=1\n",
    "            j = i+1\n",
    "\n",
    "            \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_stems(tree, ptspath, maxstems = 4, fixedheight = True, hgt=1.3, hgtlist = None):\n",
    "    # start with one stem \n",
    "    tree, tree_num = construct_treeid(tree, trop_pts_path)\n",
    "    \n",
    "    if fixedheight:\n",
    "        hgt = hgt\n",
    "    else:\n",
    "        hgt = hgtlist[tree_num]\n",
    "    \n",
    "    # get DBH from full cloud at desired height \n",
    "    pts = load_pts(tree,False,False)\n",
    "    pts_dbh = pts[(pts['z'] > pts['z'].min() + hgt - 0.03) & (pts['z'] < pts['z'].min() + hgt + 0.03)]\n",
    "    \n",
    "    # initialize data dictionaries \n",
    "    residuals = {}\n",
    "    circ_vals = {}\n",
    "\n",
    "    # loop through checking residuals and overlap for number of possible stems up to the max \n",
    "    for stems in range(1, maxstems):\n",
    "        \n",
    "        # cluster the points into the specified number of stems \n",
    "        labels = split_stems(tree, ptspath, nclusters=stems, fixedheight = fixedheight, hgt=hgt, hgtlist = hgtlist)\n",
    "\n",
    "        # empty the dictionaries to hold values for each set of stems\n",
    "        inner_residuals = {}\n",
    "        inner_circ_vals = {}\n",
    "\n",
    "          \n",
    "        enough_pts = True\n",
    "        # fit a circle to each of the clusters\n",
    "        for label in np.unique(labels):\n",
    "            # get points for the stem of interest and fit a circle to the points\n",
    "            circle_pts = pts_dbh[labels == label]\n",
    "            \n",
    "            # check how many points in this cluster\n",
    "            if len(circle_pts) <= 3:\n",
    "                enough_pts = False\n",
    "                break\n",
    "            \n",
    "            \n",
    "            residu, dbh_circ, xc, yc, circle_pts = fit_circle(circle_pts)\n",
    "\n",
    "            # append the circle data to the dictionary\n",
    "            inner_circ_vals[label] = {'r': dbh_circ/2.0, 'xc': xc, 'yc':yc}\n",
    "\n",
    "            # add residual value to list \n",
    "            inner_residuals[label] = residu\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        # check overlap between circles\n",
    "        overlap = check_overlap(inner_circ_vals)\n",
    "\n",
    "        # if the circles don't overlap:\n",
    "        if overlap == False and enough_pts == True:\n",
    "            # average the residuals for each tree stem and add to dict\n",
    "            resid_list = []\n",
    "            for stem, resid in inner_residuals.items():\n",
    "                resid_list.append(resid)\n",
    "            \n",
    "            avg_resid = np.mean(resid_list)\n",
    "            \n",
    "            # add the data for each tree stem to the dict\n",
    "            residuals[stems] = avg_resid\n",
    "            circ_vals[stems] = inner_circ_vals\n",
    "    \n",
    "    nstems = min(residuals, key=residuals.get)\n",
    "        \n",
    "    return nstems\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_circular(tree, ptspath, nstems = 1, fixedheight = True, hgt=1.3, hgtlist = None):\n",
    "    \"\"\"\n",
    "    Returns metrics describing how well circle fits the cross section.\n",
    "    https://scipy-cookbook.readthedocs.io/items/Least_Squares_Circle.html\n",
    "    \"\"\"\n",
    "    \n",
    "    tree, tree_num = construct_treeid(tree, trop_pts_path)\n",
    "        \n",
    "    if fixedheight:\n",
    "        hgt = hgt\n",
    "    else:\n",
    "        hgt = hgtlist[tree_num]\n",
    "    \n",
    "    \n",
    "    # get DBH from full cloud at desired height \n",
    "    pts = load_pts(tree,False,False)\n",
    "    pts_dbh = pts[(pts['z'] > pts['z'].min() + hgt - 0.03) & (pts['z'] < pts['z'].min() + hgt + 0.03)]\n",
    "    \n",
    "    labels = split_stems(tree, trop_pts_path, nclusters = nstems, fixedheight = fixedheight, hgt=hgt, hgtlist = hgtlist)\n",
    "    \n",
    "    datadict = {}\n",
    "    \n",
    "    # cycle through labels (tree stems)\n",
    "    for label in np.unique(labels):\n",
    "        \n",
    "        # empty the inner dict to hold data\n",
    "        innerdict = {}\n",
    "\n",
    "        # get points for the stem of interest and fit a circle to the points\n",
    "        circle_pts = pts_dbh[labels == label]\n",
    "\n",
    "        residu, dbh_circ, xc, yc, circle_pts = fit_circle(circle_pts)\n",
    "\n",
    "        # construct data dictionaries\n",
    "        innerdict = {'residual': residu, 'dbh_circ': dbh_circ, 'xc': xc, 'yc':yc, 'circle_pts':circle_pts, 'hgt': hgt}\n",
    "        datadict[label] = innerdict\n",
    "\n",
    "    \n",
    "    return datadict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wytham Woods\n",
    "wytham_pts_path = \"/Users/snix/Documents/2021-2023/UCL/Dissertation/DATA_clouds_ply\"\n",
    "wytham_qsm_path = \"/Users/snix/Documents/2021-2023/UCL/Dissertation/DATA_QSM_opt/\"\n",
    "tree_csv_path = \"/Users/snix/Documents/2021-2023/UCL/Dissertation/Calders Et Al/analysis_and_figures/trees_summary.csv\"\n",
    "\n",
    "# Tropical Trees\n",
    "trop_pts_path = '/Users/snix/Documents/2021-2023/UCL/Dissertation/Tropical_manual_ply'\n",
    "trop_csv_path = '/Users/snix/Documents/2021-2023/UCL/Dissertation/MLA01_man.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singlestemmed: 585\n",
      "Multistemmed: 291\n"
     ]
    }
   ],
   "source": [
    "# Get lists of wytham and tropical trees\n",
    "wytham_trees = glob.glob(\"%s/*ply\" % wytham_pts_path)\n",
    "trop_trees = glob.glob(\"%s/*wood.ply\" % trop_pts_path)\n",
    "\n",
    "# Split wytham trees into single/multi stemmed\n",
    "singlestemmed, multistemmed = wytham_singlemulti(wytham_pts_path)\n",
    "\n",
    "print(f'Singlestemmed: {len(singlestemmed)}')\n",
    "print(f'Multistemmed: {len(multistemmed)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get DBH Heights, Calculate Convex Hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_data(treelist, ptspath, slope_thresh = 0.1, dbh_thresh = 2, verbose=False):\n",
    "    \"\"\"\n",
    "    Main function that runs other functions and fills dictionaries with data on the trees \"\"\"\n",
    "    \n",
    "    ## create empty dictionaries to hold data\n",
    "    hgt_dict = {}\n",
    "    dbh_13_dict = {}\n",
    "    dbh_adj_dict = {}\n",
    "    stems_dict = {}\n",
    "    \n",
    "    # cycle through trees and fill dictionaries with optimal height and convex hull values\n",
    "    for tree in treelist:\n",
    "        tree, tree_num = construct_treeid(tree, ptspath)\n",
    "        if verbose: print(f'running {tree_num}')\n",
    "\n",
    "        # calculate convex hull at 1.3m\n",
    "        dbh_13, verts_13, pts_dbh_13, hull_13 = calc_convexhull(tree, ptspath, hgt=1.3)\n",
    "        dbh_13_dict[tree_num] = dbh_13\n",
    "\n",
    "        # determine optimal height from which to calculate dbh\n",
    "        hgt = get_dbh_hgt(tree, ptspath, step=0.1, slope_thresh=slope_thresh, lim=10, dbh_thresh=dbh_thresh)\n",
    "        hgt_dict[tree_num] = hgt\n",
    "\n",
    "        # get number of stems at the optimal height \n",
    "        if treelist == trop_trees:\n",
    "            nstems = get_n_stems(tree, ptspath, fixedheight = False, hgt=1.3, hgtlist = hgt_dict)\n",
    "        # the wytham dataset is already split into single and multi-stemmed trees\n",
    "        else:\n",
    "            nstems = 1\n",
    "        \n",
    "        # add the number of stems to the dictionary\n",
    "        stems_dict[tree_num] = nstems\n",
    "\n",
    "        # get height at which to calculate convex hull and the points at that height\n",
    "        pts = load_pts(tree,False,False)\n",
    "        pts_dbh = pts[(pts['z'] > pts['z'].min() + hgt - 0.03) & (pts['z'] < pts['z'].min() + hgt + 0.03)]\n",
    "\n",
    "        # if the tree has multiple stems\n",
    "        if nstems != 1:\n",
    "            labels = split_stems(tree, ptspath, nclusters=nstems, fixedheight = False, hgtlist = hgt_dict)\n",
    "            dbh_adj_list = []\n",
    "\n",
    "            # calculate convex hull for each stem (each 'label') separately\n",
    "            for label in np.unique(labels):\n",
    "                circle_pts = pts_dbh[labels == label]\n",
    "                dbh_adj, verts_adj, pts_dbh_adj, hull_adj = calc_convexhull(tree, ptspath, hgt=hgt, cluster_pts = circle_pts)\n",
    "                dbh_adj_list.append(dbh_adj)\n",
    "                #print(f'tree {tree_num}, label: {label}, dbh_adj: {dbh_adj:.2f}')\n",
    "\n",
    "            # average the dbh values for each stem and add to dictionary\n",
    "            dbh_adj = np.mean(dbh_adj_list)\n",
    "            dbh_adj_dict[tree_num] = dbh_adj\n",
    "\n",
    "        else:\n",
    "            dbh_adj, verts_adj, pts_dbh_adj, hull_adj = calc_convexhull(tree, ptspath, hgt=hgt, cluster_pts = None)\n",
    "            dbh_adj_dict[tree_num] = dbh_adj\n",
    "    \n",
    "    return hgt_dict, dbh_13_dict, dbh_adj_dict, stems_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_csv(treelist, dict_list, col_names, write_csv=False, o_path=None):\n",
    "    \"\"\"\n",
    "    Takes the dictionaries resulting from get_tree_data, creates a combined dataframe and writes to csv\n",
    "    \"\"\"\n",
    "    \n",
    "    # turn output dictionaries into a combined dataframe\n",
    "    dfs = []\n",
    "    for data_dict, column_name in zip(dict_list, col_names):\n",
    "        df = pd.DataFrame.from_dict(data_dict, orient='index').reset_index()\n",
    "        df.columns = ['TLS_ID', column_name]\n",
    "        dfs.append(df)\n",
    "\n",
    "    # merge resulting dataframes into one large dataframe \n",
    "    merged_df = dfs[0].merge(dfs[1],on='TLS_ID').merge(dfs[2],on='TLS_ID').merge(dfs[3],on='TLS_ID')\n",
    "    \n",
    "    # if running for multistemmed wytham trees, need to aggregate by row\n",
    "    if treelist == multistemmed:\n",
    "        print('in multistemmed')\n",
    "        merged_df['StemID'] = merged_df['TLS_ID'].str[-1:] # new column with stem ID\n",
    "        \n",
    "        # replace TLS_ID column with only tree ID (no stem ID)\n",
    "        stem_ID = merged_df['TLS_ID'].str[:-1]\n",
    "        merged_df['TLS_ID'] = stem_ID \n",
    "        \n",
    "        # aggregate the dataframe by tree ID (summing and averaging other columns as necessary)\n",
    "        merged_df.groupby(['TLS_ID'],as_index=False).agg(avg_dbh_hgt = (\"dbh_hgt\", \"mean\"),\n",
    "                                                                 sum_dbh13 = (\"dbh13\", \"sum\"),\n",
    "                                                                 sum_dbhadj = (\"dbhadj\", \"sum\"),\n",
    "                                                                 stems = (\"stems\", \"sum\"))\n",
    "    # write to csv \n",
    "    if write_csv: merged_df.to_csv(o_path) \n",
    "    \n",
    "    # return merged dataframe\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running for /Users/snix/Documents/2021-2023/UCL/Dissertation/DATA_clouds_ply/wytham_winter_180b.ply w pts path /Users/snix/Documents/2021-2023/UCL/Dissertation/DATA_clouds_ply\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k1/jd21bp_d1t11kc4_7fm3m1cm0000gn/T/ipykernel_12847/34469102.py:16: RuntimeWarning: All-NaN axis encountered\n",
      "  dbh_thresh = np.nanmin(over_buttress['DBH_CV'].rolling(3).mean()) * dbh_thresh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in multistemmed\n",
      "running for /Users/snix/Documents/2021-2023/UCL/Dissertation/DATA_clouds_ply/wytham_winter_1623.ply w pts path /Users/snix/Documents/2021-2023/UCL/Dissertation/DATA_clouds_ply\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k1/jd21bp_d1t11kc4_7fm3m1cm0000gn/T/ipykernel_12847/34469102.py:16: RuntimeWarning: All-NaN axis encountered\n",
      "  dbh_thresh = np.nanmin(over_buttress['DBH_CV'].rolling(3).mean()) * dbh_thresh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running for /Users/snix/Documents/2021-2023/UCL/Dissertation/Tropical_manual_ply/MLA01_2018_T38.wood.ply w pts path /Users/snix/Documents/2021-2023/UCL/Dissertation/Tropical_manual_ply\n"
     ]
    }
   ],
   "source": [
    "# output csv paths\n",
    "trop_out = '/Users/snix/Documents/2021-2023/UCL/Dissertation/Results/trop_dbh.csv'\n",
    "wytham_ss_out = '/Users/snix/Documents/2021-2023/UCL/Dissertation/Results/wytham_ss_dbh.csv'\n",
    "wytham_ms_out = '/Users/snix/Documents/2021-2023/UCL/Dissertation/Results/wytham_ms_dbh.csv'\n",
    "\n",
    "# set threshold values for buttress slope and trunk irregularities\n",
    "slope_thresh = 0.1\n",
    "dbh_thresh = 2\n",
    "\n",
    "# create list of treelists and output paths\n",
    "treelists = [multistemmed, singlestemmed, trop_trees]\n",
    "ptspathlist = [wytham_pts_path, wytham_pts_path, trop_pts_path]\n",
    "opathlist = [wytham_ms_out, wytham_ss_out, trop_out]\n",
    "\n",
    "merged_dfs = []\n",
    "for i in range (len(treelists)):\n",
    "    treelist = treelists[i]\n",
    "    ptspath = ptspathlist[i]\n",
    "    o_path = opathlist[i]\n",
    "    \n",
    "    # run the main function that fills the dictionaries with each type of data for the list of trees\n",
    "    hgt_dict, dbh_13_dict, dbh_adj_dict, stems_dict = get_tree_data(treelist, ptspath, slope_thresh = 0.1, dbh_thresh = 2, verbose=False)\n",
    "\n",
    "    # add data dictionaries to a list and define list of column names\n",
    "    data_dicts = [hgt_dict, dbh_13_dict, dbh_adj_dict, stems_dict]\n",
    "    column_names = ['dbh_hgt', 'dbh13', 'dbhadj', 'stems']\n",
    "\n",
    "    # merge the dictionaries and write to csv\n",
    "    ms_df = write_data_csv(treelist, data_dicts, column_names, write_csv=True, o_path=o_path)\n",
    "    \n",
    "    merged_dfs.append(ms_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wytham_ms_df = merged_dfs[0]\n",
    "wytham_ss_df = merged_dfs[1]\n",
    "trop_df = merged_dfs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the height at which dbh is measured for each tree (note most at 1.3m)\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "\n",
    "# create dataset of only the points where the adjusted dbh height was 1.3 to mask these values\n",
    "mask13 = trop_treedata[trop_treedata['dbh_hgt'] == 1.3]\n",
    "\n",
    "plt.scatter(trop_treedata['TLS_ID'], trop_treedata['dbh13'], color='red', label=\"adjusted dbh\")\n",
    "plt.scatter(trop_treedata['TLS_ID'], trop_treedata['dbhadj'], color='blue', label=\"dbh at 1.3m\")\n",
    "plt.scatter(mask13['TLS_ID'], mask13['dbh13'], color='gray', label=\"no adjustment\") # mask points where adjusted dbh height was 1.3\n",
    "\n",
    "yerr = abs(trop_treedata['dbhadj'] - trop_treedata['dbh13'])\n",
    "plt.errorbar(trop_treedata['TLS_ID'], trop_treedata['dbh13'], yerr, uplims=yerr, ls='none', label=\"difference (adjusted vs. 1.3m)\")\n",
    "\n",
    "#plt.legend(loc=\"upper right\", fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Some Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotlist = ['5a', '5b', '20a', '20b', '145a', '145c', '145e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot convex hull results using fixed 1.3m dbh height\n",
    "plot_convexhull(plotlist, pts_path, ncols=4, fixedheight = True, hgt=1.3, hgtlist = hgt_dict)\n",
    "\n",
    "# plot convex hull results \n",
    "plot_convexhull(plotlist, pts_path, ncols=4, fixedheight = False, hgt=1.3, hgtlist = hgt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_iter_dbh(plotlist, pts_path, ncols = 2, step=0.1, slope_thresh=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot2D(plotlist, pts_path, ncols=3, dbh=False, iterdbh=True, hgtlist = hgt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgt_dict['T102']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Circular?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dictionaries to hold data\n",
    "residu_dict = {}\n",
    "dbh_circ_dict = {}\n",
    "\n",
    "# set list of trees to run convex hull on - change to either 'trop_trees' or 'singlestemmed'\n",
    "treelist = wytham_trees[0:50]\n",
    "\n",
    "# cycle through trees, get residuals and plot circle fits\n",
    "for tree in treelist:\n",
    "    tree, tree_num = construct_treeid(tree, pts_path)\n",
    "    \n",
    "    nstems = get_n_stems(tree, pts_path, fixedheight = False, hgt=1.3, hgtlist = hgt_dict)\n",
    "    \n",
    "    # run circular function for the given tree\n",
    "    datadict = how_circular(tree, pts_path, nstems = nstems, fixedheight = False, hgtlist = hgt_dict)\n",
    "    \n",
    "    # unroll and plot dictionary data for each stem\n",
    "    for label in datadict.keys():\n",
    "    \n",
    "        xc = datadict[label]['xc']\n",
    "        yc = datadict[label]['yc']\n",
    "        dbh_circ = datadict[label]['dbh_circ']\n",
    "        residu = datadict[label]['residual']\n",
    "        pts_dbh = datadict[label]['circle_pts']\n",
    "        hgt = datadict[label]['hgt']\n",
    "    \n",
    "    \n",
    "    \n",
    "        # plotting \n",
    "        theta_fit = np.linspace(-pi, pi, 180)\n",
    "        x_fit = xc + (dbh_circ / 2.0)*np.cos(theta_fit)\n",
    "        y_fit = yc + (dbh_circ / 2.0)*np.sin(theta_fit)\n",
    "\n",
    "        axs[rowind, colind].scatter(pts_dbh['x'], pts_dbh['y'])\n",
    "        axs[rowind, colind].plot(x_fit, y_fit, 'b-' , label=f\"residu={residu:.4f}\", lw=2,zorder=5)\n",
    "        axs[rowind, colind].plot([xc], [yc], 'bD', mec='y', mew=1)\n",
    "        axs[rowind, colind].set_title(f'Circle fit for{tree_num}', fontsize=20)\n",
    "        axs[rowind, colind].legend(loc='upper right')\n",
    "\n",
    "        axs[rowind, colind].set_aspect('equal')\n",
    "\n",
    "    # fill dictionaries\n",
    "    residu_dict[tree_num] = residu\n",
    "    dbh_circ_dict[tree_num] = dbh_circ\n",
    "    \n",
    "    \n",
    "    # update indices and move to next subplot\n",
    "    treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# set list of trees to run convex hull on - change to either 'trop_trees' or 'singlestemmed'\n",
    "treelist = wytham_trees[0:50]\n",
    "#['T394', 'T267', 'T363', 'T298', 'T135', 'T366', 'T279']\n",
    "\n",
    "# format the plots\n",
    "fig, axs = formatplots(treelist, ncols=4)\n",
    "rowind = colind = treeind = 0\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# cycle through trees, get residuals and plot circle fits\n",
    "for tree in treelist:\n",
    "    tree, tree_num = construct_treeid(tree, pts_path)\n",
    "    \n",
    "    nstems = get_n_stems(tree, pts_path, fixedheight = False, hgt=1.3, hgtlist = hgt_dict)\n",
    "    \n",
    "    # run circular function for the given tree\n",
    "    datadict = how_circular(tree, pts_path, nstems = nstems, fixedheight = False, hgtlist = hgt_dict)\n",
    "    \n",
    "    # unroll and plot dictionary data for each stem\n",
    "    for label in datadict.keys():\n",
    "    \n",
    "        xc = datadict[label]['xc']\n",
    "        yc = datadict[label]['yc']\n",
    "        dbh_circ = datadict[label]['dbh_circ']\n",
    "        residu = datadict[label]['residual']\n",
    "        pts_dbh = datadict[label]['circle_pts']\n",
    "        hgt = datadict[label]['hgt']\n",
    "    \n",
    "    \n",
    "    \n",
    "        # plotting \n",
    "        theta_fit = np.linspace(-pi, pi, 180)\n",
    "        x_fit = xc + (dbh_circ / 2.0)*np.cos(theta_fit)\n",
    "        y_fit = yc + (dbh_circ / 2.0)*np.sin(theta_fit)\n",
    "\n",
    "        axs[rowind, colind].scatter(pts_dbh['x'], pts_dbh['y'])\n",
    "        axs[rowind, colind].plot(x_fit, y_fit, 'b-' , label=f\"residu={residu:.4f}\", lw=2,zorder=5)\n",
    "        axs[rowind, colind].plot([xc], [yc], 'bD', mec='y', mew=1)\n",
    "        axs[rowind, colind].set_title(f'Circle fit for{tree_num}', fontsize=20)\n",
    "        axs[rowind, colind].legend(loc='upper right')\n",
    "\n",
    "        axs[rowind, colind].set_aspect('equal')\n",
    "\n",
    "    # fill dictionaries\n",
    "    residu_dict[tree_num] = residu\n",
    "    dbh_circ_dict[tree_num] = dbh_circ\n",
    "    \n",
    "    \n",
    "    # update indices and move to next subplot\n",
    "    treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Testing - Slope Thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run functions with varying slope thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big dictionaries of dataframes with sensitivity data \n",
    "slope_sensitivity_dict = {}\n",
    "\n",
    "# set list of trees to run convex hull on \n",
    "treelist = singlestemmed\n",
    "\n",
    "# list of threshold values to test\n",
    "slope_thresh_vals = [0.07, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# cycle through threshold values for one variable\n",
    "for slope_thresh in slope_thresh_vals:\n",
    "    # hold other threshold value constant\n",
    "    dbh_thresh = 2\n",
    "    \n",
    "    print(f'slope_thresh: {slope_thresh}, dbh_thresh: {dbh_thresh}')\n",
    "    \n",
    "    # create empty dictionaries to hold data\n",
    "    hgt_dict = {}\n",
    "    dbh_13_dict = {}\n",
    "    dbh_adj_dict = {}\n",
    "\n",
    "    # cycle through trees and fill dictionaries with optimal height and convex hull values\n",
    "    for tree in treelist:\n",
    "        tree, tree_num = construct_treeid(tree, trop_pts_path)\n",
    "\n",
    "        # calculate convex hull at 1.3m\n",
    "        dbh_13, verts_13, pts_dbh_13, hull_13 = calc_convexhull(tree, trop_pts_path, hgt=1.3)\n",
    "        dbh_13_dict[tree_num] = dbh_13\n",
    "\n",
    "        # determine optimal height from which to calculate dbh\n",
    "        hgt = get_dbh_hgt(tree, trop_pts_path, step=0.1, slope_thresh=slope_thresh, lim=10, dbh_thresh=dbh_thresh)\n",
    "        hgt_dict[tree_num] = hgt\n",
    "\n",
    "        #print(f'{tree_num} hgt: {hgt}')\n",
    "\n",
    "        # calculate convex hull at optimal height \n",
    "        dbh_adj, verts_adj, pts_dbh_adj, hull_adj = calc_convexhull(tree, trop_pts_path, hgt=hgt)\n",
    "        dbh_adj_dict[tree_num] = dbh_adj\n",
    "    \n",
    "    # convert each dictionary to dataframe and rename columns\n",
    "    hgt = pd.DataFrame.from_dict(hgt_dict, orient='index').reset_index()\n",
    "    hgt.columns = ['TLS_ID', 'hgt']\n",
    "\n",
    "    dbh13 = pd.DataFrame.from_dict(dbh_13_dict, orient='index').reset_index()\n",
    "    dbh13.columns = ['TLS_ID', 'dbh13']\n",
    "\n",
    "    dbhadj = pd.DataFrame.from_dict(dbh_adj_dict, orient='index').reset_index()\n",
    "    dbhadj.columns = ['TLS_ID', 'dbhadj']\n",
    "\n",
    "    # create dataframe for the list of trees with optimal height and convex hull values\n",
    "    trop_treedata = pd.merge(pd.merge(hgt, dbh13, on ='TLS_ID'), dbhadj, on ='TLS_ID')\n",
    "    \n",
    "    # add dataframe to dictionary\n",
    "    slope_sensitivity_dict[slope_thresh] = trop_treedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Number of Adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for slope_thresh in slope_sensitivity_dict.keys():\n",
    "    datadict = slope_sensitivity_dict[slope_thresh]\n",
    "    adjusted = datadict[datadict['hgt'] != 1.3]\n",
    "    differences = np.mean(datadict['dbhadj'] - datadict['dbh13'])\n",
    "    print(f'slope thresh {slope_thresh} - {len(adjusted)} adjustments - mean dif: {differences:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Sensitivity to Slope Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 2\n",
    "fig, axs = formatplots(slope_sensitivity_dict.keys(), ncols=ncols, shape='horiz')\n",
    "rowind = colind = treeind = 0\n",
    "        \n",
    "for slope_thresh in slope_sensitivity_dict.keys():\n",
    "    datadict = slope_sensitivity_dict[slope_thresh]\n",
    "    \n",
    "    mask13 = datadict[datadict['hgt'] == 1.3]\n",
    "    \n",
    "    axs[rowind, colind].get_xaxis().set_ticks([])\n",
    "    axs[rowind, colind].set_title(f'Adjusted vs Original DBH Values (Slope Threshold: {slope_thresh})')\n",
    "    \n",
    "    axs[rowind, colind].scatter(datadict['TLS_ID'], datadict['dbh13'], color='red', label=\"adjusted dbh\")\n",
    "    axs[rowind, colind].scatter(datadict['TLS_ID'], datadict['dbhadj'], color='blue', label=\"dbh at 1.3m\")\n",
    "    axs[rowind, colind].scatter(mask13['TLS_ID'], mask13['dbh13'], color='gray', label=\"no adjustment\") # mask points where adjusted dbh height was 1.3\n",
    "\n",
    "    yerr = abs(datadict['dbhadj'] - datadict['dbh13'])\n",
    "    axs[rowind, colind].errorbar(datadict['TLS_ID'], datadict['dbh13'], yerr, uplims=yerr, ls='none', label=\"difference (adjusted vs. 1.3m)\")\n",
    "    \n",
    "\n",
    "    treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols)\n",
    "\n",
    "fig.tight_layout()\n",
    "handles, labels = axs[0,0].get_legend_handles_labels()\n",
    "#fig.legend(handles, labels, fontsize=15, bbox_to_anchor=(0.998, 0.978)) # manually placing legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#minilist = ['T207', 'T169', 'T44', 'T206', 'T102', 'T38'] \n",
    "\n",
    "minilist = ['8022', '8340', '2312', '8388', '1623', '1179'] \n",
    "\n",
    "for slope_thresh in slope_thresh_vals:\n",
    "    print(slope_thresh)\n",
    "    plot_iter_dbh(minilist, pts_path, ncols = 2, step=0.1, slope_thresh=slope_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Testing - DBH Irregularity Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big dictionaries of dataframes with sensitivity data \n",
    "dbh_sensitivity_dict = {}\n",
    "\n",
    "# set list of trees to run convex hull on \n",
    "treelist = singlestemmed\n",
    "\n",
    "# list of threshold values to test\n",
    "dbh_thresh_vals = [1.2, 1.5, 2, 2.5, 3, 3.5]\n",
    "\n",
    "# cycle through threshold values for one variable\n",
    "for dbh_thresh in dbh_thresh_vals:\n",
    "    # hold other threshold value constant\n",
    "    slope_thresh = 0.1\n",
    "    \n",
    "    print(f'dbh_thresh: {dbh_thresh}, slope_thresh: {slope_thresh}')\n",
    "    \n",
    "    # create empty dictionaries to hold data\n",
    "    hgt_dict = {}\n",
    "    dbh_13_dict = {}\n",
    "    dbh_adj_dict = {}\n",
    "    irreg_dict = {}\n",
    "\n",
    "    # cycle through trees and fill dictionaries with optimal height and convex hull values\n",
    "    for tree in treelist:\n",
    "        tree, tree_num = construct_treeid(tree, trop_pts_path)\n",
    "\n",
    "        # calculate convex hull at 1.3m\n",
    "        dbh_13, verts_13, pts_dbh_13, hull_13 = calc_convexhull(tree, trop_pts_path, hgt=1.3)\n",
    "        dbh_13_dict[tree_num] = dbh_13\n",
    "\n",
    "        # determine optimal height from which to calculate dbh\n",
    "        hgt = get_dbh_hgt(tree, trop_pts_path, step=0.1, slope_thresh=slope_thresh, lim=10, dbh_thresh=dbh_thresh)\n",
    "        hgt_dict[tree_num] = hgt\n",
    "\n",
    "        #print(f'{tree_num} hgt: {hgt}')\n",
    "\n",
    "        # calculate convex hull at optimal height \n",
    "        dbh_adj, verts_adj, pts_dbh_adj, hull_adj = calc_convexhull(tree, trop_pts_path, hgt=hgt)\n",
    "        dbh_adj_dict[tree_num] = dbh_adj\n",
    "        \n",
    "        # flag irregular function \n",
    "        irreg_df = flag_irregular(tree, trop_pts_path, step=0.1, slope_thresh=slope_thresh, lim=10, dbh_thresh = dbh_thresh)\n",
    "        irregs = len(irreg_df[irreg_df['Flagged'] == 1])\n",
    "        irreg_dict[tree_num] = irregs\n",
    "    \n",
    "    \n",
    "    # convert each dictionary to dataframe and rename columns\n",
    "    hgt = pd.DataFrame.from_dict(hgt_dict, orient='index').reset_index()\n",
    "    hgt.columns = ['TLS_ID', 'hgt']\n",
    "\n",
    "    dbh13 = pd.DataFrame.from_dict(dbh_13_dict, orient='index').reset_index()\n",
    "    dbh13.columns = ['TLS_ID', 'dbh13']\n",
    "\n",
    "    dbhadj = pd.DataFrame.from_dict(dbh_adj_dict, orient='index').reset_index()\n",
    "    dbhadj.columns = ['TLS_ID', 'dbhadj']\n",
    "    \n",
    "    irreg = pd.DataFrame.from_dict(irreg_dict, orient='index').reset_index()\n",
    "    irreg.columns = ['TLS_ID', 'irregs']\n",
    "\n",
    "    # create dataframe for the list of trees with optimal height and convex hull values\n",
    "    trop_treedata = pd.merge(pd.merge(pd.merge(hgt, dbh13, on ='TLS_ID'), dbhadj, on ='TLS_ID'), irreg, on='TLS_ID')\n",
    "    \n",
    "    # add dataframe to dictionary\n",
    "    dbh_sensitivity_dict[dbh_thresh] = trop_treedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dbh_thresh in dbh_sensitivity_dict.keys():\n",
    "    datadict = dbh_sensitivity_dict[dbh_thresh]\n",
    "    adjusted = datadict[datadict['hgt'] != 1.3]\n",
    "    differences = np.mean(datadict['dbhadj'] - datadict['dbh13'])\n",
    "    irreg_num = np.mean(datadict['irregs'])\n",
    "    print(f'dbh thresh {dbh_thresh} - {len(adjusted)} adjustments - mean dif: {differences:.2f}, flagged {irreg_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap,LinearSegmentedColormap\n",
    "\n",
    "#fig, axs = formatplots(dbh_sensitivity_dict.keys(), ncols=ncols, shape='horiz')\n",
    "#rowind = colind = treeind = 0\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "# get colormap for plotting with 6 colors\n",
    "cmap = plt.colormaps['inferno'].resampled(6)\n",
    "newcolors = cmap(np.linspace(0, 1, 6))\n",
    "\n",
    "# create the lines connecting dots to help with visualization\n",
    "high_thresh = dbh_sensitivity_dict[3.5]\n",
    "low_thresh = dbh_sensitivity_dict[1.2]\n",
    "yerr = abs(high_thresh['irregs'] - low_thresh['irregs'])\n",
    "plt.errorbar(datadict['TLS_ID'], low_thresh['irregs'], yerr, uplims=yerr, ls='none', color='gray', linewidth=0.5, zorder = 0, fmt='none')\n",
    "\n",
    "# loop through threshold values and plot on the same figure\n",
    "i = 0\n",
    "for dbh_thresh in dbh_sensitivity_dict.keys():\n",
    "    datadict = dbh_sensitivity_dict[dbh_thresh]\n",
    "    color = newcolors[i]\n",
    "    plt.scatter(datadict['TLS_ID'], datadict['irregs'], color=color, label=\"adjusted dbh\")\n",
    "    \n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 2\n",
    "fig, axs = formatplots(dbh_sensitivity_dict.keys(), ncols=ncols, shape='horiz')\n",
    "rowind = colind = treeind = 0\n",
    "        \n",
    "for dbh_thresh in dbh_sensitivity_dict.keys():\n",
    "    datadict = dbh_sensitivity_dict[dbh_thresh]\n",
    "    \n",
    "    mask13 = datadict[datadict['hgt'] == 1.3]\n",
    "    \n",
    "    axs[rowind, colind].get_xaxis().set_ticks([])\n",
    "    axs[rowind, colind].set_title(f'Adjusted vs Original DBH Values (DBH Threshold: {dbh_thresh})')\n",
    "    \n",
    "    axs[rowind, colind].scatter(datadict['TLS_ID'], datadict['dbh13'], color='red', label=\"adjusted dbh\")\n",
    "    axs[rowind, colind].scatter(datadict['TLS_ID'], datadict['dbhadj'], color='blue', label=\"dbh at 1.3m\")\n",
    "    axs[rowind, colind].scatter(mask13['TLS_ID'], mask13['dbh13'], color='gray', label=\"no adjustment\") # mask points where adjusted dbh height was 1.3\n",
    "\n",
    "    yerr = abs(datadict['dbhadj'] - datadict['dbh13'])\n",
    "    axs[rowind, colind].errorbar(datadict['TLS_ID'], datadict['dbh13'], yerr, uplims=yerr, ls='none', label=\"difference (adjusted vs. 1.3m)\")\n",
    "    \n",
    "\n",
    "    treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols)\n",
    "\n",
    "fig.tight_layout()\n",
    "handles, labels = axs[0,0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, fontsize=15, bbox_to_anchor=(0.998, 0.978)) # manually placing legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict DBH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treecsv = pd.read_csv(tree_csv_path)\n",
    "\n",
    "\n",
    "treecsv = treecsv[['TLS_ID','stemlocx_[m]', 'stemlocy_[m]', 'DBH_TLS_[m]',\n",
    "                    'Hgt_pts_[m]', 'VerticalCrownProjectedArea_pts_[m2]', 'Vol_QSM_avg_[m3]',\n",
    "                    'Vol_QSM_D0_25mm_avg_[m3]', 'Vol_QSM_D25_50mm_avg_[m3]', 'Vol_QSM_D50_75mm_avg_[m3]',\n",
    "                    'Vol_QSM_D75_100mm_avg_[m3]', 'Vol_QSM_D100_200mm_avg_[m3]', 'Vol_QSM_D200_500mm_avg_[m3]',\n",
    "                    'Vol_QSM_D500_1000mm_avg_[m3]', 'Vol_QSM_D1000mm_avg_[m3]']]\n",
    "\n",
    "\n",
    "treedata = pd.merge(big_datadf, treecsv, on ='TLS_ID')\n",
    "\n",
    "treedata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Convex Hull with \"DBH_TLS[m]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only single stem trees from csv \n",
    "#ss_treedata = treecsv[treecsv[['TLS_ID']].apply(lambda x: x[-1].isdigit(), axis=1)]\n",
    "\n",
    "#smalltrees = treedata[treedata['DBH_CV']<4]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(treedata['dbh_cv'], treedata['DBH_TLS_[m]'])\n",
    "plt.xlabel('DBH from Convex Hull', fontsize=20)\n",
    "plt.ylabel('DBH from TLS', fontsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (Predicting DBH from QSMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "\n",
    "# Labels are the values we want to predict\n",
    "labels = np.array(treedata['dbh_cv'])\n",
    "\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "features = treedata.drop(['TLS_ID','dbh_cv', 'DBH_TLS_[m]'], axis = 1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns)\n",
    "\n",
    "# Convert to numpy array\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2))\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / test_labels)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf_results = pd.DataFrame(mape, predictions, test_labels)\n",
    "\n",
    "rf_results = pd.DataFrame({'mape': mape, 'errors': errors,'predictions': predictions, 'test_labels': test_labels}, columns=['mape', 'errors', 'predictions', 'test_labels'])\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(rf_results['predictions'], rf_results['test_labels'])\n",
    "plt.xlabel('Predictions (DBH)', fontsize=20)\n",
    "plt.ylabel('Measured Values (DBH)', fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(rf_results['test_labels'], rf_results['errors'])\n",
    "plt.xlabel('Measured Values (DBH)', fontsize=20)\n",
    "plt.ylabel('Errors', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge rf results back with original dataset\n",
    "#https://stackoverflow.com/questions/40729162/merging-results-from-model-predict-with-original-pandas-dataframe\n",
    "# TODO: in final - re-run model on full dataset and split into training vs testing\n",
    "\n",
    "df_out = pd.merge(treedata,rf_results[['mape', 'errors', 'predictions']],how = 'left',left_index = True, right_index = True)\n",
    "\n",
    "#rf_results.shape\n",
    "df_out.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(df_out['verts'], df_out['errors'])\n",
    "plt.xlabel('DBH Verts', fontsize=20)\n",
    "plt.ylabel('Errors', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
