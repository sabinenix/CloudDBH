{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CloudDBH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import open3d as o3d\n",
    "\n",
    "from shapely.ops import cascaded_union, polygonize\n",
    "from shapely.geometry import Polygon, LineString\n",
    "import shapely.geometry as geometry\n",
    "\n",
    "from scipy import optimize\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d, Delaunay\n",
    "import scipy.io as sio\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import math\n",
    "from math import pi\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from descartes import PolygonPatch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Calders et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from Calders et al\n",
    "# nearest neighbour needed for outlier removal\n",
    "def nn(x,rad):\n",
    "    \"\"\"\n",
    "    Function from TLS_Inventory.\n",
    "    \n",
    "    x: a numpy array, rad: radius to search for neighbors within\n",
    "    \n",
    "    My understanding - can either specify number of neighbors to return (in sklearn NearestNeighbors),\n",
    "    or return all the neighbors and then filter to return only those within a specified distance radius\n",
    "    \"\"\"\n",
    "    nbrs = NearestNeighbors(algorithm='auto', metric='euclidean').fit(x) \n",
    "    distances, indices = nbrs.radius_neighbors(radius=rad) #find all neighbours within this search radius\n",
    "    \n",
    "    #output explained: https://stackoverflow.com/questions/53886289/confused-with-the-output-of-sklearn-neighbors-nearestneighbors \n",
    "    return distances, indices\n",
    "\n",
    "\n",
    "def load_pts(cloud,downsample=False,outliers=False): \n",
    "    \"\"\"\n",
    "    Function from TLS_Inventory that reads the point cloud data for single tree at a time and\n",
    "    returns pandas dataframe with the points split into x, y, z coordinates. \n",
    "    \n",
    "    cloud: point cloud from a single tree - note, point cloud data should be pre-processed and \n",
    "           split into individual trees prior to using this function.\n",
    "    downsample (Optional): Default is False, meaning all the points are retained. If \n",
    "                           True, the point clouds are sampled to keep only fraction of points.\n",
    "    outliers (Optional): Default is False, meaning outliers are retained. If True, \n",
    "    \"\"\"\n",
    "    # read the point cloud data for single tree and save as pandas df with columns for x, y, z coords\n",
    "    dftemp=o3d.io.read_point_cloud(cloud)\n",
    "    df=pd.DataFrame(data=np.asarray(dftemp.points),columns=['x', 'y', 'z']) #access the points\n",
    "    \n",
    "    # Optional downsampling\n",
    "    if downsample:\n",
    "        df=df.sample(frac=0.1) # keep 10pct of points\n",
    "    \n",
    "    # Optional outlier removal \n",
    "    if outliers: #remove outliers\n",
    "        xy=df.iloc[:,0:2].values # takes ALL rows, first and second column - x and y - and .values converts to numpy array\n",
    "        dist, indi = nn(xy,0.5) # get nearest neighbors within search radius of 0.5\n",
    "        cnt=[len(i) for i in indi] # count the kNN within the search radius\n",
    "        cnt = pd.DataFrame({'count':cnt})\n",
    "        \n",
    "        # set threshold for the number of neighbors we want to keep \n",
    "        threshold=df.shape[0]*0.0001 #1 neighbor for every 10 000 pts\n",
    "        \n",
    "        removed=sum(np.array(cnt)<threshold)\n",
    "        df=df[np.array(cnt)>=threshold]\n",
    "        print(\"Removed %i outliers using kNN threshold %.2f\" % (removed[0], threshold-1))\n",
    "    \n",
    "    return df #return pandas dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: dont use this format plots function find a package\n",
    "def formatplots(trees, ncols=4, shape='square'):\n",
    "    # determine number of subplots\n",
    "    nplots = len(trees)\n",
    "    \n",
    "    # determine number of rows \n",
    "    if(nplots % ncols) != 0:\n",
    "        nrows = int(nplots / ncols + 1)\n",
    "    else:\n",
    "        nrows = int(nplots / ncols)\n",
    "\n",
    "    if shape == 'horiz':\n",
    "        # initiate figure with nrows and ncols\n",
    "        fig, axs = plt.subplots(nrows, ncols, figsize=(20,nrows*4))\n",
    "        fig.tight_layout()\n",
    "    if shape == 'vert':\n",
    "        # initiate figure with nrows and ncols\n",
    "        fig, axs = plt.subplots(nrows, ncols, figsize=(20,nrows*10))\n",
    "        fig.tight_layout()\n",
    "    if shape == 'square':\n",
    "        # initiate figure with nrows and ncols\n",
    "        fig, axs = plt.subplots(nrows, ncols, figsize=(20,nrows*10))\n",
    "        fig.tight_layout()\n",
    "    \n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_inds(treeind, rowind, colind, ncols):\n",
    "    # increase tree index to get to next tree\n",
    "    treeind += 1\n",
    "\n",
    "    if (treeind % ncols) != 0:\n",
    "        colind += 1\n",
    "    else:\n",
    "        rowind +=1 \n",
    "        colind = 0\n",
    "    \n",
    "    return treeind, colind, rowind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_treeid(tree, ptspath):\n",
    "    # For wytham woods tree dataset\n",
    "    if str(ptspath).__contains__('DATA_clouds_ply'):\n",
    "        if tree[-4:] != '.ply':\n",
    "            tree = str(ptspath) + '/wytham_winter_' + str(tree) + '.ply' # TODO: don't hard code this\n",
    "        else:\n",
    "            tree = tree # TODO: make this more robust\n",
    "\n",
    "        tree_num = tree.split(\"_\")[-1].split(\".\")[0]\n",
    "    \n",
    "    # For tropical tree dataset - must have the leaf files already removed\n",
    "    if str(ptspath).__contains__('Tropical_manual_ply'):\n",
    "        if tree[-4:] != '.ply':\n",
    "            tree = str(ptspath) + '/MLA01_2018_' + str(tree) + '.wood.ply' # TODO: don't hard code this\n",
    "        else:\n",
    "            tree = str(tree) # TODO: make this more robust\n",
    "\n",
    "        tree_num = tree.split(\"_\")[-1].split(\".\")[0]\n",
    "    return tree, tree_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wytham_singlemulti(ptspath):\n",
    "    \"\"\"\n",
    "    Function that splits the wytham woods dataset into single vs multistemmed trees \n",
    "    with string operations based on how the wytham dataset is named.\n",
    "    \"\"\"\n",
    "\n",
    "    singlestemmed = []\n",
    "    multistemmed = []\n",
    "    \n",
    "    # get list of trees from pts path\n",
    "    trees = glob.glob(\"%s/*ply\" % pts_path)\n",
    "    \n",
    "    for tree in trees:\n",
    "        tree_num = tree.split(\"_\")[-1].split(\".\")[0]\n",
    "        \n",
    "        # if the treeID ends in a letter, it is not single stem\n",
    "        if tree_num[-1].isalpha():\n",
    "            multistemmed.append(tree)\n",
    "        else:\n",
    "            singlestemmed.append(tree)\n",
    "\n",
    "    return singlestemmed, multistemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_convexhull(tree, ptspath, hgt=1.3):\n",
    "    \"\"\"\n",
    "    Function to calculate and plot convex hulls for each tree in a list of trees.\n",
    "    \n",
    "    Returns: \n",
    "        dbh - value for diameter at the specified height \n",
    "        verts - number of vertices used in calculating convex hull \n",
    "        pts_dbh - the 3D point dataset for the given slice (for plotting)\n",
    "    \"\"\"\n",
    "\n",
    "    # Get treeID and ensure inputted format is correct\n",
    "    tree, tree_num = construct_treeid(tree, ptspath)\n",
    "\n",
    "    # get DBH from full cloud to preserve max number of hits on stem slice\n",
    "    pts = load_pts(tree,False,False)\n",
    "\n",
    "    # extract part of df with z values between 1.27 and 1.33 m by default - Tansey et al. 2009, Calders et al. 2015\n",
    "    pts_dbh = pts[(pts['z'] > pts['z'].min() + hgt - 0.03) & (pts['z'] < pts['z'].min() + hgt + 0.03)]\n",
    "    xy_dbh_arr = np.asarray(pts_dbh[['x', 'y']])\n",
    "\n",
    "    # Calculate DBH (convex hull) using scipy ConvexHull\n",
    "    circum = ConvexHull(xy_dbh_arr)\n",
    "    \n",
    "    # calculate return values\n",
    "    dbh = circum.area / np.pi\n",
    "    verts = len(circum.vertices)\n",
    "                      \n",
    "    return dbh, verts, pts_dbh\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: redo this function so it takes in better args\n",
    "\n",
    "def plot_convexhull(treelist, ptspath, ncols=4, fixedheight = True, hgt=1.3, hgtlist = None):\n",
    "    fig, axs = formatplots(treelist, ncols)\n",
    "    rowind = colind = treeind = 0\n",
    "    \n",
    "    for tree in treelist:\n",
    "        tree, tree_num = construct_treeid(tree, ptspath)\n",
    "        \n",
    "        if fixedheight:\n",
    "            hgt = hgt\n",
    "        else:\n",
    "            hgt = hgtlist[tree_num]\n",
    "        \n",
    "        dbh_cv, verts, pts_dbh, dbh = calc_convexhull(tree, pts_path, hgt=hgt)\n",
    "        xy_dbh_arr = np.asarray(pts_dbh[['x', 'y']])\n",
    "        \n",
    "        \n",
    "        axs[rowind, colind].scatter(pts_dbh['x'], pts_dbh['y']) \n",
    "        \n",
    "        # plot line connecting all the dbh vertices\n",
    "        for simplex in dbh.simplices:\n",
    "            axs[rowind, colind].plot(xy_dbh_arr[simplex, 0], xy_dbh_arr[simplex, 1], 'r--')\n",
    "        \n",
    "        axs[rowind, colind].plot(xy_dbh_arr[dbh.vertices,0], xy_dbh_arr[dbh.vertices,1], 'r--', label=f\"DBH: {dbh.area:.3f}\") # DBH (convex hull)\n",
    "        axs[rowind, colind].plot(xy_dbh_arr[dbh.vertices,0], xy_dbh_arr[dbh.vertices,1], 'ro', label=f\"Vertices: {len(dbh.vertices)}\") # vertices\n",
    "\n",
    "        axs[rowind, colind].set_title(f'DBH (Convex Hull) for {tree_num}')\n",
    "        axs[rowind, colind].legend(fontsize='xx-large', loc='upper left')\n",
    "        axs[rowind, colind].axis('equal')\n",
    "\n",
    "        treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_dbh(tree, ptspath, step=0.1, lim=15):\n",
    "    \"\"\"\n",
    "    Start from bottom of tree and calculate convex hull in 0.06m slices separated\n",
    "    by defined step size until limit. \n",
    "    \"\"\"\n",
    "\n",
    "    # Get treeID\n",
    "    tree, tree_num = construct_treeid(tree, ptspath)\n",
    "\n",
    "    #get DBH from full cloud to preserve max number of hits on stem slice\n",
    "    pts = load_pts(tree,False,False)\n",
    "\n",
    "    # calculate dbh for slices up to smaller of lim or tree height\n",
    "    disc = 0.1\n",
    "    hgt = pts['z'].max() - pts['z'].min()\n",
    "    \n",
    "    # set the limit equal to the smaller of tree height and defined limit\n",
    "    lim = min(hgt, lim)\n",
    "    \n",
    "    # empty lists to hold values for each tree\n",
    "    dbh_list = []\n",
    "    vert_list = []\n",
    "    disc_list = []\n",
    "\n",
    "    # loop upward through height of tree (up to lim)\n",
    "    while disc < lim:\n",
    "        # Calculate DBH (convex hull) for current slice using scipy ConvexHull\n",
    "        pts_slice = pts[(pts['z'] > pts['z'].min() + disc - 0.03) & (pts['z'] < pts['z'].min() + disc + 0.03)]\n",
    "        xy_dbh = np.asarray(pts_slice[['x', 'y']])\n",
    "\n",
    "        if len(xy_dbh) < 3:\n",
    "            print(f'tree {tree_num} had not enough points at disc {disc}')\n",
    "            disc += step\n",
    "            break\n",
    "\n",
    "        circum = ConvexHull(xy_dbh) \n",
    "        \n",
    "        dbh = circum.area / np.pi\n",
    "        verts = len(circum.vertices)\n",
    "\n",
    "        # append data to lists\n",
    "        dbh_list.append(dbh)\n",
    "        vert_list.append(verts)\n",
    "        disc_list.append(round(disc, 1))\n",
    "\n",
    "        # move up to next slice  \n",
    "        disc += step\n",
    "    \n",
    "    df_out = pd.DataFrame(list(zip(dbh_list, vert_list, disc_list)), columns = ['DBH_CV', 'DBH_Verts', 'Slice_Hgt'])\n",
    "    \n",
    "    return df_out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iter_dbh(iterdbh_dict, treeslopes, ncols = 2):\n",
    "    \"\"\"\n",
    "    Takes the output of function iter_dbh and plots results in grid.\n",
    "    \"\"\"\n",
    "    \n",
    "    # sort output of iter_dbh function and get list of unique trees\n",
    "    #iterdbhout.sort_values('TLS_ID', inplace=True)\n",
    "    #uniquetrees = iterdbhout.TLS_ID.unique()\n",
    "    \n",
    "    uniquetrees = iterdbh_dict.keys()\n",
    "    print(uniquetrees)\n",
    "    \n",
    "    # format plotting grid\n",
    "    fig, axs = formatplots(uniquetrees, ncols, shape='horiz')\n",
    "    rowind = colind = treeind = 0\n",
    "\n",
    "    # initiate figure with nrows and ncols\n",
    "    #fig, axs = plt.subplots(nrows, ncols, figsize=(40,180), sharex=True)\n",
    "    #axs[0, 0].set_xlim(right=3)\n",
    "\n",
    "    for tree in uniquetrees:\n",
    "        #treedf = iterdbhout.loc[iterdbhout['TLS_ID'] == tree]\n",
    "        #treedf = iterdbh_dict[tree]\n",
    "        treedf = treeslopes[tree]\n",
    "        \n",
    "        filt = treedf['Flagged'] == 1\n",
    "        buttress = treedf['Buttress'] == 1\n",
    "\n",
    "        # calculate trendline on valid data\n",
    "        # validdf = treedf[~filt]\n",
    "        \n",
    "        # plot data\n",
    "        axs[rowind, colind].scatter(treedf['Slice_Hgt'], treedf['DBH_CV'])\n",
    "        axs[rowind, colind].set_title(f'DBH w Height for {tree}', fontsize=20)\n",
    "        axs[rowind, colind].set_xlabel('Height of DBH Slice (m)', fontsize=20)\n",
    "        axs[rowind, colind].set_ylabel('Value of DBH Slice (m)', fontsize=20)\n",
    "        axs[rowind, colind].plot(treedf['Slice_Hgt'][filt], treedf['DBH_CV'][filt],'r+', markersize=20)\n",
    "        axs[rowind, colind].plot(treedf['Slice_Hgt'][buttress], treedf['DBH_CV'][buttress],'b+', markersize=20)\n",
    "        \n",
    "        fig.tight_layout()\n",
    "\n",
    "        \n",
    "        treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buttress_hgt(tree, ptspath, step=0.1, lim=15, thresh = 1.0):\n",
    "    \n",
    "    # Get treeID\n",
    "    tree, tree_num = construct_treeid(tree, ptspath)\n",
    "    \n",
    "    # run iterative dbh for specified step size and lim to calc slopes\n",
    "    treedf = iter_dbh(tree, ptspath, step=step, lim=lim)\n",
    "\n",
    "    # calculate slope between pair of dbh measurements\n",
    "    for disc in treedf['Slice_Hgt'].iloc[1:-1]:\n",
    "\n",
    "        # round disc heights to one decimal place\n",
    "        disc = round(disc, 1)\n",
    "        prev_disc = round(disc - step, 1)\n",
    "\n",
    "        # get dbh values at each disc height\n",
    "        val1 = treedf[treedf['Slice_Hgt'] == disc]['DBH_CV'].iloc[0]\n",
    "        val2 = treedf[treedf['Slice_Hgt'] == prev_disc]['DBH_CV'].iloc[0]\n",
    "\n",
    "        # calculate slope between dbh measurements\n",
    "        slope = (val1 - val2) / step\n",
    "\n",
    "        # make new column in treedf for slope\n",
    "        treedf.loc[treedf['Slice_Hgt'] == prev_disc, 'slope'] = slope\n",
    "\n",
    "        # flag slope values greater than given threshold\n",
    "        if abs(slope) > thresh:\n",
    "            treedf.loc[treedf['Slice_Hgt'] == prev_disc, 'Buttress'] = 1\n",
    "\n",
    "        else:\n",
    "            treedf.loc[treedf['Slice_Hgt'] == prev_disc, 'Buttress'] = 0\n",
    "        \n",
    "    # if first three points are not flagged as buttress points:\n",
    "    if all(np.array(treedf['Buttress'].iloc[0:3]) == np.array([0, 0, 0])):\n",
    "        # set buttress height to 0.0 (i.e. bottom of tree stem)\n",
    "        buttress_index = 0\n",
    "        buttress_hgt = 0.0\n",
    "\n",
    "    # otherwise, find height of top of buttress\n",
    "    else:\n",
    "        # find first sequence of three 0's (non-buttress points) \n",
    "        val_find = np.array([0, 0, 0])\n",
    "        df_condition = treedf['Buttress'].rolling(3).apply(lambda g: all(g.astype(int) == val_find), raw=True)\n",
    "\n",
    "        # get the index where three 0's first occurs and height of buttress\n",
    "        buttress_index = np.where(df_condition == 1)[0][0]\n",
    "        buttress_hgt = treedf['Slice_Hgt'].iloc[buttress_index]\n",
    "\n",
    "               \n",
    "    return buttress_hgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: lim should be set outside functions to be equal to default or tree height, whichever is less\n",
    "def flag_irregular(tree, ptspath, step=0.1, thresh=1.0, lim=3):\n",
    "    \"\"\"\n",
    "    Flag any irregular parts of the tree stem to avoid using them for dbh.\n",
    "    \n",
    "    Using a rolling mean to easily account for/ avoid the influence of extremely \n",
    "    small outliers that may be caused by data occlusion. \n",
    "    \"\"\"\n",
    "    # run iterative dbh for specified step size and lim to calc slopes\n",
    "    treedf = iter_dbh(tree, ptspath, step=step, lim=lim)\n",
    "    \n",
    "    # get buttress height and subset dataframe to parts over the buttress\n",
    "    buttress_hgt = get_buttress_hgt(tree, ptspath, step=0.1, lim=15, thresh=1.0)\n",
    "    over_buttress = treedf[treedf[\"Slice_Hgt\"] > buttress_hgt]\n",
    "    \n",
    "    # calculate a max dbh threshold - no greater 2 * minimum rolling mean dbh\n",
    "    dbh_thresh = np.nanmin(over_buttress['DBH_CV'].rolling(3).mean()) * 2\n",
    "    \n",
    "    # create a filter using this threshold\n",
    "    dbh_filt = treedf['DBH_CV'] > dbh_thresh\n",
    "    \n",
    "    # flag values greater than the threshold\n",
    "    treedf.loc[dbh_filt, 'Flagged'] = 1\n",
    "    treedf.loc[~dbh_filt, 'Flagged'] = 0\n",
    "\n",
    "    return treedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dbh_hgt(tree, ptspath):\n",
    "    \"\"\"\n",
    "    Take output of iter_dbh function and determine ideal height from which to calculate dbh\n",
    "    \n",
    "    Note: the rolling search doesn't seem to work if there is no buttress so this is broken up into \n",
    "    if else - TODO: but could simplify if I figure out how to fix the else part of the code to work\n",
    "    for non buttressed trees too \n",
    "    \"\"\"\n",
    "    # Get treeID\n",
    "    tree, tree_num = construct_treeid(tree, ptspath)\n",
    "\n",
    "    # get buttress height\n",
    "    buttress_hgt = get_buttress_hgt(tree, ptspath, step=0.1, lim=15, thresh=1.0)\n",
    "    \n",
    "    # flag irregular points to be excluded from dbh height consideration\n",
    "    treedf = flag_irregular(tree, ptspath, step=0.1, thresh=1.0, lim=15)\n",
    "\n",
    "    # subset treedf to only above buttress height\n",
    "    over_buttress = treedf[treedf[\"Slice_Hgt\"] > buttress_hgt]\n",
    "    \n",
    "    # select only non-flagged data\n",
    "    filt = over_buttress['Flagged'] == 1\n",
    "    validdf = over_buttress[~filt]\n",
    "\n",
    "    # get suitable value closest to 1.3m to use for dbh height\n",
    "    disclist = list(validdf['Slice_Hgt'])\n",
    "    dif = lambda disclist : abs(disclist - 1.3)\n",
    "    hgt = min(disclist, key=dif)\n",
    "\n",
    "    return hgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2D(trees, ptspath, ncols=4, dbh=True, iterdbh=False, hgtlist = None):\n",
    "    \"\"\"\n",
    "    Function to plot tree point clouds in x/z or y/z direction\n",
    "    \n",
    "    TODO: make iter dbh function work on just one tree - then call iter_dbh from within this function\n",
    "    \"\"\"\n",
    "    # determine number of rows to plot figures\n",
    "    fig, axs = formatplots(trees, ncols)\n",
    "    rowind = colind = treeind = 0\n",
    "\n",
    "    # loop through trees\n",
    "    for tree in trees:\n",
    "        # Get treeID\n",
    "        tree, tree_num = construct_treeid(tree, ptspath)\n",
    "\n",
    "        #get DBH from full cloud to preserve max number of hits on stem slice\n",
    "        pts = load_pts(tree,False,False)\n",
    "\n",
    "        # calculate tree height\n",
    "        tree_hgt = pts['z'].max() - pts['z'].min()\n",
    "            \n",
    "        max_pt = pts[pts['z'] == pts['z'].max()]\n",
    "\n",
    "        # plot scatter plot of points in x and z \n",
    "        axs[rowind, colind].scatter(pts['x'], pts['z'], s=3) \n",
    "        if dbh: \n",
    "            pts_dbh = pts[(pts['z'] > pts['z'].min() + 1.27) & (pts['z'] < pts['z'].min() + 1.33)]\n",
    "            axs[rowind, colind].plot(pts_dbh['x'], pts_dbh['z'], 'ro', markersize=3, label=f\"DBH Slice\")\n",
    "        \n",
    "        if iterdbh: \n",
    "            hgt = hgtlist[tree_num]\n",
    "            pts_hgt = pts[(pts['z'] > pts['z'].min() + hgt - 0.03) & (pts['z'] < pts['z'].min() + hgt + 0.03)]\n",
    "            axs[rowind, colind].plot(pts_hgt['x'], pts_hgt['z'], 'yo', markersize=3, label=f\"HGT Slice\")\n",
    "            \n",
    "\n",
    "        axs[rowind, colind].annotate(f'Height: {tree_hgt:.2f}', xy=(max_pt['x'].iloc[0], max_pt['z'].iloc[0]), va='top', ha='left', size=15)\n",
    "        axs[rowind, colind].legend(fontsize='xx-large', loc='upper left')\n",
    "        axs[rowind, colind].set_title(f'{tree_num} ({rowind}, {colind})')\n",
    "\n",
    "        # increase tree index to get to next tree\n",
    "        treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wytham Woods\n",
    "pts_path = \"/Users/snix/Documents/2021-2023/UCL/Dissertation/DATA_clouds_ply\"\n",
    "qsm_path = \"/Users/snix/Documents/2021-2023/UCL/Dissertation/DATA_QSM_opt/\"\n",
    "tree_csv_path = \"/Users/snix/Documents/2021-2023/UCL/Dissertation/Calders Et Al/analysis_and_figures/trees_summary.csv\"\n",
    "\n",
    "# Tropical Trees\n",
    "trop_pts_path = '/Users/snix/Documents/2021-2023/UCL/Dissertation/Tropical_manual_ply'\n",
    "trop_csv_path = '/Users/snix/Documents/2021-2023/UCL/Dissertation/MLA01_man.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singlestemmed: 585\n",
      "Multistemmed: 291\n"
     ]
    }
   ],
   "source": [
    "trees = glob.glob(\"%s/*ply\" % pts_path)\n",
    "singlestemmed, multistemmed = wytham_singlemulti(pts_path)\n",
    "\n",
    "print(f'Singlestemmed: {len(singlestemmed)}')\n",
    "print(f'Multistemmed: {len(multistemmed)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trop_trees = glob.glob(\"%s/*wood.ply\" % trop_pts_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running - New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbh threshold: 0.2915608991204565\n",
      "tree T38, res: 1.3\n",
      "dbh threshold: 0.8433717620074527\n",
      "tree T39, res: 1.3\n",
      "dbh threshold: 0.2866808305943029\n",
      "tree T187, res: 1.3\n",
      "dbh threshold: 0.19436154555876226\n",
      "tree T115, res: 1.3\n",
      "tree T94 had not enough points at disc 13.699999999999967\n",
      "tree T94 had not enough points at disc 13.699999999999967\n",
      "tree T94 had not enough points at disc 13.699999999999967\n",
      "dbh threshold: 0.27856595834116643\n",
      "tree T94, res: 1.3\n",
      "dbh threshold: 0.394078567984411\n",
      "tree T304, res: 1.3\n",
      "dbh threshold: 0.8739579252501121\n",
      "tree T95, res: 1.3\n",
      "dbh threshold: 0.1590663212322125\n",
      "tree T379, res: 1.3\n",
      "dbh threshold: 0.30163365519819535\n",
      "tree T120, res: 1.3\n",
      "dbh threshold: 0.252252627527427\n",
      "tree T347, res: 1.3\n"
     ]
    }
   ],
   "source": [
    "tree_data = {}\n",
    "\n",
    "for tree in trop_trees[0:10]:\n",
    "    tree, tree_num = construct_treeid(tree, trop_pts_path)\n",
    "    \n",
    "    dbh, verts, pts_dbh = calc_convexhull(tree, trop_pts_path, hgt=1.3)\n",
    "    \n",
    "    hgt = get_dbh_hgt(tree, trop_pts_path)\n",
    "    \n",
    "    tree_data[tree_num] = hgt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Convex Hull at 1.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict = {}\n",
    "\n",
    "# ENGLAND TREES\n",
    "for tree in singlestemmed:\n",
    "    # construct tree id\n",
    "    tree, tree_num = construct_treeid(tree, pts_path)\n",
    "    \n",
    "    # calc convex hull\n",
    "    dbh_cv, verts, pts_dbh, dbh = calc_convexhull(tree, pts_path, hgt=1.3)\n",
    "    \n",
    "    datadict[tree_num] = {'dbh_cv': dbh_cv, 'verts': verts}\n",
    "\n",
    "\n",
    "# MALAYSIA TREES\n",
    "datadict2 = {}\n",
    "\n",
    "for tree in trop_trees:\n",
    "    # construct tree id\n",
    "    tree, tree_num = construct_treeid(tree, trop_pts_path)\n",
    "    \n",
    "    # calc convex hull\n",
    "    dbh_cv, verts, pts_dbh, dbh = calc_convexhull(tree, trop_pts_path, hgt=1.3)\n",
    "    \n",
    "    datadict2[tree_num] = {'dbh_cv': dbh_cv, 'verts': verts}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convexhull(trop_trees[0:12], trop_pts_path, ncols=4, fixedheight = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf = pd.DataFrame.from_dict(datadict2, orient='index')\n",
    "\n",
    "datadf = datadf.reset_index()\n",
    "datadf = datadf.rename(columns={\"index\": \"TLS_ID\"})\n",
    "\n",
    "\n",
    "datadf.sort_values('dbh_cv', ascending=False, inplace=True)\n",
    "\n",
    "datadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appears that larger dbh values are coming from issues with pt clouds\n",
    "# sort by dbh to highlight trees with issues \n",
    "datadf.sort_values('dbh_cv', ascending=False, inplace=True)\n",
    "\n",
    "# make new list of just the top 12 'large dbh' trees and plot convex hull results\n",
    "bigtrees = list(datadf.iloc[0:12, 0])\n",
    "\n",
    "print(bigtrees)\n",
    "\n",
    "bigtree_datadict = {}\n",
    "\n",
    "for tree in bigtrees:\n",
    "    # construct tree id\n",
    "    tree, tree_num = construct_treeid(tree, trop_pts_path)\n",
    "    \n",
    "    # calc convex hull\n",
    "    dbh_cv, verts, pts_dbh, dbh = calc_convexhull(tree, trop_pts_path, hgt=1.3)\n",
    "    bigtree_datadict[tree_num] = {'dbh_cv': dbh_cv, 'verts': verts}\n",
    "\n",
    "plot_convexhull(bigtrees, trop_pts_path, ncols=4, fixedheight = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative DBH by Slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with tree ids as keys and values of dataframes of dbh along stem\n",
    "# TODO: make these not overwrite each other - how should I do tropical and england together :(\n",
    "iterdbh_dict = {}\n",
    "\n",
    "# ENGLAND TREES\n",
    "for tree in singlestemmed:\n",
    "    tree_num, iterdbhout = iter_dbh(tree, pts_path, step=0.1, lim=3)\n",
    "    \n",
    "    iterdbh_dict[tree_num] = iterdbhout\n",
    "    \n",
    "\n",
    "# MALAYSIA TREES\n",
    "iterdbh_dict_2 = {}\n",
    "\n",
    "for tree in trop_trees:\n",
    "    tree_num, iterdbhout = iter_dbh(tree, trop_pts_path, step=0.1, lim=15)\n",
    "    \n",
    "    iterdbh_dict_2[tree_num] = iterdbhout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for just a few trees to get a dictionary to plot\n",
    "iterdbh_dict_plot = {}\n",
    "for tree in bigtrees[5:11]:\n",
    "    # bump up limit for tropical trees to make sure we get above the buttress\n",
    "    tree_num, iterdbhout = iter_dbh(tree, trop_pts_path, step=0.1, lim=15)\n",
    "    \n",
    "    iterdbh_dict_plot[tree_num] = iterdbhout\n",
    "    \n",
    "plot_iter_dbh(iterdbh_dict_plot, ncols = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK BUTTRESS\n",
    "treeslopes = flag_buttress(iterdbh_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeslopes['T95']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nplots = len(treeslopes.keys())\n",
    "ncols = 4\n",
    "fig, axs = formatplots(treeslopes.keys(), ncols)\n",
    "rowind = colind = treeind = 0\n",
    "\n",
    "plottrees = list(treeslopes.keys())\n",
    "\n",
    "# loop through trees\n",
    "for tree in plottrees[0:20]:\n",
    "    treedf = treeslopes[tree]\n",
    "    \n",
    "    axs[rowind, colind].scatter(treedf['Slice_Hgt'], treedf['slope'])\n",
    "\n",
    "    axs[rowind, colind].set_title(f'{tree} ({rowind}, {colind})')\n",
    "\n",
    "    # increase tree index to get to next tree\n",
    "    treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for just a few trees to get a dictionary to plot\n",
    "iterdbh_dict_plot = {}\n",
    "\n",
    "for tree in plottrees[0:20]:\n",
    "    # bump up limit for tropical trees to make sure we get above the buttress\n",
    "    tree_num, iterdbhout = iter_dbh(tree, trop_pts_path, step=0.1, lim=15)\n",
    "    \n",
    "    iterdbh_dict_plot[tree_num] = iterdbhout\n",
    "    \n",
    "plot_iter_dbh(iterdbh_dict_plot, treeslopes, ncols = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbh_hgts = get_dbh_hgt(iterdbh_dict_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2D(plottrees[0:20], trop_pts_path, ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get heights at which to record dbhh for full dataset\n",
    "treedf, dbh_hgts = get_dbh_hgt(iterdbh_dict_2, check_buttress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now calculate dbh at the specified heights to avoid branches\n",
    "big_datadict = {}\n",
    "\n",
    "for tree in singlestemmed:\n",
    "    # construct tree id\n",
    "    tree, tree_num = construct_treeid(tree, pts_path)\n",
    "    \n",
    "    dbh_hgt = dbh_hgts[tree_num]\n",
    "    \n",
    "    # calc convex hull\n",
    "    dbh_cv, verts, pts_dbh, dbh = calc_convexhull(tree, pts_path, hgt=dbh_hgt)\n",
    "    \n",
    "    big_datadict[tree_num] = {'dbh_cv': dbh_cv, 'verts': verts}\n",
    "\n",
    "plot_convexhull(singlestemmed[0:8], pts_path, ncols=4, fixedheight = False, hgtlist = dbh_hgts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2D(singlestemmed[0:8], pts_path, ncols=4, dbh=True, iterdbh=True, hgtlist = dbh_hgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert full data dictionary into dataframe for analysis and predictions\n",
    "\n",
    "big_datadf = pd.DataFrame.from_dict(big_datadict, orient='index')\n",
    "\n",
    "\n",
    "big_datadf = big_datadf.reset_index()\n",
    "big_datadf = big_datadf.rename(columns={\"index\": \"TLS_ID\"})\n",
    "\n",
    "big_datadf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBH (Convex Hull) Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict DBH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treecsv = pd.read_csv(tree_csv_path)\n",
    "\n",
    "\n",
    "treecsv = treecsv[['TLS_ID','stemlocx_[m]', 'stemlocy_[m]', 'DBH_TLS_[m]',\n",
    "                    'Hgt_pts_[m]', 'VerticalCrownProjectedArea_pts_[m2]', 'Vol_QSM_avg_[m3]',\n",
    "                    'Vol_QSM_D0_25mm_avg_[m3]', 'Vol_QSM_D25_50mm_avg_[m3]', 'Vol_QSM_D50_75mm_avg_[m3]',\n",
    "                    'Vol_QSM_D75_100mm_avg_[m3]', 'Vol_QSM_D100_200mm_avg_[m3]', 'Vol_QSM_D200_500mm_avg_[m3]',\n",
    "                    'Vol_QSM_D500_1000mm_avg_[m3]', 'Vol_QSM_D1000mm_avg_[m3]']]\n",
    "\n",
    "\n",
    "treedata = pd.merge(big_datadf, treecsv, on ='TLS_ID')\n",
    "\n",
    "treedata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Convex Hull with \"DBH_TLS[m]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only single stem trees from csv \n",
    "#ss_treedata = treecsv[treecsv[['TLS_ID']].apply(lambda x: x[-1].isdigit(), axis=1)]\n",
    "\n",
    "#smalltrees = treedata[treedata['DBH_CV']<4]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(treedata['dbh_cv'], treedata['DBH_TLS_[m]'])\n",
    "plt.xlabel('DBH from Convex Hull', fontsize=20)\n",
    "plt.ylabel('DBH from TLS', fontsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (Predicting DBH from QSMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "\n",
    "# Labels are the values we want to predict\n",
    "labels = np.array(treedata['dbh_cv'])\n",
    "\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "features = treedata.drop(['TLS_ID','dbh_cv', 'DBH_TLS_[m]'], axis = 1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns)\n",
    "\n",
    "# Convert to numpy array\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2))\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / test_labels)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf_results = pd.DataFrame(mape, predictions, test_labels)\n",
    "\n",
    "rf_results = pd.DataFrame({'mape': mape, 'errors': errors,'predictions': predictions, 'test_labels': test_labels}, columns=['mape', 'errors', 'predictions', 'test_labels'])\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(rf_results['predictions'], rf_results['test_labels'])\n",
    "plt.xlabel('Predictions (DBH)', fontsize=20)\n",
    "plt.ylabel('Measured Values (DBH)', fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(rf_results['test_labels'], rf_results['errors'])\n",
    "plt.xlabel('Measured Values (DBH)', fontsize=20)\n",
    "plt.ylabel('Errors', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge rf results back with original dataset\n",
    "#https://stackoverflow.com/questions/40729162/merging-results-from-model-predict-with-original-pandas-dataframe\n",
    "# TODO: in final - re-run model on full dataset and split into training vs testing\n",
    "\n",
    "df_out = pd.merge(treedata,rf_results[['mape', 'errors', 'predictions']],how = 'left',left_index = True, right_index = True)\n",
    "\n",
    "#rf_results.shape\n",
    "df_out.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(df_out['verts'], df_out['errors'])\n",
    "plt.xlabel('DBH Verts', fontsize=20)\n",
    "plt.ylabel('Errors', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
