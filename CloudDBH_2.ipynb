{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CloudDBH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import open3d as o3d\n",
    "\n",
    "from shapely.ops import cascaded_union, polygonize\n",
    "from shapely.geometry import Polygon, LineString\n",
    "import shapely.geometry as geometry\n",
    "\n",
    "from scipy import optimize\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d, Delaunay\n",
    "import scipy.io as sio\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import math\n",
    "from math import pi\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from descartes import PolygonPatch\n",
    "\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Calders et al. 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from Calders et al\n",
    "# nearest neighbour needed for outlier removal\n",
    "def nn(x,rad):\n",
    "    \"\"\"\n",
    "    Function from TLS_Inventory.\n",
    "    \n",
    "    x: a numpy array, rad: radius to search for neighbors within\n",
    "    \n",
    "    My understanding - can either specify number of neighbors to return (in sklearn NearestNeighbors),\n",
    "    or return all the neighbors and then filter to return only those within a specified distance radius\n",
    "    \"\"\"\n",
    "    nbrs = NearestNeighbors(algorithm='auto', metric='euclidean').fit(x) \n",
    "    distances, indices = nbrs.radius_neighbors(radius=rad) #find all neighbours within this search radius\n",
    "    \n",
    "    #output explained: https://stackoverflow.com/questions/53886289/confused-with-the-output-of-sklearn-neighbors-nearestneighbors \n",
    "    return distances, indices\n",
    "\n",
    "\n",
    "def load_pts(cloud,downsample=False,outliers=False): \n",
    "    \"\"\"\n",
    "    Function from TLS_Inventory that reads the point cloud data for single tree at a time and\n",
    "    returns pandas dataframe with the points split into x, y, z coordinates. \n",
    "    \n",
    "    cloud: point cloud from a single tree - note, point cloud data should be pre-processed and \n",
    "           split into individual trees prior to using this function.\n",
    "    downsample (Optional): Default is False, meaning all the points are retained. If \n",
    "                           True, the point clouds are sampled to keep only fraction of points.\n",
    "    outliers (Optional): Default is False, meaning outliers are retained. If True, \n",
    "    \"\"\"\n",
    "    # read the point cloud data for single tree and save as pandas df with columns for x, y, z coords\n",
    "    dftemp=o3d.io.read_point_cloud(cloud)\n",
    "    df=pd.DataFrame(data=np.asarray(dftemp.points),columns=['x', 'y', 'z']) #access the points\n",
    "    \n",
    "    # Optional downsampling\n",
    "    if downsample:\n",
    "        df=df.sample(frac=0.1) # keep 10pct of points\n",
    "    \n",
    "    # Optional outlier removal \n",
    "    if outliers: #remove outliers\n",
    "        xy=df.iloc[:,0:2].values # takes ALL rows, first and second column - x and y - and .values converts to numpy array\n",
    "        dist, indi = nn(xy,0.5) # get nearest neighbors within search radius of 0.5\n",
    "        cnt=[len(i) for i in indi] # count the kNN within the search radius\n",
    "        cnt = pd.DataFrame({'count':cnt})\n",
    "        \n",
    "        # set threshold for the number of neighbors we want to keep \n",
    "        threshold=df.shape[0]*0.0001 #1 neighbor for every 10 000 pts\n",
    "        \n",
    "        removed=sum(np.array(cnt)<threshold)\n",
    "        df=df[np.array(cnt)>=threshold]\n",
    "        print(\"Removed %i outliers using kNN threshold %.2f\" % (removed[0], threshold-1))\n",
    "    \n",
    "    return df #return pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ply(fp, newline=None):\n",
    "\n",
    "    line = open(fp, encoding='ISO-8859-1').readline()\n",
    "    newline = '\\n' if line == 'ply\\n' else None\n",
    "\n",
    "    return read_ply_(fp, newline)\n",
    "    \n",
    "def read_ply_(fp, newline):\n",
    "\n",
    "    open_file = open(fp, \n",
    "                     encoding='ISO-8859-1',\n",
    "                     newline=newline) \n",
    "\n",
    "    with open_file as ply:\n",
    " \n",
    "        length = 0\n",
    "        prop = []\n",
    "        dtype_map = {'double':'d', 'float64':'f8', 'float32':'f4', 'float': 'f4', 'uchar': 'B', 'int':'i'}\n",
    "        dtype = []\n",
    "        fmt = 'binary'\n",
    "\n",
    "        for i, line in enumerate(ply.readlines()):\n",
    "            length += len(line)\n",
    "            if i == 1:\n",
    "                if 'ascii' in line:\n",
    "                    fmt = 'ascii' \n",
    "            if 'element vertex' in line: N = int(line.split()[2])\n",
    "            if 'property' in line: \n",
    "                typ = line.split()[1]\n",
    "                dtype.append(typ if typ not in dtype_map.keys() else dtype_map[typ])\n",
    "                prop.append(line.split()[2])\n",
    "            if 'element face' in line:\n",
    "                raise Exception('.ply appears to be a mesh')\n",
    "            if 'end_header' in line: break\n",
    "    \n",
    "        ply.seek(length)\n",
    "\n",
    "        if fmt == 'binary':\n",
    "            arr = np.fromfile(ply, dtype=','.join(dtype))\n",
    "        else:\n",
    "            arr = np.loadtxt(ply)\n",
    "        df = pd.DataFrame(data=arr)\n",
    "        df.columns = prop\n",
    "\n",
    "    return df\n",
    "\n",
    "def write_ply(output_name, pc, comments=[]):\n",
    "\n",
    "    cols = ['x', 'y', 'z']\n",
    "    pc[['x', 'y', 'z']] = pc[['x', 'y', 'z']].astype('f8')\n",
    "\n",
    "    with open(output_name, 'w') as ply:\n",
    "\n",
    "        ply.write(\"ply\\n\")\n",
    "        ply.write('format binary_little_endian 1.0\\n')\n",
    "        ply.write(\"comment Author: Phil Wilkes\\n\")\n",
    "        for comment in comments:\n",
    "            ply.write(\"comment {}\\n\".format(comment))\n",
    "        ply.write(\"obj_info generated with pcd2ply.py\\n\")\n",
    "        ply.write(\"element vertex {}\\n\".format(len(pc)))\n",
    "        ply.write(\"property float64 x\\n\")\n",
    "        ply.write(\"property float64 y\\n\")\n",
    "        ply.write(\"property float64 z\\n\")\n",
    "        if 'red' in pc.columns:\n",
    "            cols += ['red', 'green', 'blue']\n",
    "            pc[['red', 'green', 'blue']] = pc[['red', 'green', 'blue']].astype('i')\n",
    "            ply.write(\"property int red\\n\")\n",
    "            ply.write(\"property int green\\n\")\n",
    "            ply.write(\"property int blue\\n\")\n",
    "        for col in pc.columns:\n",
    "            if col in cols: continue\n",
    "            try:\n",
    "                pc[col] = pc[col].astype('f8')\n",
    "                ply.write(\"property float64 {}\\n\".format(col))\n",
    "                cols += [col]\n",
    "            except:\n",
    "                pass\n",
    "        ply.write(\"end_header\\n\")\n",
    "\n",
    "    with open(output_name, 'ab') as ply:\n",
    "        ply.write(pc[cols].to_records(index=False).tobytes()) \n",
    "\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "    #import sys\n",
    "    #print(read_ply(sys.argv[1]).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatplots(trees, ncols=4, shape='square'):\n",
    "    \"\"\"\n",
    "    Returns matplotlib fig and axs objects with desired shape and number of subplots.\n",
    "    \n",
    "    Args:\n",
    "    trees - the list of trees (or other objects) that need to be plotted.\n",
    "            the length of this list will be the number of subplots returned.\n",
    "    ncols - the desired number of columns in the subplot grid \n",
    "    shape - optional argument describing the shape of the subplots. \n",
    "            Options are \"square\", \"horiz\", or \"vert\"\n",
    "    \n",
    "    Returns: \n",
    "    fig, axs - matplotlib objects corresponding to the grid of subplots\n",
    "    \"\"\"\n",
    "    # determine number of subplots\n",
    "    nplots = len(trees)\n",
    "    \n",
    "    # determine number of rows \n",
    "    if(nplots % ncols) != 0:\n",
    "        nrows = int(nplots / ncols + 1)\n",
    "    else:\n",
    "        nrows = int(nplots / ncols)\n",
    "\n",
    "    if shape == 'horiz':\n",
    "        # initiate figure with nrows and ncols\n",
    "        fig, axs = plt.subplots(nrows, ncols, figsize=(20,nrows*4))\n",
    "        fig.tight_layout()\n",
    "    if shape == 'vert':\n",
    "        # initiate figure with nrows and ncols\n",
    "        fig, axs = plt.subplots(nrows, ncols, figsize=(20,nrows*10))\n",
    "        fig.tight_layout()\n",
    "    if shape == 'square':\n",
    "        # initiate figure with nrows and ncols\n",
    "        fig, axs = plt.subplots(nrows, ncols, figsize=(20,nrows*5))\n",
    "        fig.tight_layout()\n",
    "    \n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_inds(treeind, rowind, colind, ncols):\n",
    "    \"\"\"\n",
    "    Function to be used in combination with the formatplots function. This function \n",
    "    updates indices as necessary to loop through the subplots and plot data in each. \n",
    "    \"\"\"\n",
    "    # increase tree index to get to next tree\n",
    "    treeind += 1\n",
    "\n",
    "    if (treeind % ncols) != 0:\n",
    "        colind += 1\n",
    "    else:\n",
    "        rowind +=1 \n",
    "        colind = 0\n",
    "    \n",
    "    return treeind, colind, rowind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_treeid(tree, ptspath):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # For wytham woods tree dataset\n",
    "    if str(ptspath).__contains__('DATA_clouds_ply'):\n",
    "        if tree[-4:] != '.ply':\n",
    "            tree = str(ptspath) + '/wytham_winter_' + str(tree) + '.ply' # TODO: don't hard code this\n",
    "        else:\n",
    "            tree = tree # TODO: make this more robust\n",
    "\n",
    "        tree_num = tree.split(\"_\")[-1].split(\".\")[0]\n",
    "    \n",
    "    # For tropical tree dataset - must have the leaf files already removed\n",
    "    if str(ptspath).__contains__('Tropical_manual_ply'):\n",
    "        if tree[-4:] != '.ply':\n",
    "            tree = str(ptspath) + '/MLA01_2018_' + str(tree) + '.wood.ply' # TODO: don't hard code this\n",
    "        else:\n",
    "            tree = str(tree) # TODO: make this more robust\n",
    "\n",
    "        tree_num = tree.split(\"_\")[-1].split(\".\")[0]\n",
    "    return tree, tree_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wytham_singlemulti(ptspath):\n",
    "    \"\"\"\n",
    "    Function that splits the wytham woods dataset into single vs multistemmed trees \n",
    "    with string operations based on how the wytham dataset is named.\n",
    "    \"\"\"\n",
    "\n",
    "    singlestemmed = []\n",
    "    multistemmed = []\n",
    "    \n",
    "    # get list of trees from pts path\n",
    "    trees = glob.glob(\"%s/*ply\" % ptspath)\n",
    "    \n",
    "    for tree in trees:\n",
    "        tree_num = tree.split(\"_\")[-1].split(\".\")[0]\n",
    "        \n",
    "        # if the treeID ends in a letter, it is not single stem\n",
    "        if tree_num[-1].isalpha():\n",
    "            multistemmed.append(tree)\n",
    "        else:\n",
    "            singlestemmed.append(tree)\n",
    "\n",
    "    return singlestemmed, multistemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_convexhull(tree, ptspath, hgt=1.3, cluster_pts=None):\n",
    "    \"\"\"\n",
    "    Function to calculate and plot convex hulls for each tree in a list of trees.\n",
    "    \n",
    "    Returns: \n",
    "        dbh - value for diameter at the specified height \n",
    "        verts - number of vertices used in calculating convex hull \n",
    "        pts_dbh - the 3D point dataset for the given slice (for plotting)\n",
    "    \"\"\"\n",
    "\n",
    "    # Get treeID and ensure inputted format is correct\n",
    "    tree, tree_num = construct_treeid(tree, ptspath)\n",
    "\n",
    "    # get DBH from full cloud to preserve max number of hits on stem slice\n",
    "    pts = load_pts(tree,False,False)\n",
    "    \n",
    "    # extract part of df with z values between 1.27 and 1.33 m by default - Tansey et al. 2009, Calders et al. 2015\n",
    "    pts_dbh = pts[(pts['z'] > pts['z'].min() + hgt - 0.03) & (pts['z'] < pts['z'].min() + hgt + 0.03)]\n",
    "    \n",
    "    # outlier removal\n",
    "    xy=pts_dbh.iloc[:,0:2].values\n",
    "    dist, indi = nn(xy,0.1)\n",
    "    cnt=[len(i) for i in indi] #count the kNN within the search radius\n",
    "    cnt = pd.DataFrame({'count':cnt})\n",
    "    threshold=5\n",
    "    removed=sum(np.array(cnt)<threshold)\n",
    "    pts_dbh=pts_dbh[np.array(cnt)>=threshold]\n",
    "    if removed[0] > 0: print(f\"{tree_num}: Removed {removed[0]} outliers from DBH slice using kNN threshold {threshold-1}\")\n",
    "    \n",
    "    \n",
    "    if cluster_pts is not None:\n",
    "        pts_dbh = cluster_pts\n",
    "    \n",
    "    xy_dbh_arr = np.asarray(pts_dbh[['x', 'y']])\n",
    "\n",
    "    # Calculate DBH (convex hull) using scipy ConvexHull\n",
    "    hull = ConvexHull(xy_dbh_arr)\n",
    "    \n",
    "    # calculate return values\n",
    "    dbh = hull.area / np.pi\n",
    "    verts = len(hull.vertices)\n",
    "    \n",
    "    # return distances between vertices\n",
    "    dists = []\n",
    "    for i in range (len(hull.vertices) - 1):\n",
    "        #print(xy_dbh_arr[i, 0], xy_dbh_arr[i, 1])\n",
    "        \n",
    "        vert1 = [xy_dbh_arr[i, 0], xy_dbh_arr[i, 1]]\n",
    "        vert2 = [xy_dbh_arr[i+1, 0], xy_dbh_arr[i+1, 1]]\n",
    "        \n",
    "        dist = math.dist(vert1, vert2)\n",
    "        dists.append(dist)\n",
    "    \n",
    "    maxdist = np.max(dists)\n",
    "                      \n",
    "    return dbh, verts, pts_dbh, hull, maxdist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convexhull(treelist, ptspath, ncols=4, fixedheight = True, hgt=1.3, hgtlist = None):\n",
    "    fig, axs = formatplots(treelist, ncols)\n",
    "    rowind = colind = treeind = 0\n",
    "    \n",
    "    for tree in treelist:\n",
    "        tree, tree_num = construct_treeid(tree, ptspath)\n",
    "        \n",
    "        if fixedheight:\n",
    "            hgt = hgt\n",
    "        else:\n",
    "            hgt = hgtlist[tree_num]\n",
    "        \n",
    "        dbh_cv, verts, pts_dbh, hull, maxdist = calc_convexhull(tree, ptspath, hgt=hgt)\n",
    "        xy_dbh_arr = np.asarray(pts_dbh[['x', 'y']])\n",
    "        \n",
    "        \n",
    "        axs[rowind, colind].scatter(pts_dbh['x'], pts_dbh['y']) \n",
    "        \n",
    "        # plot line connecting all the dbh vertices\n",
    "        for simplex in hull.simplices:\n",
    "            axs[rowind, colind].plot(xy_dbh_arr[simplex, 0], xy_dbh_arr[simplex, 1], 'r--')\n",
    "        \n",
    "        #axs[rowind, colind].plot(xy_dbh_arr[hull.vertices,0], xy_dbh_arr[hull.vertices,1], 'r--', label=f\"DBH: {dbh_cv:.3f}\") # DBH (convex hull)\n",
    "        axs[rowind, colind].plot(xy_dbh_arr[hull.vertices,0], xy_dbh_arr[hull.vertices,1], 'ro', label=f\"Vertices: {len(hull.vertices)}\") # vertices\n",
    "\n",
    "        axs[rowind, colind].set_title(f'DBH (Convex Hull) for {tree_num}')\n",
    "        axs[rowind, colind].legend(fontsize='xx-large', loc='upper left')\n",
    "        axs[rowind, colind].axis('equal')\n",
    "\n",
    "        treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_dbh(tree, ptspath, step=0.1, lim=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Start from bottom of tree and calculate convex hull in 0.06m slices separated\n",
    "    by defined step size until limit. \n",
    "    \"\"\"\n",
    "\n",
    "    # Get treeID\n",
    "    tree, tree_num = construct_treeid(tree, ptspath)\n",
    "\n",
    "    #get DBH from full cloud to preserve max number of hits on stem slice\n",
    "    pts = load_pts(tree,False,False)\n",
    "\n",
    "    # calculate dbh for slices up to smaller of lim or tree height\n",
    "    disc = 0.1\n",
    "    hgt = pts['z'].max() - pts['z'].min()\n",
    "    \n",
    "    # set the limit equal to the smaller of tree height and defined limit\n",
    "    lim = min(hgt, lim)\n",
    "    \n",
    "    # empty lists to hold values for each tree\n",
    "    dbh_list = []\n",
    "    vert_list = []\n",
    "    disc_list = []\n",
    "\n",
    "    # loop upward through height of tree (up to lim)\n",
    "    while disc < lim:\n",
    "        # Calculate DBH (convex hull) for current slice using scipy ConvexHull\n",
    "        pts_slice = pts[(pts['z'] > pts['z'].min() + disc - 0.03) & (pts['z'] < pts['z'].min() + disc + 0.03)]\n",
    "        xy_dbh = np.asarray(pts_slice[['x', 'y']])\n",
    "\n",
    "        if len(xy_dbh) < 3:\n",
    "            if verbose: print(f'tree {tree_num} had not enough points at disc {disc}')\n",
    "            dbh_list.append(np.nan)\n",
    "            vert_list.append(np.nan)\n",
    "            disc_list.append(round(disc, 1))\n",
    "            disc += step\n",
    "            continue\n",
    "\n",
    "        circum = ConvexHull(xy_dbh) \n",
    "        \n",
    "        dbh = circum.area / np.pi\n",
    "        verts = len(circum.vertices)\n",
    "\n",
    "        # append data to lists\n",
    "        dbh_list.append(dbh)\n",
    "        vert_list.append(verts)\n",
    "        disc_list.append(round(disc, 1))\n",
    "\n",
    "        # move up to next slice  \n",
    "        disc += step\n",
    "    \n",
    "    df_out = pd.DataFrame(list(zip(dbh_list, vert_list, disc_list)), columns = ['DBH_CV', 'DBH_Verts', 'Slice_Hgt'])\n",
    "    \n",
    "    return df_out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iter_dbh(treelist, ptspath, ncols = 2, step=0.1, slope_thresh=0.1, dbh_thresh=2):\n",
    "    \"\"\"\n",
    "    Takes the output of function iter_dbh and plots results in grid.\n",
    "    \"\"\"\n",
    "    \n",
    "    # format plotting grid\n",
    "    fig, axs = formatplots(treelist, ncols, shape='horiz')\n",
    "    rowind = colind = treeind = 0\n",
    "\n",
    "    for tree in treelist:\n",
    "        \n",
    "        # Get treeID\n",
    "        tree, tree_num = construct_treeid(tree, ptspath)\n",
    "        \n",
    "        # get dataframe of iterdbh function\n",
    "        treedf = iter_dbh(tree, ptspath, step=step, lim=10)\n",
    "        \n",
    "        # get height for dbh for the tree \n",
    "        hgt = get_dbh_hgt(tree, ptspath, step=step, slope_thresh=slope_thresh, dbh_thresh=dbh_thresh)\n",
    "        hgtplot = treedf['Slice_Hgt'] == hgt\n",
    "        \n",
    "        # get buttress height \n",
    "        buttress_hgt, treedf = get_buttress_hgt(tree, ptspath, step=step, lim=10, slope_thresh = slope_thresh)\n",
    "        buttressplot = treedf['Slice_Hgt'] == buttress_hgt\n",
    "        \n",
    "        \n",
    "        # plot data\n",
    "        axs[rowind, colind].scatter(treedf['Slice_Hgt'], treedf['DBH_CV'])\n",
    "        axs[rowind, colind].set_title(f'DBH w Height for {tree_num}, Slope Thresh: {slope_thresh}', fontsize=20)\n",
    "        axs[rowind, colind].set_xlabel('Height of DBH Slice (m)', fontsize=20)\n",
    "        axs[rowind, colind].set_ylabel('Value of DBH Slice (m)', fontsize=20)\n",
    "        axs[rowind, colind].plot(treedf['Slice_Hgt'][hgtplot], treedf['DBH_CV'][hgtplot],'r+', markersize=30)\n",
    "        axs[rowind, colind].plot(treedf['Slice_Hgt'][buttressplot], treedf['DBH_CV'][buttressplot],'b+', markersize=30)\n",
    "        \n",
    "        # plot all flagged points\n",
    "        flagdf = flag_irregular(tree, ptspath, step=step, slope_thresh=slope_thresh, lim=10, dbh_thresh = dbh_thresh)\n",
    "        flagged = flagdf[flagdf['Flagged'] == 1]\n",
    "        axs[rowind, colind].plot(flagged['Slice_Hgt'], flagged['DBH_CV'],'r+', markersize=20)\n",
    "        \n",
    "        \n",
    "        fig.tight_layout()\n",
    "\n",
    "        \n",
    "        treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buttress_hgt(tree, ptspath, step=0.1, lim=10, slope_thresh = 0.1, verbose=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    tree - \n",
    "    ptspath - \n",
    "    step - \n",
    "    lim - \n",
    "    slope_thresh - slope threshold above which points are flagged as part of a buttress\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get treeID\n",
    "    tree, tree_num = construct_treeid(tree, ptspath)\n",
    "    \n",
    "    # run iterative dbh for specified step size and lim to calc slopes\n",
    "    treedf = iter_dbh(tree, ptspath, step=step, lim=lim)\n",
    "    \n",
    "    # calculate slope between pair of dbh measurements\n",
    "    for disc in treedf['Slice_Hgt'].iloc[1:-1]:\n",
    "\n",
    "        # round disc heights to one decimal place\n",
    "        disc = round(disc, 1)\n",
    "        prev_disc = round(disc - step, 1)\n",
    "\n",
    "        # get dbh values at each disc height\n",
    "        val1 = treedf[treedf['Slice_Hgt'] == disc]['DBH_CV'].iloc[0]\n",
    "        val2 = treedf[treedf['Slice_Hgt'] == prev_disc]['DBH_CV'].iloc[0]\n",
    "\n",
    "        # calculate slope between dbh measurements\n",
    "        slope = (val1 - val2) / step\n",
    "\n",
    "        # make new column in treedf for slope\n",
    "        treedf.loc[treedf['Slice_Hgt'] == prev_disc, 'slope'] = slope\n",
    "\n",
    "        # flag slope values greater than given threshold\n",
    "        if abs(slope) > slope_thresh:\n",
    "            if slope < 0: treedf.loc[treedf['Slice_Hgt'] == prev_disc, 'Buttress'] = -1\n",
    "            if slope > 0: treedf.loc[treedf['Slice_Hgt'] == prev_disc, 'Buttress'] = 1\n",
    "\n",
    "        else:\n",
    "            treedf.loc[treedf['Slice_Hgt'] == prev_disc, 'Buttress'] = 0\n",
    "        \n",
    "    # if first three points are not flagged as buttress points:\n",
    "    if all(np.array(treedf['Buttress'].iloc[0:3]) == np.array([0, 0, 0])):\n",
    "        # set buttress height to 0.0 (i.e. bottom of tree stem)\n",
    "        buttress_index = 0\n",
    "        buttress_hgt = 0.0\n",
    "\n",
    "    # otherwise, find height of top of buttress\n",
    "    else:\n",
    "        try:\n",
    "            # find first sequence of three 0's (non-buttress points) that come after a negative slope \n",
    "            val_find = np.array([-1, 0, 0, 0])\n",
    "            df_condition = treedf['Buttress'].rolling(4).apply(lambda g: all(g.astype(int) == val_find), raw=True)\n",
    "            \n",
    "            # get the index where three 0's first occurs and height of buttress\n",
    "            buttress_index = np.where(df_condition == 1)[0][0]\n",
    "            buttress_hgt = treedf['Slice_Hgt'].iloc[buttress_index]\n",
    "        \n",
    "        except:\n",
    "            if verbose: print(f\"tree {tree_num}: no suitable place to measure dbh from. Setting buttress height to 0.1m by default.\")\n",
    "            \n",
    "            buttress_hgt = 0.1\n",
    "               \n",
    "    return buttress_hgt, treedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_irregular(tree, ptspath, step=0.1, slope_thresh=0.1, lim=10, dbh_thresh = 2):\n",
    "    \"\"\"\n",
    "    Flag any irregular parts of the tree stem to avoid using them for dbh.\n",
    "    \n",
    "    Using a rolling mean to easily account for/ avoid the influence of extremely \n",
    "    small outliers that may be caused by data occlusion. \n",
    "    \"\"\"\n",
    "    # run iterative dbh for specified step size and lim to calc slopes\n",
    "    treedf = iter_dbh(tree, ptspath, step=step, lim=lim)\n",
    "    \n",
    "    # get buttress height and subset dataframe to parts over the buttress\n",
    "    buttress_hgt, treedf = get_buttress_hgt(tree, ptspath, step=step, lim=lim, slope_thresh=slope_thresh)\n",
    "    over_buttress = treedf[treedf[\"Slice_Hgt\"] > buttress_hgt]\n",
    "    \n",
    "    # calculate a max dbh threshold - no greater 2 * minimum rolling mean dbh\n",
    "    dbh_thresh = np.nanmin(over_buttress['DBH_CV'].rolling(3).mean()) * dbh_thresh\n",
    "    \n",
    "    # create a filter using this threshold\n",
    "    dbh_filt = treedf['DBH_CV'] > dbh_thresh\n",
    "    \n",
    "    # flag values greater than the threshold\n",
    "    treedf.loc[dbh_filt, 'Flagged'] = 1\n",
    "    treedf.loc[~dbh_filt, 'Flagged'] = 0\n",
    "\n",
    "    return treedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dbh_hgt(tree, ptspath, step=0.1, slope_thresh=0.1, lim=10, dbh_thresh = 2):\n",
    "    \"\"\"\n",
    "    Take output of iter_dbh function and determine ideal height from which to calculate dbh\n",
    "    \n",
    "    Note: the rolling search doesn't seem to work if there is no buttress so this is broken up into \n",
    "    if else - TODO: but could simplify if I figure out how to fix the else part of the code to work\n",
    "    for non buttressed trees too \n",
    "    \"\"\"\n",
    "    # Get treeID\n",
    "    tree, tree_num = construct_treeid(tree, ptspath)\n",
    "\n",
    "    # get buttress height\n",
    "    buttress_hgt, treedf = get_buttress_hgt(tree, ptspath, step=step, lim=lim, slope_thresh=slope_thresh)\n",
    "    \n",
    "    # flag irregular points to be excluded from dbh height consideration\n",
    "    treedf = flag_irregular(tree, ptspath, step=step, slope_thresh=slope_thresh, lim=lim, dbh_thresh = dbh_thresh)\n",
    "\n",
    "    # subset treedf to only above buttress height\n",
    "    over_buttress = treedf[treedf[\"Slice_Hgt\"] > buttress_hgt]\n",
    "    \n",
    "    # select only non-flagged data\n",
    "    filt = over_buttress['Flagged'] == 1\n",
    "    validdf = over_buttress[~filt]\n",
    "\n",
    "    # get suitable value closest to 1.3m to use for dbh height\n",
    "    disclist = list(validdf['Slice_Hgt'])\n",
    "    dif = lambda disclist : abs(disclist - 1.3)\n",
    "    hgt = min(disclist, key=dif)\n",
    "\n",
    "    return hgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2D(trees, ptspath, ncols=4, dbh=True, iterdbh=False, hgtlist = None):\n",
    "    \"\"\"\n",
    "    Function to plot tree point clouds in x/z or y/z direction\n",
    "    \n",
    "    TODO: make iter dbh function work on just one tree - then call iter_dbh from within this function\n",
    "    \"\"\"\n",
    "    # determine number of rows to plot figures\n",
    "    fig, axs = formatplots(trees, ncols)\n",
    "    rowind = colind = treeind = 0\n",
    "\n",
    "    # loop through trees\n",
    "    for tree in trees:\n",
    "        # Get treeID\n",
    "        tree, tree_num = construct_treeid(tree, ptspath)\n",
    "\n",
    "        #get DBH from full cloud to preserve max number of hits on stem slice\n",
    "        pts = load_pts(tree,False,False)\n",
    "\n",
    "        # calculate tree height\n",
    "        tree_hgt = pts['z'].max() - pts['z'].min()\n",
    "            \n",
    "        max_pt = pts[pts['z'] == pts['z'].max()]\n",
    "\n",
    "        # plot scatter plot of points in x and z \n",
    "        axs[rowind, colind].scatter(pts['x'], pts['z'], s=3) \n",
    "        if dbh: \n",
    "            pts_dbh = pts[(pts['z'] > pts['z'].min() + 1.27) & (pts['z'] < pts['z'].min() + 1.33)]\n",
    "            axs[rowind, colind].plot(pts_dbh['x'], pts_dbh['z'], 'ro', markersize=3, label=f\"DBH Slice\")\n",
    "        \n",
    "        if iterdbh: \n",
    "            hgt = hgtlist[tree_num]\n",
    "            pts_hgt = pts[(pts['z'] > pts['z'].min() + hgt - 0.03) & (pts['z'] < pts['z'].min() + hgt + 0.03)]\n",
    "            axs[rowind, colind].plot(pts_hgt['x'], pts_hgt['z'], 'yo', markersize=3, label=f\"HGT Slice\")\n",
    "            \n",
    "\n",
    "        axs[rowind, colind].annotate(f'Height: {tree_hgt:.2f}', xy=(max_pt['x'].iloc[0], max_pt['z'].iloc[0]), va='top', ha='left', size=15)\n",
    "        axs[rowind, colind].legend(fontsize='xx-large', loc='upper left')\n",
    "        axs[rowind, colind].set_title(f'{tree_num} ({rowind}, {colind})')\n",
    "\n",
    "        # increase tree index to get to next tree\n",
    "        treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Stem Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_stems(tree, ptspath, nclusters, fixedheight = True, hgt=1.3, hgtlist = None):\n",
    "    \"\"\"\n",
    "    Looks at the tree point cloud at the desired height (either 1.3m or the optimal dbh height)\n",
    "    and splits the data into clusters representing tree stems.\n",
    "    \"\"\"\n",
    "    \n",
    "    tree, tree_num = construct_treeid(tree, ptspath)\n",
    "\n",
    "    if fixedheight:\n",
    "        hgt = hgt\n",
    "    else:\n",
    "        hgt = hgtlist[tree_num]\n",
    "\n",
    "    #get DBH from full cloud at desired height \n",
    "    pts = load_pts(tree,False,False)\n",
    "    pts_dbh = pts[(pts['z'] > pts['z'].min() + hgt - 0.03) & (pts['z'] < pts['z'].min() + hgt + 0.03)]\n",
    "    \n",
    "    # create clustering object with desired number of clusters\n",
    "    kmeans = KMeans(nclusters, n_init='auto')\n",
    "\n",
    "    # Fit data to the defined number of clusters\n",
    "    kmeans.fit(pts_dbh)\n",
    "\n",
    "    # Get labels for each point \n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_circle(circle_pts):\n",
    "    \"\"\" fit the circle to the point cloud and calculate dbh, residuals, etc.\"\"\"\n",
    "    # define subfunctions used to describe how well circle fits cross section \n",
    "    def calc_R(x,y, xc, yc):\n",
    "        \"\"\" \n",
    "        calculate the distance of each 2D points from the center (xc, yc) \n",
    "        calc_R() from Calders et al. 2015 (https://doi.org/10.5281/zenodo.7307956)\n",
    "        \"\"\"\n",
    "        return np.sqrt((x-xc)**2 + (y-yc)**2)\n",
    "\n",
    "    def f(c, x, y):\n",
    "        \"\"\" \n",
    "        calculate the algebraic distance between the data points and the mean circle centered at c=(xc, yc) \n",
    "        f() from Calders et al. 2015 (https://doi.org/10.5281/zenodo.7307956)\n",
    "        \"\"\"\n",
    "        Ri = calc_R(x, y, *c)\n",
    "        return Ri - Ri.mean()\n",
    "    \n",
    "    x = circle_pts['x']\n",
    "    y = circle_pts['y']\n",
    "\n",
    "    x_m = x.mean() # as first estimte of center\n",
    "    y_m = y.mean()\n",
    "\n",
    "    center_estimate = x_m, y_m\n",
    "\n",
    "    # optimize for the distance between data points and circle centered at the center estimate\n",
    "    center, ier = optimize.leastsq(f, center_estimate, args=(x,y)) \n",
    "    xc, yc = center\n",
    "    Ri = calc_R(x, y, *center)\n",
    "    R = Ri.mean()\n",
    "    dbh_circ = R * 2.0\n",
    "    residu = np.sum((Ri - R)**2)/len(Ri) # average residual between data points and circle fit\n",
    "\n",
    "    return residu, dbh_circ, xc, yc, circle_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overlap(circle_dict):\n",
    "    \"\"\" checks how much overlap between circles and disregards if more than 10% overlap \"\"\"\n",
    "    \n",
    "    labels = list(circle_dict.keys())\n",
    "    \n",
    "    i = 0\n",
    "    j = 1\n",
    "    \n",
    "    # initialize overlap to False\n",
    "    overlap = False \n",
    "    \n",
    "    # loop through each pair of circles and check overlap\n",
    "    while j < len(labels) and i < len(labels):\n",
    "\n",
    "        # pull circle values out of the dictionary passed into function\n",
    "        r1 = circle_dict[labels[i]]['r']\n",
    "        r2 = circle_dict[labels[j]]['r']\n",
    "\n",
    "        xc1 = circle_dict[labels[i]]['xc']\n",
    "        yc1 = circle_dict[labels[i]]['yc']\n",
    "\n",
    "        xc2 = circle_dict[labels[j]]['xc']\n",
    "        yc2 = circle_dict[labels[j]]['yc']\n",
    "\n",
    "        # calculate distance between centers of circles\n",
    "        dist = math.sqrt((xc1 - xc2) * (xc1 - xc2) + (yc1 - yc2) * (yc1 - yc2))\n",
    "\n",
    "        \n",
    "        if dist <= (r1 - r2) or dist <= (r2 - r1):\n",
    "            overlap = True\n",
    "        \n",
    "        # if circles overlap more than 10%, disregard \n",
    "        if dist < (r1 + r2) * 0.9:\n",
    "            overlap = True\n",
    "\n",
    "        # if j+1 is still less than the number of labels, increase j by 1\n",
    "        if j + 1 < len(labels):\n",
    "            j+=1\n",
    "\n",
    "        elif j + 1 >= len(labels) and i + 1 <= len(labels):\n",
    "            i+=1\n",
    "            j = i+1\n",
    "\n",
    "            \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_stems(tree, ptspath, maxstems = 4, fixedheight = True, hgt=1.3, hgtlist = None):\n",
    "    # start with one stem \n",
    "    tree, tree_num = construct_treeid(tree, trop_pts_path)\n",
    "    \n",
    "    if fixedheight:\n",
    "        hgt = hgt\n",
    "    else:\n",
    "        hgt = hgtlist[tree_num]\n",
    "    \n",
    "    # get DBH from full cloud at desired height \n",
    "    pts = load_pts(tree,False,False)\n",
    "    pts_dbh = pts[(pts['z'] > pts['z'].min() + hgt - 0.03) & (pts['z'] < pts['z'].min() + hgt + 0.03)]\n",
    "    \n",
    "    # initialize data dictionaries \n",
    "    residuals = {}\n",
    "    circ_vals = {}\n",
    "\n",
    "    # loop through checking residuals and overlap for number of possible stems up to the max \n",
    "    for stems in range(1, maxstems):\n",
    "        \n",
    "        # cluster the points into the specified number of stems \n",
    "        labels = split_stems(tree, ptspath, nclusters=stems, fixedheight = fixedheight, hgt=hgt, hgtlist = hgtlist)\n",
    "\n",
    "        # empty the dictionaries to hold values for each set of stems\n",
    "        inner_residuals = {}\n",
    "        inner_circ_vals = {}\n",
    "          \n",
    "        enough_pts = True\n",
    "        # fit a circle to each of the clusters\n",
    "        for label in np.unique(labels):\n",
    "            # get points for the stem of interest and fit a circle to the points\n",
    "            circle_pts = pts_dbh[labels == label]\n",
    "            \n",
    "            # check how many points in this cluster\n",
    "            if len(circle_pts) <= 3:\n",
    "                enough_pts = False\n",
    "                break\n",
    "            \n",
    "            residu, dbh_circ, xc, yc, circle_pts = fit_circle(circle_pts)\n",
    "\n",
    "            # append the circle data to the dictionary\n",
    "            inner_circ_vals[label] = {'r': dbh_circ/2.0, 'xc': xc, 'yc':yc}\n",
    "\n",
    "            # add residual value to list \n",
    "            inner_residuals[label] = residu\n",
    "            \n",
    "        # check overlap between circles\n",
    "        overlap = check_overlap(inner_circ_vals)\n",
    "\n",
    "        # if the circles don't overlap:\n",
    "        if overlap == False and enough_pts == True:\n",
    "            # average the residuals for each tree stem and add to dict\n",
    "            resid_list = []\n",
    "            for stem, resid in inner_residuals.items():\n",
    "                resid_list.append(resid)\n",
    "            \n",
    "            avg_resid = np.mean(resid_list)\n",
    "            \n",
    "            # add the data for each tree stem to the dict\n",
    "            residuals[stems] = avg_resid\n",
    "            circ_vals[stems] = inner_circ_vals\n",
    "    \n",
    "    nstems = min(residuals, key=residuals.get)\n",
    "        \n",
    "    return nstems\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_circular(tree, ptspath, nstems = 1, fixedheight = True, hgt=1.3, hgtlist = None):\n",
    "    \"\"\"\n",
    "    Returns metrics describing how well circle fits the cross section.\n",
    "    https://scipy-cookbook.readthedocs.io/items/Least_Squares_Circle.html\n",
    "    \"\"\"\n",
    "    \n",
    "    tree, tree_num = construct_treeid(tree, trop_pts_path)\n",
    "        \n",
    "    if fixedheight:\n",
    "        hgt = hgt\n",
    "    else:\n",
    "        hgt = hgtlist[tree_num]\n",
    "    \n",
    "    \n",
    "    # get DBH from full cloud at desired height \n",
    "    pts = load_pts(tree,False,False)\n",
    "    pts_dbh = pts[(pts['z'] > pts['z'].min() + hgt - 0.03) & (pts['z'] < pts['z'].min() + hgt + 0.03)]\n",
    "    \n",
    "    labels = split_stems(tree, ptspath, nclusters = nstems, fixedheight = fixedheight, hgt=hgt, hgtlist = hgtlist)\n",
    "    \n",
    "    datadict = {}\n",
    "    \n",
    "    # cycle through labels (tree stems)\n",
    "    for label in np.unique(labels):\n",
    "        \n",
    "        # empty the inner dict to hold data\n",
    "        innerdict = {}\n",
    "\n",
    "        # get points for the stem of interest and fit a circle to the points\n",
    "        circle_pts = pts_dbh[labels == label]\n",
    "\n",
    "        residu, dbh_circ, xc, yc, circle_pts = fit_circle(circle_pts)\n",
    "\n",
    "        # construct data dictionaries\n",
    "        innerdict = {'residual': residu, 'dbh_circ': dbh_circ, 'xc': xc, 'yc':yc, 'circle_pts':circle_pts, 'hgt': hgt}\n",
    "        datadict[label] = innerdict\n",
    "\n",
    "    \n",
    "    return datadict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wytham Woods\n",
    "wytham_pts_path = \"/Users/snix/Documents/2021-2023/UCL/Dissertation/DATA_clouds_ply\"\n",
    "wytham_qsm_path = \"/Users/snix/Documents/2021-2023/UCL/Dissertation/DATA_QSM_opt/\"\n",
    "tree_csv_path = \"/Users/snix/Documents/2021-2023/UCL/Dissertation/Calders Et Al/analysis_and_figures/trees_summary.csv\"\n",
    "\n",
    "# Tropical Trees\n",
    "trop_pts_path = '/Users/snix/Documents/2021-2023/UCL/Dissertation/Tropical_manual_ply'\n",
    "trop_csv_path = '/Users/snix/Documents/2021-2023/UCL/Dissertation/MLA01_man.csv'\n",
    "\n",
    "trop_pts_aut = '/Users/snix/Documents/2021-2023/UCL/Dissertation/Trop_clouds_aut'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singlestemmed: 585\n",
      "Multistemmed: 291\n"
     ]
    }
   ],
   "source": [
    "# Get lists of wytham and tropical trees\n",
    "wytham_trees = glob.glob(\"%s/*ply\" % wytham_pts_path)\n",
    "trop_trees = glob.glob(\"%s/*wood.ply\" % trop_pts_path)\n",
    "\n",
    "trop_trees_aut = glob.glob(\"%s/*leafon.ply\" % trop_pts_aut)\n",
    "trop_aut_sub = trop_trees_aut[0:10]\n",
    "\n",
    "\n",
    "# Split wytham trees into single/multi stemmed\n",
    "singlestemmed, multistemmed = wytham_singlemulti(wytham_pts_path)\n",
    "\n",
    "print(f'Singlestemmed: {len(singlestemmed)}')\n",
    "print(f'Multistemmed: {len(multistemmed)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('x', 'y', 'z')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('x', 'y', 'z')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m woodpts \u001b[38;5;241m=\u001b[39m ptsdf[ptsdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwood\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1.0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#print(woodpts)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m pts \u001b[38;5;241m=\u001b[39m \u001b[43mwoodpts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(pts)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: ('x', 'y', 'z')"
     ]
    }
   ],
   "source": [
    "for tree in trop_aut_sub:\n",
    "    ptsdf = read_ply(tree)\n",
    "    \n",
    "    woodpts = ptsdf[ptsdf['wood']==1.0]\n",
    "    #print(woodpts)\n",
    "    \n",
    "    pts = woodpts['x', 'y', 'z']\n",
    "    \n",
    "    print(pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get DBH Heights, Calculate Convex Hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_data(treelist, ptspath, slope_thresh = 0.1, dbh_thresh = 2, verbose=False):\n",
    "    \"\"\"\n",
    "    Main function that runs other functions and fills dictionaries with data on the trees \"\"\"\n",
    "    \n",
    "    ## create empty dictionaries to hold data\n",
    "    hgt_dict = {}\n",
    "    dbh_13_dict = {}\n",
    "    dbh_adj_dict = {}\n",
    "    stems_dict = {}\n",
    "    dist_13_dict = {}\n",
    "    dist_adj_dict = {}\n",
    "    \n",
    "    # cycle through trees and fill dictionaries with optimal height and convex hull values\n",
    "    for tree in treelist:\n",
    "        tree, tree_num = construct_treeid(tree, ptspath)\n",
    "\n",
    "        # calculate convex hull at 1.3m\n",
    "        dbh_13, verts_13, pts_dbh_13, hull_13, maxdist = calc_convexhull(tree, ptspath, hgt=1.3)\n",
    "        dbh_13_dict[tree_num] = dbh_13\n",
    "        dist_13_dict[tree_num] = maxdist\n",
    "\n",
    "        # determine optimal height from which to calculate dbh\n",
    "        hgt = get_dbh_hgt(tree, ptspath, step=0.1, slope_thresh=slope_thresh, lim=10, dbh_thresh=dbh_thresh)\n",
    "        hgt_dict[tree_num] = hgt\n",
    "\n",
    "        # get number of stems at the optimal height \n",
    "        if treelist == trop_trees:\n",
    "            nstems = get_n_stems(tree, ptspath, fixedheight = False, hgt=1.3, hgtlist = hgt_dict)\n",
    "        # the wytham dataset is already split into single and multi-stemmed trees\n",
    "        else:\n",
    "            nstems = 1\n",
    "        \n",
    "        # add the number of stems to the dictionary\n",
    "        stems_dict[tree_num] = nstems\n",
    "\n",
    "        # get height at which to calculate convex hull and the points at that height\n",
    "        pts = load_pts(tree,False,False)\n",
    "        pts_dbh = pts[(pts['z'] > pts['z'].min() + hgt - 0.03) & (pts['z'] < pts['z'].min() + hgt + 0.03)]\n",
    "\n",
    "        # if the tree has multiple stems\n",
    "        if nstems != 1:\n",
    "            labels = split_stems(tree, ptspath, nclusters=nstems, fixedheight = False, hgtlist = hgt_dict)\n",
    "            dbh_adj_list = []\n",
    "\n",
    "            # calculate convex hull for each stem (each 'label') separately\n",
    "            for label in np.unique(labels):\n",
    "                circle_pts = pts_dbh[labels == label]\n",
    "                dbh_adj, verts_adj, pts_dbh_adj, hull_adj, maxdist = calc_convexhull(tree, ptspath, hgt=hgt, cluster_pts = circle_pts)\n",
    "                dbh_adj_list.append(dbh_adj)\n",
    "                #print(f'tree {tree_num}, label: {label}, dbh_adj: {dbh_adj:.2f}')\n",
    "\n",
    "            # average the dbh values for each stem and add to dictionary\n",
    "            dbh_adj = np.mean(dbh_adj_list)\n",
    "            dbh_adj_dict[tree_num] = dbh_adj\n",
    "            \n",
    "            dist_adj_dict[tree_num] = maxdist\n",
    "\n",
    "        else:\n",
    "            dbh_adj, verts_adj, pts_dbh_adj, hull_adj, maxdist = calc_convexhull(tree, ptspath, hgt=hgt, cluster_pts = None)\n",
    "            dbh_adj_dict[tree_num] = dbh_adj\n",
    "            \n",
    "            dist_adj_dict[tree_num] = maxdist\n",
    "    \n",
    "    return hgt_dict, dbh_13_dict, dbh_adj_dict, stems_dict, dist_13_dict, dist_adj_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_csv(treelist, dict_list, col_names, agg_multi = False, write_csv=False, o_path=None):\n",
    "    \"\"\"\n",
    "    Takes the dictionaries resulting from get_tree_data, creates a combined dataframe and writes to csv\n",
    "    \"\"\"\n",
    "    \n",
    "    # turn output dictionaries into a combined dataframe\n",
    "    dfs = []\n",
    "    for data_dict, column_name in zip(dict_list, col_names):\n",
    "        df = pd.DataFrame.from_dict(data_dict, orient='index').reset_index()\n",
    "        df.columns = ['TLS_ID', column_name]\n",
    "        dfs.append(df)\n",
    "\n",
    "    \n",
    "    # merge resulting dataframes into one large dataframe \n",
    "    merged_df = dfs[0].merge(dfs[1],on='TLS_ID').merge(dfs[2],on='TLS_ID').merge(dfs[3],on='TLS_ID').merge(dfs[4],on='TLS_ID').merge(dfs[5],on='TLS_ID')\n",
    "    \n",
    "    if agg_multi and treelist == multistemmed:\n",
    "    # if running for multistemmed wytham trees, need to aggregate by row\n",
    "        print('in multistemmed')\n",
    "        merged_df['StemID'] = merged_df['TLS_ID'].str[-1:] # new column with stem ID\n",
    "        \n",
    "        # replace TLS_ID column with only tree ID (no stem ID)\n",
    "        stem_ID = merged_df['TLS_ID'].str[:-1]\n",
    "        merged_df['TLS_ID'] = stem_ID \n",
    "        print(f'length before agg: {len(merged_df)}')\n",
    "        \n",
    "        # aggregate the dataframe by tree ID (summing and averaging other columns as necessary)\n",
    "        merged_df = merged_df.groupby(['TLS_ID'],as_index=False).agg(dbh_hgt = (\"dbh_hgt\", \"mean\"),\n",
    "                                                                 dbh13 = (\"dbh13\", \"sum\"),\n",
    "                                                                 dbhadj = (\"dbhadj\", \"sum\"),\n",
    "                                                                 stems = (\"stems\", \"sum\"),\n",
    "                                                                 maxdist13 = (\"maxdist13\", \"max\"),\n",
    "                                                                 maxdistadj = (\"maxdistadj\", \"max\"))\n",
    "        print(f'length after agg: {len(merged_df)}')\n",
    "    # write to csv \n",
    "    if write_csv: merged_df.to_csv(o_path) \n",
    "    \n",
    "    # return merged dataframe\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output csv paths\n",
    "trop_out = '/Users/snix/Documents/2021-2023/UCL/Dissertation/Results/trop_dbh.csv'\n",
    "wytham_ss_out = '/Users/snix/Documents/2021-2023/UCL/Dissertation/Results/wytham_ss_dbh.csv'\n",
    "wytham_ms_out = '/Users/snix/Documents/2021-2023/UCL/Dissertation/Results/wytham_ms_dbh.csv'\n",
    "\n",
    "# set threshold values for buttress slope and trunk irregularities\n",
    "slope_thresh = 0.1\n",
    "dbh_thresh = 2\n",
    "\n",
    "# create list of treelists and output paths\n",
    "treelists = [multistemmed, singlestemmed, trop_trees]\n",
    "ptspathlist = [wytham_pts_path, wytham_pts_path, trop_pts_path]\n",
    "opathlist = [wytham_ms_out, wytham_ss_out, trop_out]\n",
    "\n",
    "merged_dfs = []\n",
    "for i in range (len(treelists)):\n",
    "    treelist = treelists[i]\n",
    "    ptspath = ptspathlist[i]\n",
    "    o_path = opathlist[i]\n",
    "    \n",
    "    # run the main function that fills the dictionaries with each type of data for the list of trees\n",
    "    hgt_dict, dbh_13_dict, dbh_adj_dict, stems_dict, dist_13_dict, dist_adj_dict = get_tree_data(treelist, ptspath, slope_thresh = 0.1, dbh_thresh = 2, verbose=False)\n",
    "\n",
    "    # add data dictionaries to a list and define list of column names\n",
    "    data_dicts = [hgt_dict, dbh_13_dict, dbh_adj_dict, stems_dict, dist_13_dict, dist_adj_dict]\n",
    "    column_names = ['dbh_hgt', 'dbh13', 'dbhadj', 'stems', 'maxdist13', 'maxdistadj']\n",
    "\n",
    "    # merge the dictionaries and write to csv\n",
    "    ms_df = write_data_csv(treelist, data_dicts, column_names, write_csv=True, o_path=o_path)\n",
    "    \n",
    "    merged_dfs.append(ms_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data formats to use in plotting, analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wytham_ms_df = merged_dfs[0]\n",
    "wytham_ss_df = merged_dfs[1]\n",
    "trop_df = merged_dfs[2]\n",
    "\n",
    "all_trees = pd.concat([wytham_ms_df, wytham_ss_df, trop_df])\n",
    "\n",
    "hgt_dict = dict(zip(all_trees.TLS_ID, all_trees.dbh_hgt))\n",
    "dist_dict = dict(zip(all_trees.TLS_ID, all_trees.maxdistadj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circle fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set threshold values for buttress slope and trunk irregularities\n",
    "slope_thresh = 0.1\n",
    "dbh_thresh = 2\n",
    "\n",
    "# create list of treelists and output paths\n",
    "#treelists = treelists\n",
    "#dbh_df_list = merged_dfs\n",
    "#ptspathlist = ptspathlist\n",
    "\n",
    "residual_dfs = []\n",
    "\n",
    "# cycle through trees, get residuals and plot circle fits\n",
    "for i in range (len(treelists)):\n",
    "    treelist = treelists[i]\n",
    "    ptspath = ptspathlist[i]\n",
    "    o_path = opathlist[i]\n",
    "    dbh_df = merged_dfs[i]\n",
    "    \n",
    "    # create empty dictionaries to hold data\n",
    "    residu_dict = {}\n",
    "    dbh_circ_dict = {}\n",
    "    \n",
    "    for tree in treelist:\n",
    "\n",
    "        tree, tree_num = construct_treeid(tree, ptspath)\n",
    "        \n",
    "        nstems = dbh_df[dbh_df['TLS_ID'] == tree_num]['stems'].iloc[0]\n",
    "\n",
    "        #hgt_dict = dict(zip(dbh_df.TLS_ID, dbh_df.dbh_hgt))\n",
    "        \n",
    "        # wytham dataset is already broken into single stems - run circular function on single stems only\n",
    "        if treelist == multistemmed or treelist == singlestemmed:\n",
    "            nstems = 1\n",
    "\n",
    "        # run circular function for the given tree\n",
    "        datadict = how_circular(tree, ptspath, nstems = nstems, fixedheight = False, hgtlist = hgt_dict)\n",
    "\n",
    "        resid_list = []\n",
    "        dbh_list = []\n",
    "\n",
    "        # unroll and plot dictionary data for each stem\n",
    "        for label in datadict.keys():\n",
    "\n",
    "            xc = datadict[label]['xc']\n",
    "            yc = datadict[label]['yc']\n",
    "            dbh_circ = datadict[label]['dbh_circ']\n",
    "            residu = datadict[label]['residual']\n",
    "            pts_dbh = datadict[label]['circle_pts']\n",
    "            hgt = datadict[label]['hgt']\n",
    "\n",
    "            # add residual to list\n",
    "            resid_list.append(residu)\n",
    "            dbh_list.append(dbh_circ)\n",
    "\n",
    "        # average the residuals for the given tree \n",
    "        avg_resid = np.mean(resid_list)\n",
    "        sum_dbh = np.sum(dbh_list)\n",
    "\n",
    "        # fill dictionaries\n",
    "        residu_dict[tree_num] = avg_resid\n",
    "        dbh_circ_dict[tree_num] = sum_dbh\n",
    "        \n",
    "        # list of dicts and columns\n",
    "        dicts = [residu_dict, dbh_circ_dict]\n",
    "        cols = ['residuals', 'dbh_circ']\n",
    "\n",
    "    dfs = []\n",
    "    for data_dict, column_name in zip(dicts, cols):\n",
    "        df = pd.DataFrame.from_dict(data_dict, orient='index').reset_index()\n",
    "        df.columns = ['TLS_ID', column_name]\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # merge resulting dataframes into one large dataframe \n",
    "    residual_df = dfs[0].merge(dfs[1],on='TLS_ID')\n",
    "    residual_dfs.append(residual_df)\n",
    "\n",
    "# add normalized residuals onto the dataframes\n",
    "for df in residual_dfs:\n",
    "    df['resid_norm'] = (df['residuals'] / df['dbh_circ']) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the height at which dbh is measured for each tree (note most at 1.3m)\n",
    "fig, axs = plt.subplots(3, 1, figsize=(30,20))\n",
    "plt.tight_layout()\n",
    "\n",
    "rowind = treeind = colind = 0\n",
    "\n",
    "for df in merged_dfs:\n",
    "\n",
    "    # create dataset of only the points where the adjusted dbh height was 1.3 to mask these values\n",
    "    \n",
    "    mask13 = df[df['dbh_hgt'] == 1.3]\n",
    "\n",
    "    axs[rowind].scatter(df['TLS_ID'], df['dbh13'], color='red', label=\"adjusted dbh\")\n",
    "    axs[rowind].scatter(df['TLS_ID'], df['dbhadj'], color='blue', label=\"dbh at 1.3m\")\n",
    "    axs[rowind].scatter(mask13['TLS_ID'], mask13['dbh13'], color='gray', label=\"no adjustment\") # mask points where adjusted dbh height was 1.3\n",
    "    axs[rowind].get_xaxis().set_ticks([])\n",
    "    axs[rowind].set_ylabel('Adjustment to DBH Height (m)', fontsize=15)\n",
    "    \n",
    "    yerr = abs(df['dbhadj'] - df['dbh13'])\n",
    "    axs[rowind].errorbar(df['TLS_ID'], df['dbh13'], yerr, uplims=yerr, ls='none', label=\"difference (adjusted vs. 1.3m)\")\n",
    "\n",
    "    #plt.legend(loc=\"upper right\", fontsize=30)\n",
    "    \n",
    "    treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols=1)\n",
    "\n",
    "axs[0].set_title('Wytham - Multistemmed', loc='left', fontsize=20)\n",
    "axs[1].set_title('Wytham - Singlestemmed', loc='left', fontsize=20)\n",
    "axs[2].set_title('Tropical', loc='left', fontsize=20)\n",
    "axs[2].set_xlabel('Unique Trees', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the height at which dbh is measured for each tree (note most at 1.3m)\n",
    "fig, axs = plt.subplots(1, 1, figsize=(30,10))\n",
    "plt.tight_layout()\n",
    "\n",
    "rowind = treeind = 0\n",
    "colors = ['red', 'blue', 'green']\n",
    "labels = ['Wytham - Multistemmed', 'Wytham - Singlestemmed', 'Tropical']\n",
    "\n",
    "for df in merged_dfs:\n",
    "    color = colors[treeind]\n",
    "    label = labels[treeind]\n",
    "\n",
    "    axs.scatter(df['dbh13'], df['dbhadj'], color=color, label=label)\n",
    "    axs.set_aspect('equal')\n",
    "    axs.set_ylabel('DBH at Adjusted Height (m)', fontsize=15)\n",
    "    axs.set_xlabel('DBH at 1.3m (m)', fontsize=15)\n",
    "\n",
    "    plt.legend(loc=\"lower right\", fontsize=30)\n",
    "    \n",
    "    treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the height at which dbh is measured for each tree (note most at 1.3m)\n",
    "fig, axs = plt.subplots(1, 1, figsize=(20,30))\n",
    "plt.tight_layout()\n",
    "\n",
    "rowind = treeind = 0\n",
    "colors = ['white', 'white', 'green']\n",
    "labels = ['Wytham - Multistemmed', 'Wytham - Singlestemmed', 'Tropical']\n",
    "\n",
    "for df in merged_dfs:\n",
    "    color = colors[treeind]\n",
    "    label = labels[treeind]\n",
    "\n",
    "    axs.scatter(df['dbh_hgt'], df['dbhadj'], color=color, label=label)\n",
    "    axs.set_aspect('equal')\n",
    "    axs.set_ylabel('DBH (m)', fontsize=15)\n",
    "    axs.set_xlabel('DBH Height (m)', fontsize=15)\n",
    "\n",
    "    plt.legend(loc=\"upper right\", fontsize=20)\n",
    "    \n",
    "    treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wytham_ms_df_ov10 = wytham_ms_df[wytham_ms_df['dbhadj'] > 0.1]\n",
    "\n",
    "#print(f'full len: {len(wytham_ms_df)}, after removing smalls: {len(wytham_ms_df_ov10)}')\n",
    "\n",
    "#hgt_dict = dict(zip(wytham_ms_df.TLS_ID, wytham_ms_df.dbh_hgt))\n",
    "\n",
    "#ov10_list = list(wytham_ms_df_ov10['TLS_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wytham_notcircular = ['421', '281a', '838', '1558', '599c', '1179', '8178', '1107']\n",
    "\n",
    "# plot convex hull results using fixed 1.3m dbh height\n",
    "#plot_convexhull(plotlist, wytham_pts_path, ncols=4, fixedheight = True, hgt=1.3, hgtlist = hgt_dict)\n",
    "\n",
    "# plot convex hull results \n",
    "plot_convexhull(wytham_notcircular, wytham_pts_path, ncols=4, fixedheight = False, hgt=1.3, hgtlist = hgt_dict)\n",
    "\n",
    "for tree in wytham_notcircular:\n",
    "    print(f'max dist {tree}: {dist_dict[tree]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_iter_dbh(plotlist, pts_path, ncols = 2, step=0.1, slope_thresh=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot2D(wytham_notcircular, wytham_pts_path, ncols=3, dbh=False, iterdbh=True, hgtlist = hgt_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Circular?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Circle Fits of Some Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trop_residuals.sort_values(by='resid_norm', ascending=True, inplace=True)\n",
    "\n",
    "trop_residuals.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot very circular and very not-circular trees\n",
    "ptspath = trop_pts_path\n",
    "#ptspath = wytham_pts_path\n",
    "\n",
    "circular = ['T354', 'T203', 'T151',\n",
    "            'T359', 'T141', 'T161',\n",
    "            'T53', 'T345']\n",
    "\n",
    "not_circular = ['T303', 'T254', 'T341', 'T287', 'T132',\n",
    "                'T44', 'T288', 'T39']\n",
    "\n",
    "wytham_notcircular = ['421', '281a', '838', '1558', \n",
    "                      '599c', '1179', '8178', '1107']\n",
    "\n",
    "ncols = 4\n",
    "\n",
    "fig, axs = formatplots(treelist, ncols=ncols, shape='horiz')\n",
    "rowind = colind = treeind = 0\n",
    "\n",
    "for tree in circular:\n",
    "\n",
    "    tree, tree_num = construct_treeid(tree, ptspath)\n",
    "        \n",
    "    nstems = all_trees[all_trees['TLS_ID'] == tree_num]['stems'].iloc[0]\n",
    "    \n",
    "    # get max dist for the tree out of the other dataframe\n",
    "    maxdist = dist_dict[tree_num]\n",
    "\n",
    "    # run circular function for the given tree\n",
    "    datadict = how_circular(tree, ptspath, nstems = nstems, fixedheight = False, hgtlist = hgt_dict)\n",
    "\n",
    "    resid_list = []\n",
    "    dbh_list = []\n",
    "\n",
    "    # unroll and plot dictionary data for each stem\n",
    "    for label in datadict.keys():\n",
    "\n",
    "        xc = datadict[label]['xc']\n",
    "        yc = datadict[label]['yc']\n",
    "        dbh_circ = datadict[label]['dbh_circ']\n",
    "        residu = datadict[label]['residual']\n",
    "        pts_dbh = datadict[label]['circle_pts']\n",
    "        hgt = datadict[label]['hgt']\n",
    "\n",
    "        # plotting \n",
    "        theta_fit = np.linspace(-pi, pi, 180)\n",
    "        x_fit = xc + (dbh_circ / 2.0)*np.cos(theta_fit)\n",
    "        y_fit = yc + (dbh_circ / 2.0)*np.sin(theta_fit)\n",
    "\n",
    "        axs[rowind, colind].scatter(pts_dbh['x'], pts_dbh['y'])\n",
    "        axs[rowind, colind].plot(x_fit, y_fit, 'b-' , label=f\"residu={(residu / dbh_circ)*100:.4f}cm\", lw=2,zorder=5)\n",
    "        axs[rowind, colind].set_title(f'{tree_num} ({dbh_circ:.2f}m diam), (max dist:{maxdist:0.3f})', fontsize=20)\n",
    "        axs[rowind, colind].legend(loc='upper right')\n",
    "        axs[rowind, colind].get_xaxis().set_ticks([])\n",
    "        axs[rowind, colind].get_yaxis().set_ticks([])\n",
    "\n",
    "        axs[rowind, colind].set_aspect('equal')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols=ncols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Residuals of All Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals\n",
    "fig, axs = plt.subplots(1, 1, figsize=(30,10))\n",
    "plt.tight_layout()\n",
    "\n",
    "rowind = treeind = 0\n",
    "colors = ['blue', 'blue', 'red']\n",
    "labels = ['Wytham', 'Wytham', 'Tropical']\n",
    "\n",
    "\n",
    "for residual_df in residual_dfs:\n",
    "    color = colors[treeind]\n",
    "\n",
    "    axs.scatter(residual_df['dbh_circ'], residual_df['resid_norm'], color = color, label=labels[treeind])\n",
    "    #axs.set_aspect('equal')\n",
    "    axs.set_ylabel('Residuals from Circle Fit (cm)', fontsize=15)\n",
    "    axs.set_xlabel('Circle Fit Diameter (m)', fontsize=15)\n",
    "    axs.set_xlim(0,2)\n",
    "    axs.set_ylim(0,2)\n",
    "\n",
    "    plt.legend(loc=\"upper right\", fontsize=30)\n",
    "\n",
    "    treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wytham_resids = pd.concat([residual_dfs[0], residual_dfs[1]], ignore_index=True)\n",
    "\n",
    "wytham_resids.sort_values(by='resid_norm', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wytham_resids.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wytham_resids_np = np.array(wytham_resids['resid_norm'])\n",
    "tropical_resids_np = np.array(residual_dfs[1]['resid_norm'])\n",
    "\n",
    "n, bins, patches = plt.hist(x=tropical_resids_np, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Values')\n",
    "plt.text(23, 45, r'$\\mu=15, b=3$')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Dist Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdist_trop = np.array(trop_df['maxdistadj'])\n",
    "maxdist_wytham = np.array(pd.concat([wytham_ms_df, wytham_ss_df])['maxdistadj'])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "\n",
    "n, bins, patches = plt.hist(x=maxdist_wytham, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Max Dist in Convex Hull')\n",
    "plt.text(23, 45, r'$\\mu=15, b=3$')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Testing - Slope Thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run functions with varying slope thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big dictionaries of dataframes with sensitivity data \n",
    "slope_sensitivity_dict = {}\n",
    "\n",
    "# set list of trees to run convex hull on \n",
    "treelist = singlestemmed\n",
    "\n",
    "# list of threshold values to test\n",
    "slope_thresh_vals = [0.07, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# cycle through threshold values for one variable\n",
    "for slope_thresh in slope_thresh_vals:\n",
    "    # hold other threshold value constant\n",
    "    dbh_thresh = 2\n",
    "    \n",
    "    print(f'slope_thresh: {slope_thresh}, dbh_thresh: {dbh_thresh}')\n",
    "    \n",
    "    # create empty dictionaries to hold data\n",
    "    hgt_dict = {}\n",
    "    dbh_13_dict = {}\n",
    "    dbh_adj_dict = {}\n",
    "\n",
    "    # cycle through trees and fill dictionaries with optimal height and convex hull values\n",
    "    for tree in treelist:\n",
    "        tree, tree_num = construct_treeid(tree, trop_pts_path)\n",
    "\n",
    "        # calculate convex hull at 1.3m\n",
    "        dbh_13, verts_13, pts_dbh_13, hull_13, maxdist = calc_convexhull(tree, trop_pts_path, hgt=1.3)\n",
    "        dbh_13_dict[tree_num] = dbh_13\n",
    "\n",
    "        # determine optimal height from which to calculate dbh\n",
    "        hgt = get_dbh_hgt(tree, trop_pts_path, step=0.1, slope_thresh=slope_thresh, lim=10, dbh_thresh=dbh_thresh)\n",
    "        hgt_dict[tree_num] = hgt\n",
    "\n",
    "        #print(f'{tree_num} hgt: {hgt}')\n",
    "\n",
    "        # calculate convex hull at optimal height \n",
    "        dbh_adj, verts_adj, pts_dbh_adj, hull_adj, maxdist = calc_convexhull(tree, trop_pts_path, hgt=hgt)\n",
    "        dbh_adj_dict[tree_num] = dbh_adj\n",
    "    \n",
    "    # convert each dictionary to dataframe and rename columns\n",
    "    hgt = pd.DataFrame.from_dict(hgt_dict, orient='index').reset_index()\n",
    "    hgt.columns = ['TLS_ID', 'hgt']\n",
    "\n",
    "    dbh13 = pd.DataFrame.from_dict(dbh_13_dict, orient='index').reset_index()\n",
    "    dbh13.columns = ['TLS_ID', 'dbh13']\n",
    "\n",
    "    dbhadj = pd.DataFrame.from_dict(dbh_adj_dict, orient='index').reset_index()\n",
    "    dbhadj.columns = ['TLS_ID', 'dbhadj']\n",
    "\n",
    "    # create dataframe for the list of trees with optimal height and convex hull values\n",
    "    trop_treedata = pd.merge(pd.merge(hgt, dbh13, on ='TLS_ID'), dbhadj, on ='TLS_ID')\n",
    "    \n",
    "    # add dataframe to dictionary\n",
    "    slope_sensitivity_dict[slope_thresh] = trop_treedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Number of Adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for slope_thresh in slope_sensitivity_dict.keys():\n",
    "    datadict = slope_sensitivity_dict[slope_thresh]\n",
    "    adjusted = datadict[datadict['hgt'] != 1.3]\n",
    "    differences = np.mean(datadict['dbhadj'] - datadict['dbh13'])\n",
    "    print(f'slope thresh {slope_thresh} - {len(adjusted)} adjustments - mean dif: {differences:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Sensitivity to Slope Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 2\n",
    "fig, axs = formatplots(slope_sensitivity_dict.keys(), ncols=ncols, shape='horiz')\n",
    "rowind = colind = treeind = 0\n",
    "        \n",
    "for slope_thresh in slope_sensitivity_dict.keys():\n",
    "    datadict = slope_sensitivity_dict[slope_thresh]\n",
    "    \n",
    "    mask13 = datadict[datadict['hgt'] == 1.3]\n",
    "    \n",
    "    axs[rowind, colind].get_xaxis().set_ticks([])\n",
    "    axs[rowind, colind].set_title(f'Adjusted vs Original DBH Values (Slope Threshold: {slope_thresh})')\n",
    "    \n",
    "    axs[rowind, colind].scatter(datadict['TLS_ID'], datadict['dbh13'], color='red', label=\"adjusted dbh\")\n",
    "    axs[rowind, colind].scatter(datadict['TLS_ID'], datadict['dbhadj'], color='blue', label=\"dbh at 1.3m\")\n",
    "    axs[rowind, colind].scatter(mask13['TLS_ID'], mask13['dbh13'], color='gray', label=\"no adjustment\") # mask points where adjusted dbh height was 1.3\n",
    "\n",
    "    yerr = abs(datadict['dbhadj'] - datadict['dbh13'])\n",
    "    axs[rowind, colind].errorbar(datadict['TLS_ID'], datadict['dbh13'], yerr, uplims=yerr, ls='none', label=\"difference (adjusted vs. 1.3m)\")\n",
    "    \n",
    "\n",
    "    treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols)\n",
    "\n",
    "fig.tight_layout()\n",
    "handles, labels = axs[0,0].get_legend_handles_labels()\n",
    "#fig.legend(handles, labels, fontsize=15, bbox_to_anchor=(0.998, 0.978)) # manually placing legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#minilist = ['T207', 'T169', 'T44', 'T206', 'T102', 'T38'] \n",
    "\n",
    "minilist = ['8022', '8340', '2312', '8388', '1623', '1179'] \n",
    "\n",
    "for slope_thresh in slope_thresh_vals:\n",
    "    print(slope_thresh)\n",
    "    plot_iter_dbh(minilist, pts_path, ncols = 2, step=0.1, slope_thresh=slope_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Testing - DBH Irregularity Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big dictionaries of dataframes with sensitivity data \n",
    "dbh_sensitivity_dict = {}\n",
    "\n",
    "# set list of trees to run convex hull on \n",
    "treelist = singlestemmed\n",
    "\n",
    "# list of threshold values to test\n",
    "dbh_thresh_vals = [1.2, 1.5, 2, 2.5, 3, 3.5]\n",
    "\n",
    "# cycle through threshold values for one variable\n",
    "for dbh_thresh in dbh_thresh_vals:\n",
    "    # hold other threshold value constant\n",
    "    slope_thresh = 0.1\n",
    "    \n",
    "    print(f'dbh_thresh: {dbh_thresh}, slope_thresh: {slope_thresh}')\n",
    "    \n",
    "    # create empty dictionaries to hold data\n",
    "    hgt_dict = {}\n",
    "    dbh_13_dict = {}\n",
    "    dbh_adj_dict = {}\n",
    "    irreg_dict = {}\n",
    "\n",
    "    # cycle through trees and fill dictionaries with optimal height and convex hull values\n",
    "    for tree in treelist:\n",
    "        tree, tree_num = construct_treeid(tree, trop_pts_path)\n",
    "\n",
    "        # calculate convex hull at 1.3m\n",
    "        dbh_13, verts_13, pts_dbh_13, hull_13, maxdist = calc_convexhull(tree, trop_pts_path, hgt=1.3)\n",
    "        dbh_13_dict[tree_num] = dbh_13\n",
    "\n",
    "        # determine optimal height from which to calculate dbh\n",
    "        hgt = get_dbh_hgt(tree, trop_pts_path, step=0.1, slope_thresh=slope_thresh, lim=10, dbh_thresh=dbh_thresh)\n",
    "        hgt_dict[tree_num] = hgt\n",
    "\n",
    "        #print(f'{tree_num} hgt: {hgt}')\n",
    "\n",
    "        # calculate convex hull at optimal height \n",
    "        dbh_adj, verts_adj, pts_dbh_adj, hull_adj, maxdist = calc_convexhull(tree, trop_pts_path, hgt=hgt)\n",
    "        dbh_adj_dict[tree_num] = dbh_adj\n",
    "        \n",
    "        # flag irregular function \n",
    "        irreg_df = flag_irregular(tree, trop_pts_path, step=0.1, slope_thresh=slope_thresh, lim=10, dbh_thresh = dbh_thresh)\n",
    "        irregs = len(irreg_df[irreg_df['Flagged'] == 1])\n",
    "        irreg_dict[tree_num] = irregs\n",
    "    \n",
    "    \n",
    "    # convert each dictionary to dataframe and rename columns\n",
    "    hgt = pd.DataFrame.from_dict(hgt_dict, orient='index').reset_index()\n",
    "    hgt.columns = ['TLS_ID', 'hgt']\n",
    "\n",
    "    dbh13 = pd.DataFrame.from_dict(dbh_13_dict, orient='index').reset_index()\n",
    "    dbh13.columns = ['TLS_ID', 'dbh13']\n",
    "\n",
    "    dbhadj = pd.DataFrame.from_dict(dbh_adj_dict, orient='index').reset_index()\n",
    "    dbhadj.columns = ['TLS_ID', 'dbhadj']\n",
    "    \n",
    "    irreg = pd.DataFrame.from_dict(irreg_dict, orient='index').reset_index()\n",
    "    irreg.columns = ['TLS_ID', 'irregs']\n",
    "\n",
    "    # create dataframe for the list of trees with optimal height and convex hull values\n",
    "    trop_treedata = pd.merge(pd.merge(pd.merge(hgt, dbh13, on ='TLS_ID'), dbhadj, on ='TLS_ID'), irreg, on='TLS_ID')\n",
    "    \n",
    "    # add dataframe to dictionary\n",
    "    dbh_sensitivity_dict[dbh_thresh] = trop_treedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dbh_thresh in dbh_sensitivity_dict.keys():\n",
    "    datadict = dbh_sensitivity_dict[dbh_thresh]\n",
    "    adjusted = datadict[datadict['hgt'] != 1.3]\n",
    "    differences = np.mean(datadict['dbhadj'] - datadict['dbh13'])\n",
    "    irreg_num = np.mean(datadict['irregs'])\n",
    "    print(f'dbh thresh {dbh_thresh} - {len(adjusted)} adjustments - mean dif: {differences:.2f}, flagged {irreg_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap,LinearSegmentedColormap\n",
    "\n",
    "#fig, axs = formatplots(dbh_sensitivity_dict.keys(), ncols=ncols, shape='horiz')\n",
    "#rowind = colind = treeind = 0\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "# get colormap for plotting with 6 colors\n",
    "cmap = plt.colormaps['inferno'].resampled(6)\n",
    "newcolors = cmap(np.linspace(0, 1, 6))\n",
    "\n",
    "# create the lines connecting dots to help with visualization\n",
    "high_thresh = dbh_sensitivity_dict[3.5]\n",
    "low_thresh = dbh_sensitivity_dict[1.2]\n",
    "yerr = abs(high_thresh['irregs'] - low_thresh['irregs'])\n",
    "plt.errorbar(datadict['TLS_ID'], low_thresh['irregs'], yerr, uplims=yerr, ls='none', color='gray', linewidth=0.5, zorder = 0, fmt='none')\n",
    "\n",
    "# loop through threshold values and plot on the same figure\n",
    "i = 0\n",
    "for dbh_thresh in dbh_sensitivity_dict.keys():\n",
    "    datadict = dbh_sensitivity_dict[dbh_thresh]\n",
    "    color = newcolors[i]\n",
    "    plt.scatter(datadict['TLS_ID'], datadict['irregs'], color=color, label=\"adjusted dbh\")\n",
    "    \n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 2\n",
    "fig, axs = formatplots(dbh_sensitivity_dict.keys(), ncols=ncols, shape='horiz')\n",
    "rowind = colind = treeind = 0\n",
    "        \n",
    "for dbh_thresh in dbh_sensitivity_dict.keys():\n",
    "    datadict = dbh_sensitivity_dict[dbh_thresh]\n",
    "    \n",
    "    mask13 = datadict[datadict['hgt'] == 1.3]\n",
    "    \n",
    "    axs[rowind, colind].get_xaxis().set_ticks([])\n",
    "    axs[rowind, colind].set_title(f'Adjusted vs Original DBH Values (DBH Threshold: {dbh_thresh})')\n",
    "    \n",
    "    axs[rowind, colind].scatter(datadict['TLS_ID'], datadict['dbh13'], color='red', label=\"adjusted dbh\")\n",
    "    axs[rowind, colind].scatter(datadict['TLS_ID'], datadict['dbhadj'], color='blue', label=\"dbh at 1.3m\")\n",
    "    axs[rowind, colind].scatter(mask13['TLS_ID'], mask13['dbh13'], color='gray', label=\"no adjustment\") # mask points where adjusted dbh height was 1.3\n",
    "\n",
    "    yerr = abs(datadict['dbhadj'] - datadict['dbh13'])\n",
    "    axs[rowind, colind].errorbar(datadict['TLS_ID'], datadict['dbh13'], yerr, uplims=yerr, ls='none', label=\"difference (adjusted vs. 1.3m)\")\n",
    "    \n",
    "\n",
    "    treeind, colind, rowind = update_inds(treeind, rowind, colind, ncols)\n",
    "\n",
    "fig.tight_layout()\n",
    "handles, labels = axs[0,0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, fontsize=15, bbox_to_anchor=(0.998, 0.978)) # manually placing legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
