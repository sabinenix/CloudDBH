{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Code\n",
    "This code is provided by [Mat Disney, Phil Wilkes etc] \n",
    "Running on data from Calders et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is from TLS_Inventory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import open3d as o3d\n",
    "\n",
    "from shapely.ops import cascaded_union, polygonize\n",
    "from shapely.geometry import Polygon, LineString\n",
    "import shapely.geometry as geometry\n",
    "\n",
    "from scipy import optimize\n",
    "from scipy.spatial import ConvexHull, Delaunay\n",
    "import scipy.io as sio\n",
    "\n",
    "from matplotlib import pyplot as plt, cm, colors\n",
    "\n",
    "import math\n",
    "from math import pi\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from descartes import PolygonPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from TLS_Inventory\n",
    "# Some of the docstrings for functions added by Sabine to help make code easier to understand and modify later on.\n",
    "\n",
    "# neareast neighbour needed for outlier removal\n",
    "def nn(x,rad):\n",
    "    \"\"\"\n",
    "    Function from TLS_Inventory.\n",
    "    \n",
    "    x: a numpy array, rad: radius to search for neighbors within\n",
    "    \n",
    "    My understanding - can either specify number of neighbors to return (in sklearn NearestNeighbors),\n",
    "    or return all the neighbors and then filter to return only those within a specified distance radius\n",
    "    \"\"\"\n",
    "    nbrs = NearestNeighbors(algorithm='auto', metric='euclidean').fit(x) \n",
    "    distances, indices = nbrs.radius_neighbors(radius=rad) #find all neighbours within this search radius\n",
    "    \n",
    "    #output explained: https://stackoverflow.com/questions/53886289/confused-with-the-output-of-sklearn-neighbors-nearestneighbors \n",
    "    return distances, indices\n",
    "\n",
    "\n",
    "def load_pts(cloud,downsample=False,outliers=False): \n",
    "    \"\"\"\n",
    "    Function from TLS_Inventory that reads the point cloud data for single tree at a time and\n",
    "    returns pandas dataframe with the points split into x, y, z coordinates. \n",
    "    \n",
    "    cloud: point cloud from a single tree - note, point cloud data should be pre-processed and \n",
    "           split into individual trees prior to using this function.\n",
    "    downsample (Optional): Default is False, meaning all the points are retained. If \n",
    "                           True, the point clouds are sampled to keep only fraction of points.\n",
    "    outliers (Optional): Default is False, meaning outliers are retained. If True, \n",
    "    \"\"\"\n",
    "    # read the point cloud data for single tree and save as pandas df with columns for x, y, z coords\n",
    "    dftemp=o3d.io.read_point_cloud(cloud)\n",
    "    df=pd.DataFrame(data=np.asarray(dftemp.points),columns=['x', 'y', 'z']) #access the points\n",
    "    \n",
    "    # Optional downsampling\n",
    "    if downsample:\n",
    "        df=df.sample(frac=0.1) # keep 10pct of points\n",
    "    \n",
    "    # Optional outlier removal \n",
    "    if outliers: #remove outliers\n",
    "        xy=df.iloc[:,0:2].values # takes ALL rows, first and second column - x and y - and .values converts to numpy array\n",
    "        dist, indi = nn(xy,0.5) # get nearest neighbors within search radius of 0.5\n",
    "        cnt=[len(i) for i in indi] # count the kNN within the search radius\n",
    "        cnt = pd.DataFrame({'count':cnt})\n",
    "        \n",
    "        # set threshold for the number of neighbors we want to keep \n",
    "        threshold=df.shape[0]*0.0001 #1 neighbor for every 10 000 pts\n",
    "        \n",
    "        removed=sum(np.array(cnt)<threshold)\n",
    "        df=df[np.array(cnt)>=threshold]\n",
    "        print(\"Removed %i outliers using kNN threshold %.2f\" % (removed[0], threshold-1))\n",
    "    \n",
    "    return df #return pandas dataframe \n",
    "\n",
    "def plot_polygon(x,y,polygon,treename):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    margin = .3 #plotbounds margin\n",
    "    x_min, y_min, x_max, y_max = polygon.bounds\n",
    "    ax.set_xlim([x_min-margin, x_max+margin])\n",
    "    ax.set_ylim([y_min-margin, y_max+margin])\n",
    "    plt.axis('equal')\n",
    "    plt.xlabel('X [m]')\n",
    "    plt.ylabel('Y [m]')\n",
    "    plt.plot(x, y, 'ro', label='data', mew=1,alpha=0.3,zorder=1)\n",
    "    patch = PolygonPatch(polygon, fc='#999999',ec='#000000', fill=True,zorder=2,label='Concave hull',alpha=0.5,lw=2)\n",
    "    ax.add_patch(patch)\n",
    "    plt.legend(loc='best',labelspacing=0.1 )\n",
    "    plt.grid()\n",
    "    plt.title(\"Vertical crown projection of %s\" %treename)\n",
    "#     plt.show()\n",
    "    if not os.path.exists('plot/VCP/'):\n",
    "        os.makedirs('plot/VCP')\n",
    "    plt.savefig('plot/VCP/VCP_'+str(treename)+'.png')\n",
    "    plt.close()\n",
    "    \n",
    "def alpha_shape(x, y, alpha,treename,plotsavefig=False):\n",
    "    \"\"\"\n",
    "    Compute the alpha shape (concave hull) of a set\n",
    "    of points.\n",
    "    @param points: Iterable container of points.\n",
    "    @param alpha: alpha value to influence the\n",
    "        gooeyness of the border. Smaller numbers\n",
    "        don't fall inward as much as larger numbers.\n",
    "        Too large, and you lose everything!\n",
    "    \"\"\"\n",
    "    points=list(zip(x,y))\n",
    "    points=[geometry.Point(point) for point in points]\n",
    "    total_area = 0\n",
    "    if len(points) < 4:\n",
    "        # When you have a triangle, there is no sense\n",
    "        # in computing an alpha shape.\n",
    "        return geometry.MultiPoint(list(points)).convex_hull\n",
    "    def add_edge(edges, edge_points, coords, i, j):\n",
    "        \"\"\"\n",
    "        Add a line between the i-th and j-th points,\n",
    "        if not in the list already\n",
    "        \"\"\"\n",
    "        if (i, j) in edges or (j, i) in edges:\n",
    "            # already added\n",
    "            return\n",
    "        edges.add( (i, j) )\n",
    "        edge_points.append(coords[ [i, j] ])\n",
    "    coords = np.array([point.coords[0] for point in points])\n",
    "    tri = Delaunay(coords)\n",
    "    edges = set()\n",
    "    edge_points = []\n",
    "    # loop over triangles:\n",
    "    # ia, ib, ic = indices of corner points of the\n",
    "    # triangle\n",
    "    for ia, ib, ic in tri.simplices:\n",
    "        pa = coords[ia]\n",
    "        pb = coords[ib]\n",
    "        pc = coords[ic]\n",
    "        # Lengths of sides of triangle\n",
    "        a = math.sqrt((pa[0]-pb[0])**2 + (pa[1]-pb[1])**2)\n",
    "        b = math.sqrt((pb[0]-pc[0])**2 + (pb[1]-pc[1])**2)\n",
    "        c = math.sqrt((pc[0]-pa[0])**2 + (pc[1]-pa[1])**2)\n",
    "        # Semiperimeter of triangle\n",
    "        s = (a + b + c)/2.0\n",
    "        # Area of triangle by Heron's formula\n",
    "        area = math.sqrt(s*(s-a)*(s-b)*(s-c))\n",
    "        circum_r = a*b*c/(4.0*area)\n",
    "        # Here's the radius filter.\n",
    "        #print circum_r\n",
    "        if circum_r < 1.0/alpha: #the larger alpha, the fewer triangles will be included\n",
    "            add_edge(edges, edge_points, coords, ia, ib)\n",
    "            add_edge(edges, edge_points, coords, ib, ic)\n",
    "            add_edge(edges, edge_points, coords, ic, ia)\n",
    "    m = geometry.MultiLineString(edge_points)\n",
    "    triangles = list(polygonize(m))\n",
    "    poly=cascaded_union(triangles)\n",
    "    if plotsavefig:\n",
    "        plot_polygon(df['x'],df['y'],poly,treename)\n",
    "    return poly.area, poly.length, poly, edge_points\n",
    "\n",
    "def calc_hgt(z): #z is pandasdf column of z\n",
    "    hgt=z.max() - z.min()\n",
    "    return hgt\n",
    "\n",
    "def calc_R(x,y, xc, yc):\n",
    "    \"\"\" calculate the distance of each 2D points from the center (xc, yc) \"\"\"\n",
    "    return np.sqrt((x-xc)**2 + (y-yc)**2)\n",
    "        \n",
    "def f(c, x, y):\n",
    "    \"\"\" calculate the algebraic distance between the data points and the mean circle centered at c=(xc, yc) \"\"\"\n",
    "    Ri = calc_R(x, y, *c)\n",
    "    return Ri - Ri.mean()\n",
    "    \n",
    "def plot_dbh(x,y, xc, yc, R,polygon,df,treename,QSM=False):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    x_min, y_min, x_max, y_max = polygon.bounds\n",
    "    plt.axis('equal')\n",
    "    theta_fit = np.linspace(-pi, pi, 180)\n",
    "    x_fit = xc + R*np.cos(theta_fit)\n",
    "    y_fit = yc + R*np.sin(theta_fit)\n",
    "    plt.plot(x_fit, y_fit, 'b-' , label=\"fitted circle\", lw=2,zorder=5)\n",
    "    plt.plot([xc], [yc], 'bD', mec='y', mew=1)\n",
    "    plt.xlabel('X [m]')\n",
    "    plt.ylabel('Y [m]')\n",
    "    if QSM:\n",
    "        iter_list = glob.glob(\"%s%s-*\" % (qsm_path,treename))\n",
    "        lab_legend=0\n",
    "        for qsmfile in iter_list:\n",
    "            data = sio.loadmat(qsmfile)\n",
    "            i = 0\n",
    "            while sum(data['Len'][0:i+1]) < 1.3:\n",
    "                i = i+1;\n",
    "            offset_of_dbhHeight=1.3-sum(data['Len'][0:i])\n",
    "            #need to correct the xy location for plotting since it's a cylinder spanning some length\n",
    "            xqsm = data['Sta'][i][0]+offset_of_dbhHeight*data['Axe'][i][0]\n",
    "            yqsm = data['Sta'][i][1]+offset_of_dbhHeight*data['Axe'][i][1]\n",
    "            Rqsm = data['Rad'][i][0]\n",
    "            df_qsm = df[(df['z'] > data['Sta'][i][2])  & (df['z'] < data['Sta'][i][2]+data['Len'][i][0])]\n",
    "            if (lab_legend == 0): #only need legend once\n",
    "#                 plt.plot(df_qsm['x'], df_qsm['y'], 'go', label='Points construction QSM', mew=1,alpha=0.2,zorder=-1)\n",
    "                xqsm_fit = xqsm + Rqsm*np.cos(theta_fit)\n",
    "                yqsm_fit = yqsm + Rqsm*np.sin(theta_fit)\n",
    "                plt.plot(xqsm_fit, yqsm_fit, 'g-' , label=\"QSM cylinder\", lw=2,zorder=3)  \n",
    "                lab_legend=1\n",
    "            else:\n",
    "#                plt.plot(df_qsm['x'], df_qsm['y'], 'go', mew=1,alpha=0.3) #don't need to plot these points 10 times, might be \"problem\" if cylinder close to 1.3m length\n",
    "                xqsm_fit = xqsm + Rqsm*np.cos(theta_fit)\n",
    "                yqsm_fit = yqsm + Rqsm*np.sin(theta_fit)\n",
    "                plt.plot(xqsm_fit, yqsm_fit, 'g-', lw=2,zorder=2)  \n",
    "    # plot data\n",
    "    plt.plot([xqsm], [yqsm], 'gD', mec='y', mew=1)\n",
    "    plt.plot(x, y, 'ro', label='Points circle fit', mew=1,alpha=0.3,zorder=4)\n",
    "    patch = PolygonPatch(polygon, fc='#999999',ec='#000000', fill=True,label='Concave hull',zorder=2,alpha=0.7)\n",
    "    ax.add_patch(patch)\n",
    "    plt.legend(loc='best',labelspacing=0.1 )\n",
    "    plt.grid()\n",
    "    plt.title(\"DBH cross section of %s\" %treename)\n",
    "    if not os.path.exists('plot/DBH/'):\n",
    "        os.makedirs('plot/DBH')\n",
    "    plt.savefig('plot/DBH/DBH_'+str(treename)+'.png')\n",
    "#     plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def calc_dbh(treename,df): #treename string, pandasdf with x y z columns\n",
    "    if df['z'].max() - df['z'].min() > 1.3: #check if tree is at least 1.3m height\n",
    "        #get slice 1.27 - 1.33 m - limits according to Tansey et al 2009; Calders et al. 2015\n",
    "        df_dbh = df[(df['z'] > df['z'].min() + 1.27) & (df['z'] < df['z'].min() + 1.33)]\n",
    "        #filter outliers of DBH slice:\n",
    "        #take xy data only\n",
    "        xy=df_dbh.iloc[:,0:2].values #takes ALL rows, first and second column - x and y - and .values converts to numpy array\n",
    "        dist, indi = nn(xy,0.05)\n",
    "        cnt=[len(i) for i in indi] #count the kNN within the search radius\n",
    "        cnt = pd.DataFrame({'count':cnt})\n",
    "        threshold=2\n",
    "        removed=sum(np.array(cnt)<threshold)\n",
    "        df_dbh=df_dbh[np.array(cnt)>=threshold]\n",
    "        print(\"Removed %i outliers from DBH slice using kNN threshold %.2f\" % (removed[0], threshold-1))\n",
    "        ###\n",
    "        x = df_dbh['x']\n",
    "        y = df_dbh['y']\n",
    "        #calc DBH ; from https://scipy-cookbook.readthedocs.io/items/Least_Squares_Circle.html\n",
    "        #Optimize.leastsq is the most efficient method ; algebraic does not work well when not full circle is covered\n",
    "            # coordinates of the barycenter\n",
    "        x_m = x.mean() #as first estimte of center\n",
    "        y_m = y.mean()\n",
    "        center_estimate = x_m, y_m\n",
    "        center, ier = optimize.leastsq(f, center_estimate, args=(x,y)) #f is the function you want to optimise/minimise\n",
    "        xc, yc = center\n",
    "        Ri       = calc_R(x, y, *center)\n",
    "        R        = Ri.mean() #this is the radius\n",
    "        residu   = np.sum((Ri - R)**2)/len(Ri) #avg residual between pts and fitted circle\n",
    "        #outer hull:\n",
    "        ##slightly larger alpha here, want to resolve some detail in dbh slice\n",
    "        Tarea, Lshape, concave_hull, edge_points = alpha_shape(x,y,5,treename,False) \n",
    "        plot_dbh(x,y, xc, yc, R,concave_hull,df,treename,True)\n",
    "    else:\n",
    "        xc = yc = R = residu = 'NA'\n",
    "    return xc, yc, R, residu, Tarea, Lshape,concave_hull\n",
    "\n",
    "def averageQSM(treename,path_to_qsm): #this will calculate the average of multiple QSM iterations of treename\n",
    "    iter_list = glob.glob(\"%s%s-*\" % (path_to_qsm,treename))\n",
    "    if (len(iter_list) != 10):\n",
    "        sys.exit(\"There are no 10 QSMs for tree %s\" %treename)\n",
    "    vol = []\n",
    "    dbh = []\n",
    "    x_qsm = []\n",
    "    y_qsm = []\n",
    "    vol0_25 = []\n",
    "    vol25_50 = []\n",
    "    vol50_75 = []\n",
    "    vol75_100 = []\n",
    "    vol100_200 = []\n",
    "    vol200_500 = []\n",
    "    vol500_1000 = []\n",
    "    vol1000 = []\n",
    "    #Loop over iterations of QSM\n",
    "    for file in iter_list:\n",
    "#         print(\"opening %s\" %file)\n",
    "        data = sio.loadmat(file)\n",
    "        vol.append(data['TreeData'][0][0]/1000) #convert to m3\n",
    "        dbh.append(data['TreeData'][9][0]/100) #convert to m\n",
    "        i=0\n",
    "        while sum(data['Len'][0:i+1]) < 1.3:\n",
    "            i = i+1;\n",
    "        if (sum(data['Len'][0:i])==0):\n",
    "            offset_of_dbhHeight=1.3-sum(data['Len'][0:i])\n",
    "        else:\n",
    "            offset_of_dbhHeight=1.3-sum(data['Len'][0:i][0])\n",
    "        #need to correct the xy location for plotting since it's a cylinder spanning some length\n",
    "        x_qsm.append(data['Sta'][i][0]+offset_of_dbhHeight*data['Axe'][i][0])\n",
    "        y_qsm.append(data['Sta'][i][1]+offset_of_dbhHeight*data['Axe'][i][1])\n",
    "        #calculate vols per diam class\n",
    "        #divide in classes\n",
    "        tmp_vol0_25 = []\n",
    "        tmp_vol25_50 = []\n",
    "        tmp_vol50_75 = []\n",
    "        tmp_vol75_100 = []\n",
    "        tmp_vol100_200 = []\n",
    "        tmp_vol200_500 = []\n",
    "        tmp_vol500_1000 = []\n",
    "        tmp_vol1000 = []\n",
    "        for i in range(0,len(data['Rad'])):\n",
    "            if (data['Rad'][i]<=0.0125):\n",
    "                tmp_vol0_25.append(pi*data['Len'][i][0]*data['Rad'][i][0]**2) #this is m3\n",
    "            elif (data['Rad'][i]<=0.025):\n",
    "                tmp_vol25_50.append(pi*data['Len'][i][0]*data['Rad'][i][0]**2) #this is m3\n",
    "            elif (data['Rad'][i]<=0.0375):\n",
    "                tmp_vol50_75.append(pi*data['Len'][i][0]*data['Rad'][i][0]**2) #this is m3\n",
    "            elif (data['Rad'][i]<=0.05):\n",
    "                tmp_vol75_100.append(pi*data['Len'][i][0]*data['Rad'][i][0]**2) #this is m3\n",
    "            elif (data['Rad'][i]<=0.1):\n",
    "                tmp_vol100_200.append(pi*data['Len'][i][0]*data['Rad'][i][0]**2) #this is m3\n",
    "            elif (data['Rad'][i]<=0.25):\n",
    "                tmp_vol200_500.append(pi*data['Len'][i][0]*data['Rad'][i][0]**2) #this is m3\n",
    "            elif (data['Rad'][i]<=0.5):\n",
    "                tmp_vol500_1000.append(pi*data['Len'][i][0]*data['Rad'][i][0]**2) #this is m3\n",
    "            else:\n",
    "                tmp_vol1000.append(pi*data['Len'][i][0]*data['Rad'][i][0]**2) #this is m3\n",
    "        # sum up per class\n",
    "        vol0_25.append(sum(tmp_vol0_25))\n",
    "        vol25_50.append(sum(tmp_vol25_50))\n",
    "        vol50_75.append(sum(tmp_vol50_75))\n",
    "        vol75_100.append(sum(tmp_vol75_100))\n",
    "        vol100_200.append(sum(tmp_vol100_200))\n",
    "        vol200_500.append(sum(tmp_vol200_500))\n",
    "        vol500_1000.append(sum(tmp_vol500_1000))\n",
    "        vol1000.append(sum(tmp_vol1000))\n",
    "    #Calculate avg and sd of QSM\n",
    "    vol_avg = np.mean(vol,axis=0)\n",
    "    vol_sd = np.std(vol, ddof=1,axis=0) #the same as excell STDEV\n",
    "    dbh_avg = np.mean(dbh,axis=0)\n",
    "    dbh_sd = np.std(dbh, ddof=1,axis=0) #the same as excell STDEV\n",
    "    xqsm= np.mean(x_qsm,axis=0)\n",
    "    yqsm= np.mean(y_qsm,axis=0)\n",
    "    vol0_25_avg = np.mean(vol0_25,axis=0)\n",
    "    vol0_25_sd = np.std(vol0_25, ddof=1,axis=0) #the same as excell STDEV\n",
    "    vol25_50_avg = np.mean(vol25_50,axis=0)\n",
    "    vol25_50_sd = np.std(vol25_50, ddof=1,axis=0) #the same as excell STDEV\n",
    "    vol50_75_avg = np.mean(vol50_75,axis=0)\n",
    "    vol50_75_sd = np.std(vol50_75, ddof=1,axis=0) #the same as excell STDEV\n",
    "    vol75_100_avg = np.mean(vol75_100,axis=0)\n",
    "    vol75_100_sd = np.std(vol75_100, ddof=1,axis=0) #the same as excell STDEV\n",
    "    vol100_200_avg = np.mean(vol100_200,axis=0)\n",
    "    vol100_200_sd = np.std(vol100_200, ddof=1,axis=0) #the same as excell STDEV\n",
    "    vol200_500_avg = np.mean(vol200_500,axis=0)\n",
    "    vol200_500_sd = np.std(vol200_500, ddof=1,axis=0) #the same as excell STDEV\n",
    "    vol500_1000_avg = np.mean(vol500_1000,axis=0)\n",
    "    vol500_1000_sd = np.std(vol500_1000, ddof=1,axis=0) #the same as excell STDEV\n",
    "    vol1000_avg = np.mean(vol1000,axis=0)\n",
    "    vol1000_sd = np.std(vol1000, ddof=1,axis=0) #the same as excell STDEV\n",
    "\n",
    "    return xqsm, yqsm,vol_avg, vol_sd, dbh_avg, dbh_sd,vol0_25_avg,vol0_25_sd,\\\n",
    "    vol25_50_avg,vol25_50_sd,vol50_75_avg,vol50_75_sd,vol75_100_avg,vol75_100_sd,\\\n",
    "    vol100_200_avg,vol100_200_sd,vol200_500_avg,vol200_500_sd,vol500_1000_avg,vol500_1000_sd,\\\n",
    "    vol1000_avg,vol1000_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code - from TLS Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Note: If you re-run this code you might get very small variations from our run \\\\\n",
    "## on cloud derived metrics due to the downsampling of the point cloud (randomness of sample())  \\\\\n",
    "## QSMs are generated on the full point cloud\n",
    "pts_path=\"/Users/snix/Documents/2021-2023/UCL/Dissertation/subset_data_ply\"\n",
    "qsm_path=\"../DATA/qsm_opt/\"\n",
    "\n",
    "#header output file\n",
    "f_out = open('tls_summary3.csv','w')\n",
    "f_out.write('Tree_ID,stemlocx_[m],stemlocy_[m],stemloc_source,DBH_pts_[m],DBH_QSM_avg_[m],\\\n",
    "DBH_QSM_sd[m],Hgt_pts_[m],VerticalCrownProjectedArea_pts_[m2],Vol_QSM_avg_[m3],Vol_QSM_sd_[m3],\\\n",
    "Vol_QSM_D0_25mm_avg_[m3],Vol_QSM_D0_25mm_sd_[m3],Vol_QSM_D25_50mm_avg_[m3],Vol_QSM_D25_50mm_sd_[m3],\\\n",
    "Vol_QSM_D50_75mm_avg_[m3],Vol_QSM_D50_75mm_sd_[m3],Vol_QSM_D75_100mm_avg_[m3],Vol_QSM_D75_100mm_sd_[m3],\\\n",
    "Vol_QSM_D100_200mm_avg_[m3],Vol_QSM_D100_200mm_sd_[m3],Vol_QSM_D200_500mm_avg_[m3],Vol_QSM_D200_500mm_sd_[m3],\\\n",
    "Vol_QSM_D500_1000mm_avg_[m3],Vol_QSM_D500_1000mm_sd_[m3],Vol_QSM_D1000mm_avg_[m3],Vol_QSM_D1000mm_sd_[m3]\\n')\n",
    "\n",
    "\n",
    "trees = glob.glob(\"%s/*ply\" % pts_path)\n",
    "for tree in trees:\n",
    "    treeID=tree.split(\"/\")[-1].split(\".\")[0]\n",
    "    print(\"Processing tree %s\" %treeID)\n",
    "    #downsample and remove outliers\n",
    "    df=load_pts(tree,True,True)\n",
    "    #HEIGHT\n",
    "    hgt = calc_hgt(df['z'])\n",
    "    #VERTICAL CROWN PROJECTION VCP\n",
    "    VCP_area, VCP_length, VCP_concave_hull, edge_points = alpha_shape(df['x'],df['y'],2,treeID,True)\n",
    "    #get DBH from full cloud to preserve max number of hits on stem slice\n",
    "    df=load_pts(tree,False,False)\n",
    "    xc, yc, R, residu, DBH_CH_area, DBH_CH_length,poly=calc_dbh(treeID,df)\n",
    "    \n",
    "    #analyse QSMs\n",
    "    xqsm, yqsm,vol_avg, vol_sd, dbh_avg, dbh_sd,vol0_25_avg,vol0_25_sd,\\\n",
    "    vol25_50_avg,vol25_50_sd,vol50_75_avg,vol50_75_sd,vol75_100_avg,vol75_100_sd,\\\n",
    "    vol100_200_avg,vol100_200_sd,vol200_500_avg,vol200_500_sd,vol500_1000_avg,\\\n",
    "    vol500_1000_sd,vol1000_avg,vol1000_sd = averageQSM(treeID,qsm_path)\n",
    "\n",
    "    #    if (pi*R**2 > 1.5*DBH_CH_area or 1.5*pi*R**2 < DBH_CH_area):\n",
    "    if (treeID.split(\"_\")[2] == '8211' or treeID.split(\"_\")[2] == '8418'):\n",
    "        print(\"special rule for tree 8211 and 8418\")\n",
    "        DBH_pts=\"NA\"\n",
    "        stemx=xqsm\n",
    "        stemy=yqsm\n",
    "        stemsource=\"QSM\"\n",
    "    elif (residu > 0.01*R):\n",
    "        DBH_pts=\"NA\"\n",
    "        stemx=xqsm\n",
    "        stemy=yqsm\n",
    "        stemsource=\"QSM\"\n",
    "    elif (pi*R**2 > 1.5*DBH_CH_area or 1.5*pi*R**2 < DBH_CH_area):\n",
    "        DBH_pts=\"NA\"\n",
    "        stemx=xqsm\n",
    "        stemy=yqsm\n",
    "        stemsource=\"QSM\"\n",
    "    else:\n",
    "        DBH_pts=R*2\n",
    "        stemx=xc\n",
    "        stemy=yc\n",
    "        stemsource=\"PTS\"\n",
    "    f_out.write(treeID.split(\"_\")[2]+ ',' + str(stemx) + ',' + str(stemy) + ','+ str(stemsource) + ',' + \\\n",
    "    str(DBH_pts)+ ',' + str(float(dbh_avg))+ ',' + str(float(dbh_sd))  +',' + str(hgt) + ',' + \\\n",
    "    str(VCP_area) +',' + str(float(vol_avg))+ ',' + str(float(vol_sd))+',' + str(float(vol0_25_avg))+',' + \\\n",
    "    str(float(vol0_25_sd))+',' + str(float(vol25_50_avg))+',' + str(float(vol25_50_sd))+',' + \\\n",
    "    str(float(vol50_75_avg))+',' + str(float(vol50_75_sd))+',' + str(float(vol75_100_avg))+',' + \\\n",
    "    str(float(vol75_100_sd))+',' + str(float(vol100_200_avg))+',' + str(float(vol100_200_sd))+',' + \\\n",
    "    str(float(vol200_500_avg))+',' + str(float(vol200_500_sd))+',' + str(float(vol500_1000_avg))+',' + \\\n",
    "    str(float(vol500_1000_sd))+',' + str(float(vol1000_avg))+',' + str(float(vol1000_sd)) +'\\n')\n",
    "              \n",
    "f_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
